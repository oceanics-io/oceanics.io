from ftplib import FTP
from io import BytesIO
from minio import Minio
from minio.error import NoSuchKey
from uuid import uuid4
from json import loads as load_json
from flask import send_file
from functools import reduce
from redis import StrictRedis

from bathysphere_graph.drivers import *


def connect(host, root=None, **kwargs):

    ftp = FTP(host, **kwargs)
    assert "230" in ftp.login()  # attach if no open socket
    assert ftp.sock
    if root is not None:
        _ = ftp.cwd(root)
    return ftp


def indexFilesystem(
    ftp, graph, node=".", depth=0, limit=None, metadata=None, parent=None
):
    # type: (FTP, Driver, str, int, int, dict, dict) -> None
    """
    Build directory structure recursively.

    :param graph: database
    :param ftp: persistent ftp connection
    :param node: node in current working directory
    :param depth: current depth, do not set
    :param limit: maximum depth,
    :param metadata: pass the object metadata down one level
    :return:
    """

    def _map(rec):
        values = rec.split()
        key = values.pop().strip()
        return {key: values}

    if depth == 0 and parent is None:
        parent = create(
            db=graph,
            obj=Locations(
                **{"name": "FTP Server", "description": "Autogenerated FTP Server"}
            ),
        )

    if limit is None or depth <= limit:
        try:
            _ = ftp.cwd(node)  # target is a file
        except:
            create(
                db=graph,
                obj=Proxy(
                    **{"name": node, "description": "Autogenerated", "url": node}
                ),
                links=[parent],
            )

        else:
            collection = create(
                db=graph,
                obj=Proxy(
                    **{"name": node, "description": "Autogenerated", "url": node}
                ),
                links=[parent],
            )

            files = []
            ftp.retrlines("LIST", files.append)
            _fs = dict()
            for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                indexFilesystem(
                    ftp=ftp,
                    graph=graph,
                    node=k,
                    depth=depth + 1,
                    limit=limit,
                    metadata=v,
                    parent=collection,
                )

            if node != ".":
                _ = ftp.cwd("..")


def search(pattern, filesystem):
    # type: (str, dict) -> None or str
    """
    Recursively search a directory structure for a key.
    Call this on the result of `index`

    :param filesystem: paths
    :param pattern: search key
    :return:
    """
    for key, level in filesystem.items():
        if key == pattern:
            return key
        try:
            result = search(pattern, level)
        except AttributeError:
            result = None
        if result:
            return f"{key}/{result}"
    return None


def sync(ftp, remote, local, limit=None, filesystem=None):
    # type: (FTP, str, str, int, dict) -> int
    if filesystem is None:
        filesystem = indexFilesystem(ftp=ftp, node=".", limit=limit)
    path = search(filesystem=filesystem, pattern=remote)
    with open(local, "wb+") as fid:
        return int(ftp.retrbinary(f"RETR {path}", fid.write))


class Storage(Minio):

    def __init__(self, bucket_name=None, **kwargs):
        self.bucket_name = bucket_name
        Minio.__init__(self, **kwargs)
        if bucket_name is not None and not self.bucket_exists(bucket_name):
            _ = self.make_bucket(bucket_name)

    @staticmethod
    def connection(config=None):
        if config is None:
            config = app.app.config
        return Storage(
            bucket_name=config["bucketName"], **config["storage"]
        )

    def lock(self, session, key, dataset=None, headers=None):
        # type: (str, str, str, dict) -> dict or None
        """
        Place a lock on the dataset, may be partial - although this is not fully implemented
        """
        metadata = self.metadata_template("lock", headers=headers)
        object_name = f"{dataset}/{key}" if dataset is not None else key
        stat = self.head(object_name)
        if stat is not None:
            return None

        data = {session: []}
        key = self.create(
            dataset=dataset,
            key=key,
            data={session: []},
            metadata=metadata,
        )
        return {key: data}

    def unlock(self, session, key, dataset=None):
        # type: (str, str, str) -> bool
        """
        Unlock the dataset or repository IFF it contains the session ID
        """
        object_name = f"{dataset}/{key}" if dataset is not None else key
        stat = self.head(object_name)
        if stat is None:
            return False
        self.remove_object(self.bucket_name, object_name)
        return True

    @classmethod
    def session(cls, config, locking=False, fork=False):
        # type: (dict, bool, bool) -> Callable

        client = cls(bucket_name=config["bucketName"], **config["storage"])

        def decorator(fcn):
            # type: (Callable) -> Callable

            def wrapper(dataset=None, key=None, *args, **kwargs):
                # type: (str, str, list, dict) -> Any

                session = str(uuid4()).replace("-", "")
                lock = {
                    "session": session,
                    "dataset": dataset,
                    "key": app.app.config["lock"]
                } if locking else None

                if locking and client.lock(**lock, headers=app.app.config["headers"]) is None:
                    return "Lock in place", 500
                err = None
                try:
                    result = fcn(
                        storage=client,
                        dataset=None,
                        *args,
                        session=session,
                        **kwargs,
                    )
                except Exception as ex:
                    err = ex
                unlock_failed = locking and not client.unlock(**lock)
                if err is not None:
                    raise err
                if unlock_failed:
                    raise BlockingIOError

                if fork:
                    metadata = client.metadata_template(
                        file_type="image",
                        headers=app.app.config["headers"]
                    )
                    _ = client.create(
                        dataset=dataset,
                        key=key+".png",
                        data=result,
                        metadata=metadata,
                        content_type="image/png",
                    )
                    return send_file(result, mimetype="image/png")
                return result
            return wrapper
        return decorator

    def head(self, object_name):
        # type: (str) -> dict or None
        """
        Object metadata, None if not found
        """
        try:
            return self.stat_object(self.bucket_name, object_name)
        except NoSuchKey:
            return None

    def get(self, object_name, stream=False):
        # type: (str, bool) -> bytes or None
        """
        Get object data
        """
        try:
            data = self.get_object(
                bucket_name=self.bucket_name, object_name=object_name
            )
        except NoSuchKey:
            return None
        return data.data if not stream else data

    def create(self, data, dataset=None, key=None, metadata=None, codec="utf-8", content_type=None):
        # type: (dict or list or tuple or set or bytes, str, str, dict, str, str) -> str
        """
        Create an s3 connection if necessary, then create bucket if it doesn't exist.

        :param dataset: label for file
        :param key: object key
        :param data: data to serialize
        :param metadata: headers
        :param codec: how to encode strings
        :param content_type: only required if sending bytes
        """
        if key is None:
            key = str(uuid4()).replace("-", "")
        object_name = f"{dataset}/{key}" if dataset is not None else key

        if isinstance(data, set):
            data = tuple(data)

        if isinstance(data, (dict, list, tuple)):
            content_type = "application/json"
            buffer = bytes(dumps(data).encode(codec))
        elif isinstance(data, str):
            content_type = "text/plain"
            buffer = data.encode(codec)
        elif isinstance(data, (bytes, BytesIO)):
            if content_type is None:
                raise ValueError
            buffer = data
        else:
            raise TypeError

        if isinstance(buffer, BytesIO):
            _data = buffer
            length = len(buffer.getvalue())
        else:
            _data = BytesIO(buffer)
            length = len(buffer)

        self.put_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            data=_data,
            length=length,
            metadata=metadata,
            content_type=content_type,
        )
        return key

    def update(self, object_name, object_source, metadata):
        # type: (str, str, dict) -> bool
        try:
            self.copy_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                object_source=object_source,
                metadata=metadata
            )
        except NoSuchKey:
            return False
        return True

    def delete(self, conditions=None, pattern=None, objects_iter=None, limit=100):
        # type: (dict, str, str or tuple, int) -> (str, )
        """
        Remove all matching objects

        :param conditions: header matching
        :param pattern: file prefix/dataset
        :param limit: most to process at once
        :param objects_iter: known keys to remove

        :return: deleted files
        """
        if conditions is None:
            conditions = {
                "x-amz-meta-service": "bathysphere",
            }

        remove = ()
        if objects_iter is None:
            objects_iter = self.list_objects(self.bucket_name, prefix=pattern)
        elif isinstance(objects_iter, str):
            objects_iter = deque((objects_iter, ))
        else:
            objects_iter = deque(objects_iter)
        while objects_iter and limit:
            if isinstance(objects_iter, deque):
                object_name = (objects_iter.popleft(), )
            else:
                try:
                    object_name = next(objects_iter).object_name
                except StopIteration:
                    break

            stat = self.head(object_name)
            if stat is not None and all(stat.metadata.get(key) == val for key, val in conditions.items()):
                remove += (object_name,)
                limit -= 1

        self.remove_objects(
            bucket_name=self.bucket_name,
            objects_iter=remove
        )
        return remove

    def index(self, key=None, dataset=None, data=None, ext=None, replace=False):
        # type: (str, str, dict, ExtentType, bool) -> dict or None
        """
        Get or create an index under the top-level or an object prefix/dataset

        :param key: object name
        :param dataset: object storage prefix
        :param data: contents of index
        :param ext: data extent of indexed files if applicable
        :param replace: replace the current one

        :return: dictionary with index name and data
        """
        if data is None:
            data = {}
        if key is None:
            key = app.app.config["index"]
        object_name = key if dataset is None else f"{dataset}/{key}"
        stat = self.head(object_name)
        if stat is not None and not replace:
            return load_json(self.get(object_name))

        key = self.create(
            data=data,
            dataset=dataset,
            key=key,
            metadata=self.metadata_template(
                file_type="index",
                headers=app.app.config["headers"],
                ext=ext
            )
        )
        return {key: data}

    def parts(self, dataset, key):
        part = 0
        result = []
        while True:
            k = f"{dataset}/{key}-{part}"
            stat = self.head(k)
            if stat is None:
                break
            result.append(k)
            part += 1
        return result

    @staticmethod
    def metadata_template(file_type=None, parent=None, ext=None, headers=None):
        # type: (str, str, ExtentType, dict) -> dict
        if file_type == "lock":
            write = {"x-amz-acl": "private"}
        else:
            write = {"x-amz-acl": "public-read"}
        if parent:
            write["x-amz-meta-parent"] = parent
        if ext:
            write["x-amz-meta-extent"] = dumps(ext)

        write["x-amz-meta-created"] = datetime.utcnow().isoformat()
        write["x-amz-meta-service-file-type"] = file_type
        return {
            **(headers if isinstance(headers, dict) else {}),
            **write
        }

    def listener(self, filetype="", channel="bathysphere-events"):
        fcns = ("s3:ObjectCreated:*", "s3:ObjectRemoved:*", "s3:ObjectAccessed:*")
        r = StrictRedis()
        ps = r.pubsub()
        for event in self.listen_bucket_notification(
            self.bucket_name, "", filetype, fcns
        ):
            ps.publish(channel, str(event))
