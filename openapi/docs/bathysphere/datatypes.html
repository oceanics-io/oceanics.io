<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>bathysphere.datatypes API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>bathysphere.datatypes</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=line-too-long,invalid-name
from __future__ import annotations
from enum import Enum
from typing import Callable, Any, Coroutine
from datetime import datetime, date, timedelta
from math import floor
from json import dumps, loads, decoder, load as load_json
from collections import deque
from uuid import uuid4
from os import getpid, getenv
from os.path import isfile
from io import BytesIO, TextIOWrapper
from difflib import SequenceMatcher
from functools import reduce
from ftplib import FTP
from pickle import loads as unpickle
from re import sub
from xml.etree import ElementTree
from itertools import repeat, chain
from multiprocessing import Pool
from warnings import warn, simplefilter

from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.engine.url import URL
import hmac
import hashlib

from bidict import bidict
import attr
from minio import Minio
from minio.error import NoSuchKey
from requests import get, post
from requests.exceptions import ConnectionError
from urllib3.exceptions import MaxRetryError
from flask import Response, Request, request
from redis import StrictRedis
from redis.client import PubSub

from numpy import (
    array,
    append,
    frombuffer,
    argmax,
    argmin,
    random,
    where,
    isnan,
    cross,
    argwhere,
    arange,
    array,
    hstack,
    vstack,
    repeat,
    zeros,
    flip,
    unique,
)
from numpy.linalg import norm
from netCDF4 import Dataset as _Dataset
from pandas import read_html, date_range, Series, DataFrame

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KernelDensity
from pyproj import transform
from matplotlib import rc
from matplotlib.pyplot import subplots, subplots_adjust

try:
    import tensorflow as tf
    from tensorflow.keras import reduce_sum, square, placeholder, Session
except ImportError:
    pass

# Use ArrayFire for multiple GPU bindings if available, else use ndarray as stand-in
try:
    from arrayfire import array as texture
    import arrayfire as af
except ImportError:
    af = None
texture = af if af is not None else array
if af:
    _Array = af.Array or array
else:
    _Array = array

from bathysphere.utils import (
    join,
    parsePostgresValueIn,
    _parse_str_to_float,
    resolveTaskTree,
    synchronous,
    googleCloudSecret,
    normal,
    Path
)

SEC2DAY = 86400


ExtentType = (float, float, float, float)
IntervalType = (float, float)
ResponseJSON = (dict, int)
ResponseOctet = (dict, int)


@attr.s
class Array:
    &#34;&#34;&#34;
    Encapsulates ND Array IO and operations, using either
    numpy or arrayfire as a backend. 
    &#34;&#34;&#34;
    data: array = attr.ib(default=None)
    gpu: bool = attr.ib(default=False)

    @property
    def interval(self) -&gt; Interval:
        &#34;&#34;&#34;
        Get range of an array, which may be in GPU memory
        &#34;&#34;&#34;
        if self.gpu:
            tex = af.np_to_af_array(self.data)
            mn = af.min(tex)
            mx = af.max(tex)
        else:
            mn = min(self.data)
            mx = max(self.data)
        return mn, mx

    @property
    def range(self) -&gt; float:
        &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
        return self.data.max() - self.data.min()


    @property
    def normalized(self) -&gt; Array:
        &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
        return (self.data - self.data.min()) / self.range


    @property
    def colorize(self) -&gt; Array:
        &#34;&#34;&#34;
        Convert data field to color and transparency components
        &#34;&#34;&#34;
        normalized = self.normalized
        colors = zeros((*self.data.shape, 4), dtype=int) + 255
        colors[:, :, :, 0] *= normalized  # red
        colors[:, :, :, 1] *= 0  # green
        colors[:, :, :, 2] *= 1 - normalized  # blue
        colors[:, :, :, 3] *= 0.5 * normalized  # alpha
        return colors

@attr.s
class Bound:
    &#34;&#34;&#34;
    A bound is on an interval, may ne upper or lower, closed or open
    &#34;&#34;&#34;
    value: Any = attr.ib()
    closed: bool = attr.ib(default=False)


@attr.s
class BoundingBox:
    &#34;&#34;&#34;
    A bounding box is similar to an extent, but is define by two points instead of intervals
    &#34;&#34;&#34;
    lower_left: (float, float) = attr.ib()
    uppper_right: (float, float) =  attr.ib()


@attr.s
class ChemicalSystem:
    &#34;&#34;&#34;
    ChemicalSystems encapuslate conservative mass transport for a single
    tracked species of reactant
    &#34;&#34;&#34;
    sources = None
    value = None
    massAdded: Array = None
    symbol = None
    validRange = (0.0, None)

    @property
    def flux(self) -&gt; None:
        &#34;&#34;&#34;
        Transfer of concentration between control volumes
        &#34;&#34;&#34;
        return None

    @property
    def mass(self) -&gt; None:
        &#34;&#34;&#34;Calculate mass from concentration&#34;&#34;&#34;
        return None

    @property
    def delta(self):
        &#34;&#34;&#34;Current rate of changed, dynamically calculated&#34;&#34;&#34;
        return 0.0

    def __add__(self, other):
        &#34;&#34;&#34;Add two systems&#34;&#34;&#34;
        try:
            return self.value + other.value
        except:
            return self.value + other

    def __truediv__(self, other):
        &#34;&#34;&#34;Divide, such as for unit conversion&#34;&#34;&#34;
        try:
            return self.value / other.value
        except:
            return self.value / other

    def __lt__(self, other):
        &#34;&#34;&#34;Array compare&#34;&#34;&#34;
        return self.value &lt; other

    def __gt__(self, other):
        &#34;&#34;&#34;Array compare&#34;&#34;&#34;
        return self.value &gt; other

    def clamp(
        self,
        future: array,
        volume: array
    ):
        &#34;&#34;&#34;
        Enforce range

        :param concentration:
        :param future:
        :param volume:
        &#34;&#34;&#34;
        nodes, layers = where(self.value &lt; self.validRange[0])
        self.massAdded[nodes, layers] += volume * (self.value - future)
        return future.clip(max=self.validRange[1])

    def transfer(self, conversion: float = 1.0):
        &#34;&#34;&#34;
        :param conversion:

        :return:
        &#34;&#34;&#34;
        # Transport.horizontal(mesh, reactor, self.key)  # Mass flux, advection and diffusion
        # Transport.vertical(mesh, reactor, self.key)  # Mass flux, vertical sigma velocity
        self.mass += self.delta * conversion  # update state from reaction equations


@attr.s
class Clock:
    &#34;&#34;&#34;
    Timekeeper object with integer clock
    &#34;&#34;&#34;

    dt = attr.ib()
    start: int = attr.ib()  # time in seconds
    elapsed: int = attr.ib(default=0)

    @property
    def yd(self):
        return self.days % 365

    @property
    def time(self) -&gt; float:
        return self.days % 1.0

    @property
    def days(self) -&gt; float:
        return (self.start + self.elapsed) / SEC2DAY  # current time in days

    @property
    def next(self) -&gt; int:
        return self.start + self.elapsed + SEC2DAY

    def tick(self, dt: int = None) -&gt; int:
        &#34;&#34;&#34;
        Update clock

        :param dt: Optional parameter to assign new time step (integer seconds)
        :return: None
        &#34;&#34;&#34;
        if dt is not None:
            self.dt = dt
        self.elapsed += self.dt
        return self.start + self.elapsed


@attr.s
class CloudSQL:
    &#34;&#34;&#34;
    This class encapsulates a connection pool to a cloud based PostgreSQL provider.
    By default it expects a Google CloudSQL database. 
    &#34;&#34;&#34;

    auth: (str, str) = attr.ib()
    instance: str = attr.ib()
    port: int = attr.ib(default=5432)
    pool_size: int = attr.ib(default=4)
    max_overflow: int = attr.ib(default=2)
    pool_timeout: int = attr.ib(default=5)
    pool_recycle: int = attr.ib(default=1800)

    @property
    def engine(self) -&gt; Engine:
        &#34;&#34;&#34;
        The engine property will be used only once per request, so
        can safely be generated as a property. 
        &#34;&#34;&#34;
        user, password = self.auth
        return create_engine(
            URL(
                drivername=&#34;postgres+pg8000&#34;,
                username=user,
                password=password,
                database=&#34;postgres&#34;,
                query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
            ),
            pool_size=self.pool_size,
            max_overflow=self.max_overflow,
            pool_timeout=self.pool_timeout,
            pool_recycle=self.pool_recycle,
        )

    def query(self, table, **kwargs) -&gt; [dict]:
        &#34;&#34;&#34;
        Execute an arbitrary query.
        &#34;&#34;&#34;
        with self.engine.connect() as cursor:
            query: Query = table.select(**kwargs)
            return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]

    def handle(self, request: Request) -&gt; ResponseJSON:
        &#34;&#34;&#34;
        Do some postgres stuff
        &#34;&#34;&#34;
        # pylint: disable=broad-exception
        conf = request.body[&#34;table&#34;]
        fields = [
            Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
        ]
        table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

        try:
            records = self.query(table=table)
        except Exception as ex:  
            return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

        try:
            return (
                dumps(
                    {
                        &#34;count&#34;: len(records),
                        &#34;data&#34;: records,
                        &#34;method&#34;: str(request.method),
                        &#34;query_string&#34;: str(request.query_string),
                    }
                ),
                200,
            )
        except Exception as ex:
            return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500

@attr.s
class Condition:
    &#34;&#34;&#34;
    Conditions are a base class for BOUNDARY and SOURCE types.

    :param nodes: optional node indices, if None same value applied universally (non-point)
    :param layers: optional layer indices, if None same value applied over column
    &#34;&#34;&#34;

    value: array = attr.ib()
    shape: (int) = attr.ib()
    mapping: (array, array) = attr.ib()
    scale: float = attr.ib(default=1.0)
    mass: float = attr.ib(default=0.0)
    next: float = attr.ib(default=None)
    last: float = attr.ib(default=None)

    @property
    def delta(self):
        &#34;&#34;&#34;
        Convenient property to auto
        &#34;&#34;&#34;
        return self.value * self.scale if self.scale is not None else self.value

    def boundary(self, system) -&gt; None:
        &#34;&#34;&#34;
        Boundaries are conditions which override the current state, and impose a new value. They may be a time-varying
        function, constant, or may be controlled by an external simulation.
        &#34;&#34;&#34;
        system[self.mapping] = self.value
        return system

    def mark(self, nodes):
        &#34;&#34;&#34;
        flag nodes as source

        :param nodes:
        :return:
        &#34;&#34;&#34;
        nodes.source[self.mapping] = True

    def update(self, dt: float):
        &#34;&#34;&#34;
        Update values from slope, and calculate new slope

        :param dt:
        :return:
        &#34;&#34;&#34;

        self.value += self.delta * dt
        return self

    def read(self, path: str, conversion: float = 1000):
        &#34;&#34;&#34;
        Read forcing conditions from CSV file, and update difference equation.
        Will fail silently if condition was declared constant

        :param path: path to CSV file
        :param conversion: unit conversion factor

        :return: success
        &#34;&#34;&#34;

        try:
            fid = open(path, &#34;r&#34;)
            data = array(fid.readline().split(&#34;,&#34;)).astype(float)
            fid.close()

            self.last, self.next = (
                self.next,
                data[0],
            )  # simulation time or reads in integer seconds
            self.delta = (data[1:] * conversion * self.scale - self.value) / (
                self.next - self.last
            )
        except AttributeError:
            return False
        else:
            return True

    def source(self, system: ChemicalSystem) -&gt; None:
        &#34;&#34;&#34;
        Source are a type of condition. They are added to their parent state array.

        :param system: chemistry instance
        :param key: internal pool key of tracer
        :param scale: optional conversion factor, used primarily for surface area correction
        &#34;&#34;&#34;
        system.mass[self.mapping] += self.delta
        self.mass += self.delta.sum()  # add to mass balance counter
        return system

    @classmethod
    def NonPointSource(cls):
        &#34;&#34;&#34;
        Uniform by default. Can also be vertically or horizontally uniform if desired.

        Atmospheric and sediment sources are special cases.
        &#34;&#34;&#34;
        return cls()

    @classmethod
    def PointSource(cls, nodes: (int) = None, layers: (int) = None):
        &#34;&#34;&#34;
        Point source loads are defined at some but not all nodes. Points which are not part of the mesh model
        (locations that are not nodes, or location that ARE elements) are divided amongst nearest neighbors.
        This is also true when mass is released between sigma layers,
        such as Lagrangian particle models with vertical dynamics.
        &#34;&#34;&#34;
        return cls(mapping=(nodes, layers))

    @classmethod
    def Surface(cls, nodes: (int) = None):
        &#34;&#34;&#34;
        Atmospheric loads are non-point sources. They may vary in space.
        &#34;&#34;&#34;
        return cls(mapping=(nodes, (0,)))

    @classmethod
    def FallLine(cls, nodes: (int), layers: (int) = None):
        &#34;&#34;&#34;
        Fall-line loads occur where mass enters the system at a boundary, usually a well-mixed freshwater discharge.
        The same concentration is added along a node-defined path, composed of at least two points on the shoreline,
        which are joined by edges either transecting the discharge stream, or following the shoreline (e.g. ground
        water).

        They are a special type of point source.
        &#34;&#34;&#34;
        cls(mapping=(nodes, layers))


def ConvexHull(points):
    &#34;&#34;&#34;
    Convex hulls are used to speed up spatial relation queries
    &#34;&#34;&#34;
    def segment(u, v, indices, points):
        &#34;&#34;&#34;Bisect the points&#34;&#34;&#34;
        if indices.shape[0] == 0:
            return array([], dtype=int)

        def crossProduct(i, j):
            &#34;&#34;&#34;Calculate angles&#34;&#34;&#34;
            return cross(points[indices, :] - points[i, :], points[j, :] - points[i, :])

        w = indices[argmin(crossProduct(u, v))]
        a = indices[argwhere(crossProduct(w, v) &lt; 0).flatten()]
        b = indices[argwhere(crossProduct(u, w) &lt; 0).flatten()]

        return hstack((segment(w, v, a, points), w, segment(u, w, b, points)))

    u = argmin(points[:, 0])
    v = argmax(points[:, 0])
    indices = arange(0, points.shape[0])
    parted = cross(points[indices, :] - points[u, :], points[v, :] - points[u, :]) &lt; 0

    a = indices[argwhere(~parted)]
    b = indices[argwhere(parted)]

    return hstack((u, segment(v, u, a, points), v, segment(u, v, b, points), u))


@attr.s
class Coordinates:
    &#34;&#34;&#34;Point coordinates for spatial applications&#34;&#34;&#34;

    x: float = attr.ib()
    y: float = attr.ib()


class CoordinateSystem(Enum):
    &#34;&#34;&#34;Well Known Coordinate Systems&#34;&#34;&#34;
    Sigma = 1
    Cartesian = 2
    Gaussian = 3
    Spherical = 4
    Periodic = 5


class DataFormat(Enum):
    &#34;&#34;&#34;Well Knonw NDArray formats&#34;&#34;&#34;
    NETCDF3_CLASSIC = 1
    NETCDF4 = 2
    NETCDF5 = 3
    Custom = 4
    Binary = 5
    NumpyArray = 6
    ArrayfireTexture = 7


class Dataset(_Dataset):
    &#34;&#34;&#34;
    Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
    or drop-outs.

    * Query: Get an array of a single variable
    * Cache: Save chunk in object storage or local filesystem
    &#34;&#34;&#34;

    def query(
        self,
        observed_property: str,
        samples: int = None,
        reduce_dim: bool = False,
        kind: str = &#34;float64&#34;,
    ) -&gt; array:
        &#34;&#34;&#34;
        Extract an observedProperty, and optionally extract pixel samples from it.
        :param observed_property: field to extract
        :param samples: buffer of pixel indices to sample
        :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
        :param kind: format for numerical data
        &#34;&#34;&#34;
        simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
        if samples:
            return array(
                self.variables[observed_property][0, i, j].astype(kind)
                for i, j in samples
            )
        return (
            self.variables[observed_property][:, 0].astype(kind)
            if reduce_dim
            else self.variables[observed_property][:].astype(kind)
        )

    def copy(self, path: str, observed_properties: (str) = None):
        &#34;&#34;&#34;
        Copy parts into a new file
        &#34;&#34;&#34;
        fid = _Dataset(path=path)
        if isfile(path=path) and not self.policy():
            return False
        for name, obj in self.dimensions.items():
            fid.createDimension(name, obj)
        for name, obj in self.variables.items():
            if observed_properties and str(name) not in observed_properties:
                continue  # not matching variables in source data
            fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
            fid.variables[name][:] = self.variables[name][:]
        fid.close()
        return fid


@attr.s
class Distance:
    &#34;&#34;&#34;Special distance type&#34;&#34;&#34;
    value: float = attr.ib()
    unit: str = attr.ib()


@attr.s
class Extent:
    &#34;&#34;&#34;Extents speed up relational queries&#34;&#34;&#34;
    value: ExtentType = attr.ib()

    def __call__(self):
        &#34;&#34;&#34;Unwrap the extent value when calling instance&#34;&#34;&#34;
        return self.value

    @property
    def vertex_array(self):
        &#34;&#34;&#34;
        Convert an Extent to a VertexArray
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])

    @property
    def bounding_box(self):
        &#34;&#34;&#34;
        Convert an Extent to a BoundingBox
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[3]]])

    @property
    def path(self) -&gt; Path:
        &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
        ext = self.value
        xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
        return Path(xy)

    @property
    def intervals(self):
        &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
        return (
            Interval(Bound(self.value[0]), Bound(self.value[1])),
            Interval(Bound(self.value[2]), Bound(self.value[3]))
        )


    def __add__(self, other: Extent) -&gt; Extent:
        &#34;&#34;&#34;
        Reduce extents through addition
        &#34;&#34;&#34;
        dat = zip(self.value, other.value)
        return min(next(dat)), max(next(dat)), min(next(dat)), max(next(dat))


    def overlaps(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        def _mapped(item: (Extent, Extent)):
            a, b = item
            return a.overlaps(b)

        return all(map(_mapped, zip(self.intervals, other.intervals)))


    def __contains__(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly contains B
        &#34;&#34;&#34;
        a, b = self.intervals
        c, d = other.intervals

        return c in a and d in b


@attr.s
class Feature:
    &#34;&#34;&#34;
    Format as GeoJSON feature
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    geometry: [[float]] = attr.Factory(list)
    properties: dict = attr.Factory(dict)
   

@attr.s
class FeatureCollection:
    &#34;&#34;&#34;
    GeoJSON feature collection
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    features: [Feature] = attr.Factory(list)
    properties: dict = attr.Factory(dict)
        

@attr.s
class Field:
    &#34;&#34;&#34;Column for Postgres table&#34;&#34;&#34;

    name: Any = attr.ib()
    type: str = attr.ib()

    @staticmethod
    def autoCorrect(
        key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
    ) -&gt; str:
        &#34;&#34;&#34;
        Match fieldnames probabilistically
        &#34;&#34;&#34;
        fields = lookup.keys()
        seq = SequenceMatcher(isjunk=None, autojunk=False)

        def _score(x):
            seq.set_seqs(key.lower(), x.lower())
            return seq.ratio()

        def _reduce(a, b):
            return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

        return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))

    @staticmethod
    def restore(final, units):
        # type: ((str, ), (str, )) -&gt; (str,)
        &#34;&#34;&#34;
        Get the original header name back by reversing clean_fields() operation.
        &#34;&#34;&#34;
        names = map(
            lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
        )
        return tuple(
            map(
                lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
                zip(names, units),
            )
        )

    @staticmethod
    def clean(fields: (str,)) -&gt; ((str,), (str,)):
        &#34;&#34;&#34;
        Make friendly formats for object and table naming. The inverse is restore_fields().
        &#34;&#34;&#34;

        def _clean(x):
            return (
                x.strip()
                .replace(&#34; &#34;, &#34;_&#34;)
                .replace(&#34;%&#34;, &#34;_percent&#34;)
                .replace(&#34;+&#34;, &#34;_plus&#34;)
                .replace(&#34;-&#34;, &#34;_minus&#34;)
            )

        return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))


@attr.s
class File:
    &#34;&#34;&#34;
    Originally used for Satlantic files, repurposed as general file system object.

    Very similar to Assets.
    &#34;&#34;&#34;
    name: str = attr.ib(default=&#34;&#34;)
    sn: int = attr.ib(default=None)
    url: str = attr.ib(default=None)
    time: datetime = attr.ib(default=None)
    ts: datetime = attr.ib(default=attr.Factory(datetime.now))
    kb: float = attr.ib(default=0.0)
    encoding: str = attr.ib(default=None)
    content: Any = attr.ib(default=None)

    def __repr__(self):
        &#34;&#34;&#34;Print formatting&#34;&#34;&#34;
        return &#34;{} ({}): {}&#34;.format(self.__class__.__name__, self.encoding, self.name)

    def __cmp__(self, other):
        &#34;&#34;&#34;Compare wrapper&#34;&#34;&#34;
        if hasattr(other, &#34;sort_key&#34;):
            return self.sort_key().__cmp__(other.sort_key())

    def serialize(self):
        &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
        return {
            &#34;url&#34;: self.url,
            &#34;ts&#34;: self.ts,
            &#34;kb&#34;: self.kb,
            &#34;encoding&#34;: self.encoding,
            &#34;content&#34;: self.content,
        }

    def sort_key(self):
        &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
        return self.time

    async def get_and_decode(self, headers, auth, span=32, sn=None):
        # type: (dict, str, int, int) -&gt; str or list
        &#34;&#34;&#34;
        Run a batch of jobs with processor pool and collapse results. This will use the file descriptions to
        retrieve and format object remote raw file, with partially parsed observations in binary buffer.

        Split the binary buffer into labeled frames. A frame corresponds to data from a single sensor/process,
        which may become mingled by the data-logger before serialization and transmission.
        &#34;&#34;&#34;
        response = get(self.url, auth=auth)
        if not response.ok:
            return response

        if self.encoding == FileType.XML:
            self.content = response.content.decode()  #

        elif self.encoding == FileType.Config:
            parts = (&#34;sensor&#34;, &#34;frame&#34;, &#34;parameter&#34;)
            self.content = deque(
                dict(zip(parts, each.decode().split(&#34;:&#34;)))
                for each in response.content.split(b&#34;\n&#34;)
                if each is not b&#34;&#34;
            )  # frames

        elif self.encoding == FileType.Raw:
            breaks = dict()
            for key in headers.keys():
                bkey = key.encode()
                cursor = response.content.find(bkey)
                while cursor != -1:
                    breaks[cursor] = key
                    cursor = response.content.find(bkey, cursor + span)

            breaks = list(breaks.keys())
            breaks.append(len(response.content))
            sorted_breaks = sorted(breaks)  # add end of buffer as the last stop

            self.content = deque(
                Frame(
                    data=response.content[start:end],
                    key=breaks[start],
                    sn=sn,
                    headers=headers,
                )
                for start, end in zip(sorted_breaks[:-1], sorted_breaks[1:])
            )

        self.content = response.content

    @classmethod
    def metadata(cls, url: str, filename: str, ts: str, size: str):
        &#34;&#34;&#34;
        Create a file metadata object
        &#34;&#34;&#34;
        fields = filename.split(&#34;.&#34;)
        encoding = None
        if len(fields) &gt; 1:
            fmt = fields.pop()
            if &#34;sensors&#34; == fmt:
                encoding = FileType.Config
            elif &#34;xml&#34; == fmt:
                encoding = FileType.Schema
            elif &#34;raw&#34; == fmt:
                encoding = FileType.Raw
            elif &#34;txt&#34; == fmt:
                if fields[-1] == &#34;raw&#34;:
                    fields.pop()  # convention is to have &#34;.raw.txt&#34;
                encoding = FileType.Log

        time = None
        if len(fields) &gt; 1:  # dated files
            ft = fields.pop()
            try:
                dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
                time = datetime.strptime(ft, dt_fmt)
            except ValueError:
                pass

        try:
            sn = int(fields.pop())
        except ValueError:
            sn = None

        path = url + filename

        return cls(
            name=filename,
            sn=sn,  # maybe None
            url=path,  # retrieval path
            time=time,  # file time from name, maybe None
            ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
            kb=_parse_str_to_float(size),  # float kilobytes
            encoding=encoding,
        )

    def _match(self, fmt=None, identity=None):
        # type: (File, set, set) -&gt; bool
        &#34;&#34;&#34;Filter for file objects&#34;&#34;&#34;
        return (not identity or self.sn in identity) and (
            not fmt or self.encoding in fmt
        )

    @staticmethod
    async def metadata_promise(url, auth):
        # type: (str, str) -&gt; tuple
        &#34;&#34;&#34;
        Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
        &#34;&#34;&#34;
        response = get(url, auth=auth)
        if not response.ok:
            return response.content

        df = read_html(response.content, skiprows=3)[0]
        return tuple(
            File.metadata(url, *r)
            for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
        )


@attr.s
class FileSystem:
    &#34;&#34;&#34;
    File systems are made up of files!
    &#34;&#34;&#34;
    @attr.s
    class OverwritePolicy:
        &#34;&#34;&#34;
        Basic logical unit for allowing/preventing mutability
        &#34;&#34;&#34;
        policy: str = attr.ib(default=&#34;never&#34;)

        def __call__(self, *args, **kwargs):
            if self == &#34;always&#34;:
                return True
            if self == &#34;prompt&#34;:
                print(&#34;Cache already exists. Overwrite? [y/N]&#34;)
                return input() in (&#34;Y&#34;, &#34;y&#34;)
            return False

    policy = OverwritePolicy(policy=&#34;never&#34;)

    @staticmethod
    def load_year_cache(local, years):
        # type: (str, (int, )) -&gt; dict
        &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
        combined = dict()
        for year in years:
            fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
            new = unpickle(fid)
            for key in new.keys():
                try:
                    combined[key] = append(combined[key], new[key])
                except KeyError:
                    combined[key] = array([])
                    combined[key] = append(combined[key], new[key])
        return combined

    @staticmethod
    def indexFileMetadata(url, year, auth=None):
        # type: (str, int, (str,)) -&gt; deque
        &#34;&#34;&#34;
        Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
        that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
        for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
        cached at a leisurely interactive pace.
        &#34;&#34;&#34;
        collector = deque()
        for record in resolveTaskTree(
            FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
        ):
            path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
            collector.append(
                {
                    &#34;date&#34;: date(*record),
                    &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                    &#34;url&#34;: path,
                    &#34;files&#34;: File.metadata_promise(path, auth=auth),
                }
            )
        return collector

    @staticmethod
    async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
        # type: (str, int, int, int, (str, )) -&gt; datetime or None
        &#34;&#34;&#34;
        Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

        Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
        into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
        using the `render()` method, until the specified depth is reached.
        &#34;&#34;&#34;

        def __parse(value):
            &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
            return value if type(value) == int else int(value[:-1])

        if count == depth:
            return enum, None

        try:
            formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
            insert = __parse(enum)
        except TypeError:
            return enum, None

        sublevel = formatter.format(url, insert)
        response = get(sublevel, auth=auth)
        if not response.ok:
            return enum, None

        collector = deque()
        for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
            collector.append(
                FileSystem.indexTaskTree(
                    url=sublevel,
                    enum=__parse(record),  # name
                    count=count + 1,
                    depth=depth,
                    auth=auth,
                )
            )

        return enum, collector

    @staticmethod
    def search(pattern, filesystem):
        # type: (str, dict) -&gt; None or str
        &#34;&#34;&#34;
        Recursively search a directory structure for a key.
        Call this on the result of `index`

        :param filesystem: paths
        :param pattern: search key
        :return:
        &#34;&#34;&#34;
        for key, level in filesystem.items():
            if key == pattern:
                return key
            try:
                result = FileSystem._search(pattern, level)
            except AttributeError:
                result = None
            if result:
                return f&#34;{key}/{result}&#34;
        return None

    @staticmethod
    def _search(
        queue: deque,
        pool: Pool,
        fmt: set = None,
        identity: set = None,
        ts: datetime = None
    ) -&gt; list or None:
        &#34;&#34;&#34;
        Get all XML and configuration files within a directory

        Find configurations from metadata by serial number and date.

        The files can be:
        - On a remote server
        - In the bathysphere_functions_cache
        - Supplied as a list of dictionaries
        &#34;&#34;&#34;
        iterators = []
        if identity:
            iterators.append(repeat(identity))
        if fmt:
            iterators.append(repeat(fmt))
        if ts:
            iterators.append(repeat(ts))

        def _chrono(x: File, ts: datetime = None):
            &#34;&#34;&#34;Chronoloigcal sorting method&#34;&#34;&#34;
            return (
                (x.time is None if ts else x.time is not None),
                (ts - x.time if ts else x.time),
            )

        queue = sorted(queue, key=_chrono, reverse=(False if ts else True))
        if fmt or identity:
            matching = pool.starmap(self._match, zip(queue, *iterators))
            queue = deque(queue)
        else:
            return {}, queue

        collector = dict()
        for condition in matching:
            if not condition:
                queue.rotate(1)
                continue
            file = queue.popleft()
            if not collector.get(file.sn, None):
                collector[file.sn] = deque()
            if (
                not ts or len(collector[file.sn]) == 0
            ):  # limit to length 1 for getting most recent
                collector[file.sn].append(file)
                continue

            queue.append(file)  # put the file back if unused

        return collector, queue

    @staticmethod
    def get_files(
        queue: deque, 
        pool: Pool, 
        **kwargs
    ):
        &#34;&#34;&#34;
        Create and process a day of raw files
        &#34;&#34;&#34;
        extracted, queue = FileSystem.search(
            queue=queue, pool=pool, **kwargs
        )  # get active configuration files
        headers = dict()
        for sn, files in extracted.keys():
            headers[sn] = deque()
            for file in files:
                synchronous(file.get_and_decode())
                if file.encoding == FileType.Config:
                    headers[sn].append(file.frames)

        return extracted, headers, queue

    @staticmethod
    def download(url, prefix=&#34;&#34;):
        # type: (str, str) -&gt; str
        &#34;&#34;&#34;
        Download a file accessible through HTTP/S.
        :param url: location of remote data
        :param prefix: local file path
        &#34;&#34;&#34;
        response = get(url, stream=True)
        filename = url.split(&#34;/&#34;).pop()
        if not response.ok:
            raise ConnectionError
        with open(f&#34;{prefix}{filename}&#34;, &#34;wb&#34;) as fid:
            copyfileobj(response.raw, fid)
        return filename

    def get(
        self,
        observed_properties,
        path=None,
        transpose=True,
        dataset=None,
        kind=&#34;float64&#34;,
        date=None,
    ):
        # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
        &#34;&#34;&#34;
        Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
        by name, resulting in an array. For previously processed internal data, arrays are stored as
        binary data in either `.pkl` or `.bathysphere_functions_cache` files.

        :param observed_properties: lookup field names
        :param path: path to local files if loading
        :param transpose: transpose the array before saving, makes join later easier
        :param dataset: NetCDF reference as in-memory object
        :param kind: numerical format for arrays
        :param date: specific timestamp to sample
        &#34;&#34;&#34;
        result = dict()

        if isinstance(observed_properties, str):
            fields = keys = [observed_properties]
        elif isinstance(observed_properties, dict):
            keys = observed_properties.keys()
            fields = observed_properties.values()
        else:
            fields = keys = observed_properties
        iterator = zip(*(keys, fields))

        for key, rename in iterator:
            if path:
                try:
                    fid = open(key, &#34;rb&#34;)
                except FileNotFoundError:
                    continue
                data = self.load_year_cache(fid).transpose() if transpose else self.load_year_cache(fid)
                fid.close()

            elif dataset:
                data = dataset.variables[key][:].astype(kind)
                self.set(date, data, key)
            else:
                data = None

            result[rename] = data

        return result

    @staticmethod
    def syncFtp(ftp, remote, local, filesystem=None):
        # type: (FTP, str, str, dict) -&gt; int
        &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
        path = FileSystem.search(pattern=remote, filesystem=filesystem)
        with open(local, &#34;wb+&#34;) as fid:
            return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))

    @staticmethod
    def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
        # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
        &#34;&#34;&#34;
        Build directory structure recursively.

        :param ftp: persistent ftp connection
        :param node: node in current working directory
        :param depth: current depth, do not set
        :param limit: maximum depth,
        :param metadata: pass the object metadata down one level
        :param parent:
        :return:
        &#34;&#34;&#34;

        body = loads(req)
        host = body.get(&#34;host&#34;, None)
        root = body.get(&#34;root&#34;, None)
        ftp = FTP(host, timeout=4)
        assert &#34;230&#34; in ftp.login()  # attach if no open socket
        assert ftp.sock
        if root is not None:
            _ = ftp.cwd(root)

        def _map(rec):
            values = rec.split()
            key = values.pop().strip()
            return {key: values}

        if depth == 0 and parent is None:
            parent = create(
                db=graph,
                obj=Locations(
                    **{&#34;name&#34;: &#34;FTP Server&#34;, &#34;description&#34;: &#34;Autogenerated FTP Server&#34;}
                ),
            )

        if limit is None or depth &lt;= limit:
            try:
                _ = ftp.cwd(node)  # target is a file
            except:
                create(
                    db=graph,
                    obj=Proxy(
                        **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                    ),
                    links=[parent],
                )

            else:
                collection = create(
                    db=graph,
                    obj=Proxy(
                        **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                    ),
                    links=[parent],
                )

                files = []
                ftp.retrlines(&#34;LIST&#34;, files.append)
                for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                    indexFtp(
                        ftp=ftp,
                        graph=graph,
                        node=k,
                        depth=depth + 1,
                        limit=limit,
                        metadata=v,
                        parent=collection,
                    )

                if node != &#34;.&#34;:
                    _ = ftp.cwd(&#34;..&#34;)



class FileType(Enum):
    &#34;&#34;&#34;Well known file types&#34;&#34;&#34;
    Schema = 1
    Config = 2
    Log = 3
    Raw = 4
    CSV = 5
    JSON = 6
    XML = 7


@attr.s
class Frame(dict):
    &#34;&#34;&#34;
    Data frames are partially parsed binary messages, usually from sensor streams
    &#34;&#34;&#34;
    data: bytes = attr.ib()
    label: bytes = attr.ib()
    headers: dict = attr.ib()
    sn: int = attr.ib()
    schema: dict = attr.ib()
    key: str = attr.ib(default=None)
    span: int = attr.ib(default=32)
    ts: datetime = attr.ib(default=None)
    _dict: dict = attr.ib(default=attr.Factory(dict))

    def goto(self, pattern: bytes, root: str = &#34;SensorFieldGroup&#34;):
        &#34;&#34;&#34;Move cursor&#34;&#34;&#34;
        return [self._dict[key][root] for key in self._dict.keys() if pattern in key][0]

    def wqm(self, keys: (str,)) -&gt; dict:
        &#34;&#34;&#34;
        Decode dataframe form water quality monitor instrument
        &#34;&#34;&#34;
        self.sensor = self.bytes[:9].decode()
        self.time = datetime.strptime(
            self.bytes[20:26].decode() + self.bytes[27:33].decode(), &#34;%m%d%y%H%M%S&#34;
        )
        assert self.time &lt; self.ts  # created date is before arrival time stamp

        self.data = {
            key: value for key, value in zip(keys, self.bytes[34:].decode().split(&#34;,&#34;))
        }
        return self

    def seafet(self, brk: int, keys: list, sep: bytes = b&#34;,&#34;) -&gt; None:
        &#34;&#34;&#34;vendor specific frame&#34;&#34;&#34;
        self.sensor = self.bytes[:brk].decode()
        assert self.sensor[3:6] == &#34;PHA&#34;
        data = self.bytes[brk + 1 :].split(sep)
        self.time = datetime.strptime(data[1].decode(), &#34;%Y%j&#34;) + timedelta(
            hours=float(data[2].decode())
        )
        self.data = {key: value.decode() for key, value in zip(keys, data[3:])}

    def by_key(self, frames: dict, headers: dict):
        &#34;&#34;&#34;sort frames by sensor id&#34;&#34;&#34;
        sn = int(self.label[-4:])
        pattern = self.bytes[:10].decode()
        if pattern[:3] == &#34;SAT&#34;:
            pattern = pattern

        key = [
            headers[sn][key]
            for key in headers[sn].keys()
            if pattern in headers[sn][key]
        ][0]
        loc = self.bytes.find(key.encode())
        buffer = self.bytes[loc + len(key) :]
        fr = [
            frames[sn][key][&#34;SensorFieldGroup&#34;]
            for key in frames[sn].keys()
            if pattern in headers[sn][key]
        ][0]

        binary = ((True for key in each.keys() if (&#34;Binary&#34; in key)) for each in fr)
        dat, extra = (Frame.binary_xml if any(binary) else Frame.ascii_xml)(buffer, fr)
        loc = extra.find(b&#34;\r\n&#34;)

        if self.data is None:
            self.data = []

        self.data = {
            &#34;content&#34;: dat,
            &#34;ts&#34;: TimeStamp.parseBinary(extra[loc + 2 : loc + 9]),
            &#34;type&#34;: &#34;sensor&#34;,
        }
        self.bytes = extra[loc + 9 :]
        self.size: len(self.bytes)

    def analog(self, headers: dict, width: int = 35, key: str = &#34;STORX&#34;):
        &#34;&#34;&#34;
        Parse analog frame

        :param frame: dictionary frame
        :param frames: dataframe description
        :param width: width of frame
        :param key: search pattern
        :return: updated frame
        &#34;&#34;&#34;
        sn = int(self.key[-4:])
        buffer = self.bytes[10:width]
        f = headers[sn].goto(key)
        values, extra = Frame.binary_xml(buffer, f)
        self.update(values)
        self.ts = TimeStamp.parseBinary(extra[:7])
        self.bytes = self.bytes[width:]
        self.size = len(self.bytes)

    def gps(self, headers: dict, key: bytes = b&#34;$GPRMC&#34;):
        &#34;&#34;&#34;Decode bytes as GPGGA or GPRMC location stream&#34;&#34;&#34;
        sn = int(self.key[-4:])
        loc = self.bytes.find(key)
        if loc == -1:
            return
        buffer = self.bytes[loc + len(key) + 1 :]
        f = headers[sn].goto(&#34;MODEM&#34;)
        nav, extra = Frame.ascii_xml(buffer, f)
        self.data = {
            &#34;content&#34;: nav,
            &#34;ts&#34;: TimeStamp.parseBinary(extra[2:9]),
            &#34;type&#34;: &#34;nav&#34;,
        }
        self.bytes = extra[9:]
        self[&#34;size&#34;] = len(self.bytes)

    @staticmethod
    def line(txt: str, bytes_string: bytes):
        &#34;&#34;&#34;Single line&#34;&#34;&#34;
        keys = [b&#34;SAT&#34;, b&#34;WQM&#34;]
        lines = txt.split(bytes_string, keys)
        results = []
        for each in lines:
            result = Frame()
            result.raw = each
            try:
                data, ts = each.split(b&#34;\r\n&#34;)
                result.ts = TimeStamp(ts)
            except ValueError:
                data = each
                result.ts = None

            result.bytes = data
            results.append(result)
        return results

    @staticmethod
    def ascii_xml(buffer: str, frames: list):
        &#34;&#34;&#34;XML&#34;&#34;&#34;
        result = dict()
        offset = 0
        delims = [each[&#34;SensorField&#34;][&#34;Delimiter&#34;].encode() for each in frames]
        for each, sep in zip(frames[:-1], delims[1:]):
            loc = buffer.find(sep, offset)
            count = loc - offset
            wd = count + 1
            name = each[&#34;Name&#34;]
            result[name] = buffer[offset:loc]
            offset += wd
        end = offset + 2
        result[frames[-1][&#34;Name&#34;]] = buffer[offset:end]
        return result, buffer[end:]

    @staticmethod
    def binary_xml(buffer: bytes, frames: list, byteorder: str = &#34;&gt;&#34;) -&gt; (dict, bytes):
        &#34;&#34;&#34;
        Parse raw bytes according to format described in XML frames
        &#34;&#34;&#34;
        result = dict()
        offset = 0
        wc = {&#34;BF&#34;: 4, &#34;BD&#34;: 8, &#34;BS&#34;: 1, &#34;BU&#34;: 1, &#34;AF&#34;: 1, &#34;BULE&#34;: 1, &#34;BSLE&#34;: 1}

        for each in frames:
            keys = each.keys()
            dtype_key = [
                key for key in keys if (&#34;Binary&#34; in key and &#34;Data&#34; in key)
            ].pop()

            if &#34;BinaryFloatingPoint&#34; in dtype_key:
                txt = each[dtype_key]
                wd = wc[txt]
                np_type = byteorder + &#34;f&#34; + str(wd)

            elif &#34;BinaryInteger&#34; in dtype_key:
                txt = each[dtype_key][&#34;Type&#34;]
                wd = wc[txt] * int(each[dtype_key][&#34;Length&#34;])
                np_type = byteorder + &#34;u&#34; + str(wd)

            else:
                break

            name = each[&#34;Name&#34;]
            result[name] = frombuffer(buffer, dtype=np_type, count=1, offset=offset)
            offset += wd

        return result, buffer[offset:]

    def storx(
        self, fields: (Field,), name_length: int = 10, verb: bool = False
    ) -&gt; None:
        &#34;&#34;&#34;
        Decode and process Satlantic sensor frames if format is known, or fail silently

        :param frame: incoming frame dictionary structure
        :param fields: field mappings for known sensor formats
        :param name_length: maximum size for name search pattern, 10 is Satlantic standard
        :param verb: verbose mode

        :return: possibly processed frame
        &#34;&#34;&#34;

        delim = {
            &#34;PHA&#34;: b&#34;,&#34;,
            &#34;CST&#34;: b&#34;\t&#34;,
        }  # SEAFET pH instrument  # CSTAR transmissometer

        brk = self.bytes.find(b&#34;\t&#34;)
        if brk == -1 or brk &gt; name_length:
            brk = self.bytes.find(b&#34;\x00&#34;)
            if brk &gt; name_length:
                print(&#34;Error. Instrument name appears to be too long:&#34;, self.bytes[:32])
                return  # return unmodified frame

        self.sensor = self.bytes[:brk].decode()
        self.time = None
        self.data = None

        sensor = self.sensor[3:6]
        try:
            sep = delim[sensor]
        except KeyError:
            pass  # just copy bytes
        else:
            start = 1
            rest = self.bytes[brk + 1 :].split(sep)
            if sensor == &#34;PHA&#34;:
                self.seafet(brk, fields[sensor])
            else:
                try:
                    keys = fields[sensor]
                except KeyError:
                    self.data = rest[start:]
                else:
                    self.data = {
                        key: value.decode() for key, value in zip(keys, rest[start:])
                    }

        if verb and self.data.__class__.__name__ == &#34;dict&#34;:
            print(self.sensor, &#34;[&#34;, self.time, &#34;] ::&#34;, self.data)

    @staticmethod
    def parse_buffer_queue(
        queue: deque, sequence: list, pool: Pool, frames: list
    ) -&gt; (list, list):
        &#34;&#34;&#34;
        Create a job queue and use pool of workers to process byte strings until consumed
        &#34;&#34;&#34;
        processed = deque()
        for job in sequence:
            queue = pool.starmap(job, zip(queue, repeat(frames, len(queue))))
            processed.append(
                queue.pop(buffer) for buffer in queue if buffer[&#34;size&#34;] == 0
            )

        return processed, queue

    @staticmethod
    def _tree_depth(xml: str) -&gt; int:
        &#34;&#34;&#34;
        Get depth of tree
        &#34;&#34;&#34;

        class _Parser:
            maxDepth = 0
            depth = 0

            def start(self, tag, attrib):
                self.depth += 1
                if self.depth &gt; self.maxDepth:
                    self.maxDepth = self.depth

            def end(self, tag):
                self.depth -= 1

            def close(self):
                return self.maxDepth

        parser = ElementTree.XMLParser(target=_Parser())
        parser.feed(xml)
        return parser.close()

    def parse_xml_frames(
        self,
        config: dict,
        key: str = &#34;sensor&#34;,
        depth: int = 10,
        verb: bool = False
    ) -&gt; dict:
        &#34;&#34;&#34;
        Get frames for all sensors on platform

        :param config: xml style dictionary format with all configuration data for sensor platform
        :param key: key for configured items
        :return: dictionary of with sensors as keys, and dataframe schema as value
        &#34;&#34;&#34;

        def _goto(item):
            &#34;&#34;&#34;
            Start node of frame
            &#34;&#34;&#34;
            sensor = root.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;sensor&#34;] + &#34;&#39;]&#34;)[0]
            frame = sensor.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;frame&#34;] + &#34;&#39;]&#34;)[0]
            if verb:
                print(
                    &#34;Parsing from: . &gt;&#34;,
                    sensor.identifier,
                    &#34;&gt;&#34;,
                    self.identifier,
                )
            return frame

        ns = &#34;{http://www.satlantic.com/instrument}&#34;
        root = ElementTree.fromstring(config[&#34;xml&#34;][&#34;content&#34;])
        return {
            item[key]: Frame._collect(_goto(item), depth=depth, namespace=ns, verb=verb)
            for item in config[&#34;config&#34;][&#34;content&#34;]
        }

    @staticmethod
    def parse_xml(xml, depth=None, verb=False):
        &#34;&#34;&#34;
        Recursively collect XML sensor info as dict
        &#34;&#34;&#34;
        return Frame._collect(
            node=ElementTree.fromstring(xml),
            depth=depth if depth else Frame._tree_depth(xml),
            namespace=&#34;{http://www.satlantic.com/instrument}&#34;,
            verb=verb,
        )

    @staticmethod
    def _collect(
        node: ElementTree,
        depth: int,
        count: int = 0,
        namespace: str = None,
        verb: bool = False,
    ) -&gt; dict or None:
        &#34;&#34;&#34;
        Recursively collect child nodes and info.
        &#34;&#34;&#34;
        collector = dict()
        if count &gt;= depth:
            return None

        for child in node:
            below = Frame._collect(child, depth, count=count + 1, namespace=namespace)
            tag = sub(namespace, &#34;&#34;, child.tag)
            if below is None:
                collector[tag] = child.text
                continue

            queue = collector.get(tag, None)
            if queue is None:
                queue = collector[tag] = []
            queue.append(below)
            if verb:
                print(&#34;\t&#34; * count + &#34;&gt;&#34;, tag + &#34;:&#34;, collector[tag])

        return collector


class Graph:
    &#34;&#34;&#34;
    Abstract graphDB 
    &#34;&#34;&#34;
    @staticmethod
    def register(config: dict):

        if config[&#34;join&#34;] and not config[&#34;graph&#34;]:
            hosts = config[&#34;join&#34;].copy()
            while hosts:
                host = hosts.pop()
                response = get(config[&#34;graphHealthcheck&#34;].format(host))
                if response.ok:
                    config[&#34;graph&#34;] = host
                    break

        if config[&#34;graph&#34;] is not None:
            try:
                register = post(
                    config[&#34;graphAuth&#34;].format(config[&#34;graph&#34;]),
                    json={
                        &#34;email&#34;: config[&#34;graphUser&#34;],
                        &#34;password&#34;: config[&#34;graphPassword&#34;],
                        &#34;apiKey&#34;: config[&#34;graphApiKey&#34;],
                    },
                )
            except (ConnectionError, MaxRetryError):
                config[&#34;graph&#34;] = None
            else:
                assert register.ok

    @staticmethod
    def create(cls, obj: dict, url: str, token: str) -&gt; tuple or None:
        url = f&#34;{url}/{cls}&#34;
        return post(url=url, json=obj, headers={&#34;Authorization&#34;: f&#34;Bearer {token}&#34;})


@attr.s
class Interval:
    &#34;&#34;&#34;Intervals are convenience data structs for sorting and numerical queries&#34;&#34;&#34;
    lower: Bound = attr.ib(default=None)
    upper: Bound = attr.ib(default=None)


    def overlaps(self, other: Interval) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.upper.value and 
            self.upper.value &gt;= other.lower.value
        )


    def __contains__(self, other: Interval):
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.lower.value and 
            self.upper.value &gt;= other.upper.value
        )


class JSONIOWrapper(TextIOWrapper):
    &#34;&#34;&#34;
    Use JSON messages piped between between processes
    &#34;&#34;&#34;
    @staticmethod
    def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
        &#34;&#34;&#34;
        Log notifications.

        :param message: some event notification
        :param data: data that resulted in message
        :param log: log file or interface
        :param arrow: symbol indicating direction of flow

        :return:
        &#34;&#34;&#34;
        timestamp = datetime.now().isoformat(sep=&#34; &#34;)
        string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
        if log is not None:
            log.write((string + &#34;\n&#34;).encode())
            return None
        print(string)

    def receive(self, log: BytesIO) -&gt; dict:
        &#34;&#34;&#34;
        Receive serialized data from command line interface.
        &#34;&#34;&#34;
        json = self.readline()
        self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
        try:
            data = loads(json.rstrip())
        except decoder.JSONDecodeError as decode_error:
            self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
            message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
            return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

        return data

    def send(self, data: dict, log: BytesIO) -&gt; None:
        &#34;&#34;&#34;
        Write serialized data to interface.
        &#34;&#34;&#34;

        def _transform():
            safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
            return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

        json = _transform()
        self.log(message=&#34;Send&#34;, data=json, log=log)
        self.write(f&#34;{json}\n&#34;)

    def dump(self) -&gt; None:
        &#34;&#34;&#34;
        Propagates messages up through C#, subprocess, and control layers.
        &#34;&#34;&#34;
        response = self.readline()
        while response != &#34;&#34;:
            response = self.readline()
            print(response.rstrip())


@attr.s
class KernelDensityEstimator(KernelDensity):
    &#34;&#34;&#34;Predict events in space&#34;&#34;&#34;
    @staticmethod
    def glm():
        &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
        return LinearRegression()  

    @staticmethod
    def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
        &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
        epsilon = mesh.fields[key]
        field = mesh.nodes.xye(epsilon)
        target = mesh.interp2d(xx, yy, epsilon)  # location suitability

        return field, target

    def intensity(self, field: object):
        &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
        intensity = self.score_samples(field)  # create intensity field
        maximum = intensity.max()
        minimum = intensity.min()
        cost = (intensity - minimum) / (maximum - minimum)

        return intensity, cost

    @staticmethod
    def train(self, target: iter, field: object, xx: iter, yy: iter):
        &#34;&#34;&#34;
        Train kernel density estimator model using a quantized mesh

        :param mesh: Mesh object of the Interpolator super type
        :param key: Spatial field to train on
        :return:
        &#34;&#34;&#34;
        subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
        self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
        return self.intensity(field)

    @staticmethod
    def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
        &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

        xnew = []
        ynew = []

        def prohibit():
            &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
            xtemp = array(xin + xnew)
            ytemp = array(yin + ynew)
            dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
            nearest = dxy.min()
            return nearest &lt; 0.5 * bandwidth

        xmin, ymin = transform(view, native, extent[0], extent[1])
        xmax, ymax = transform(view, native, extent[2], extent[3])

        total = 0
        passes = 0
        while total &lt; count and passes &lt; count * 10:

            sample = kde.sample()
            xx = sample[0][0]
            yy = sample[0][1]

            if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

                if bandwidth is not None and prohibit():
                    xnew.append(xx)
                    ynew.append(yy)
                    total += 1

                else:
                    passes += 1


class LinkedListNode:
    &#34;&#34;&#34;Node in linked list&#34;&#34;&#34;
    def __init__(self, value):
        &#34;&#34;&#34;create a node&#34;&#34;&#34;
        self.next = None
        self.prev = None
        self.value = value


class LinkedList:
    &#34;&#34;&#34;LL abstraction&#34;&#34;&#34;
    def __init__(self, data: (float,) = ()):

        self.head = None
        prev = None
        for value in data:
            n = LinkedListNode(value)
            if prev is None:
                self.head = n
            else:
                prev.next = n
            prev = n

        self.tail = prev

    def traverse(self) -&gt; None:
        &#34;&#34;&#34;Move across nodes&#34;&#34;&#34;
        cursor = self.head
        while cursor is not None:
            print(cursor.value)
            cursor = cursor.next

    def deduplicate(self):
        &#34;&#34;&#34;Remove duplicates&#34;&#34;&#34;
        cursor, last, exists = self.head, None, set()
        while cursor is not None:
            if last is not None and cursor.value in exists:
                last.next = cursor.next.next if cursor.next is not None else None
            else:
                exists |= {cursor.value}
            last, cursor = cursor, cursor.next
        return last

    def k_from_head(self, k: int) -&gt; None or LinkedListNode:
        &#34;&#34;&#34;Get selected&#34;&#34;&#34;
        cursor = self.head
        while cursor.next is not None and k:
            cursor = cursor.next
            k -= 1
        return cursor.value

    def k_from_end(self, k: int) -&gt; None or LinkedListNode:
        cursor = self.head
        total = -k
        while cursor is not None:
            cursor = cursor.next
            total += 1

        assert total &gt; 0

        cursor = self.head
        while cursor is not None and total:
            cursor.next = cursor.next
            total -= 1
        return cursor.value

    def prepend(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.next, self.head = self.head, n

    def append(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        if self.head is None:
            self.head = n
        if self.tail is not None:
            self.tail.next = n
        self.tail = n

    def add(self, other):
        ...


class DoublyLinkedList(LinkedList):
    prev = None  # only for doubly-linked

    def __init__(self, data: (float,) = ()):
        LinkedList.__init__(self, data)
        cursor = self.head
        while cursor.next is not None:
            cursor.next.prev = cursor

    def k_from_end(self, n: int = None) -&gt; None or LinkedListNode:

        _next = self.tail
        _last = None
        while _next is not None and (n is None or n):
            _last = _next
            _next = _next.prev
            if n:
                n -= 1
        return _last

    def traverse_backward(self) -&gt; None:
        cursor = self.tail
        while cursor is not None:
            print(cursor.value)
            cursor = cursor.prev

    def push_front(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.next = self.head
        self.head.prev = n
        self.head = n

    def push_back(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.prev = self.tail
        if self.head is None:
            self.head = n
        if self.tail is not None:
            self.tail.next = n
        self.tail = n

    def insert_after(self, insert: LinkedListNode, ref: LinkedListNode):
        ...

    def insert_before(self, insert: LinkedListNode, ref: LinkedListNode):
        ...


class Memory:
    def __init__(self, size, max_size=int(1e6)):
        # type: (int, int) -&gt; None
        &#34;&#34;&#34;
        Memory manager class for allocating and freeing bytes string, only implements contiguous chunks.
        &#34;&#34;&#34;
        if not isinstance(size, int):
            raise TypeError
        if size &gt; max_size:
            raise MemoryError

        self.buffer = zeros(size, dtype=bytes)
        self.mask = zeros(size, dtype=bool)
        self.map = dict()
        self.remaining = size
        self._count = 0

    def alloc(self, size):
        # type: (int) -&gt; int
        &#34;&#34;&#34;
        Allocate and return a fixed length buffer. Raise error if out of memory.
        &#34;&#34;&#34;
        if self.remaining &lt; size:
            raise MemoryError

        # find indices of sufficient free memory, return pointers
        # optionally shuffle memory to create contiguous blocks
        self._count += 1
        self.remaining -= size

        start = self._find(size)
        if start is None:
            raise MemoryError

        ptr = self.buffer[start : start + size]
        self.map[self._count] = {&#34;mask&#34;: arange(start, start + size), &#34;data&#34;: ptr}
        return self._count

    def set(self, key, values):
        # type: (int or str, bytes) -&gt; None
        &#34;&#34;&#34;
        Set buffer to specified values, or singleton
        &#34;&#34;&#34;
        self.map[key][&#34;data&#34;][:] = values

    def data(self, key):
        # type: (int or str) -&gt; bytes
        &#34;&#34;&#34;Return data&#34;&#34;&#34;
        return self.map[key][&#34;data&#34;]

    def free(self, key):
        # type: (int or str) -&gt; bool
        &#34;&#34;&#34;
        Free previously allocated variable
        &#34;&#34;&#34;
        try:
            indices = self.map[key][&#34;mask&#34;]  # get indices from memory map dict
            # reset mask and increment available memory
            self.mask[indices] = False
            self.remaining += len(indices)
            del key

        except (MemoryError, TypeError):
            return False
        else:
            return True

    def _find(self, size):
        # type: (int) -&gt; int or None
        &#34;&#34;&#34;Find the starting index of the first available contiguous chunk&#34;&#34;&#34;
        start = 0
        while True:
            offset = 1
            if not self.mask[start]:
                while not self.mask[start + offset] and offset &lt;= size:
                    if offset == size:
                        return start
                    else:
                        offset += 1
            else:
                start += 1

            if start == len(self.mask) - size:
                return None


    @staticmethod
    def cache(data, path, free=False):
        # type: (bytes, str, bool) -&gt; int
        fid = open(path, &#34;wb+&#34;)  # open pickled file to read
        dump(data, fid)  # save array
        fid.close()
        if free:
            del data
        return len(data)


    @staticmethod
    def vertex_array_buffer(data, dataset, key, strategy, sequential=False, nb=None, headers=None):
        # type: (deque or (Array, ), str, str, str, bool, float, dict) -&gt; set
        &#34;&#34;&#34;
        Take an iterable of arrays, and chunk them for upload.

        :param data: deque or iterable
        :param dataset: prefix for object storage
        :param key: key for object storage
        :param strategy: how to chunk (aggregate or bisect)
        :param sequential: create an index if False
        :param nb: max number of bytes
        :param headers: headers!
        &#34;&#34;&#34;
        _data = data if isinstance(data, deque) else deque(data)
        if strategy not in (&#34;aggregate&#34;, &#34;bisect&#34;):
            raise ValueError
        if strategy == &#34;aggregate&#34; and nb is None:
            raise ValueError

        last = 0
        indx = 0
        real = len(_data)
        index = set()

        while _data:
            current = int(100 * indx / real)
            if current != last:
                print(current, &#34;%&#34;)

            c = ()
            if strategy == &#34;aggregate&#34;:
                size = 0
                while size &lt; nb and _data:
                    c += (_data.popleft(),)
                    size += c[-1].nbytes
            if strategy == &#34;bisect&#34;:
                c += (_data.popleft(),)

            _key = f&#34;{key}-{indx}&#34; if sequential else None
            ext = reduce(reduce_extent, (extent(*s) for s in c))

            try:
                assert False  # post here
            except SignatureDoesNotMatch:
                to_append = ()
                if strategy == &#34;bisect&#34;:
                    to_append = array_split(c[0], 2, axis=0)
                if strategy == &#34;aggregate&#34;:
                    tilt = len(c) // 2 + 1
                    to_append = c[:tilt], c[tilt:]
                _data.extend(to_append)
                real += 1
            else:
                index |= {_key}
                indx += 1

        return index


    @staticmethod
    def parts(dataset, key):
        part = 0
        result = []
        while True:
            k = f&#34;{dataset}/{key}-{part}&#34;
            stat = head(k)
            if stat is None:
                break
            result.append(k)
            part += 1
        return result


    @staticmethod
    def restore(dataset, key, fcn=None, sequential=True, stack=False, limit=None, **kwargs):
        # type: (str, str, Callable, bool, bool, int, dict) -&gt; (Array, ) or Array
        &#34;&#34;&#34;
        Reconstruct a single or multi-part array dataset

        :param dataset: object storage prefix
        :param key: object name, lat part
        :param fcn: method to perform on
        :param sequential: use a sequential naming scheme rather than an index file
        :param stack: append all array chunks into one
        :param limit: max number to process
        :param kwargs: arguments for the function

        :return: transformed array, or none, if the method return no results
        &#34;&#34;&#34;
        base = f&#34;{dataset}/{key}&#34;
        stat = head(base)
        if stat is None and not sequential:
            raise ValueError

        if stat is not None:
            if sequential:
                for s in unpickle(get(base).content):
                    fcn(s, **kwargs)
                return
        elif sequential:
            raise ValueError

        index = (
            Memory.parts(dataset, key) if sequential else
            tuple(f&#34;{dataset}/{key}&#34; for key in load_json(get(base)))
        )

        if len(index) == 0:
            raise ValueError

        vertex_array_buffer = ()
        part = 0
        for key in index:
            if part &gt; limit:
                break
            c = unpickle(get(key).content)
            if isinstance(c, list):
                c = tuple(c)
            if not isinstance(c, tuple):
                raise TypeError

            part += 1
            if fcn is None:
                vertex_array_buffer += c
                continue

            y = (fcn(x[0] if isinstance(x, tuple) else x, **kwargs) for x in c)
            vertex_array_buffer += tuple(yi for yi in y if yi is not None)

        if not len(vertex_array_buffer):
            return None
        if stack:
            return vstack(vertex_array_buffer)
        return vertex_array_buffer


@attr.s
class Mesh:
    &#34;&#34;&#34;
    Meshes are collections of points and their topology
    &#34;&#34;&#34;
    vertex_array: VertexArray = attr.ib(default=None)
    topology: Topology = attr.ib(default=None)  # Topology


    def cell_normals(self) -&gt; Array:
        &#34;&#34;&#34;Normals of faces&#34;&#34;&#34;
        topo = self.topology
        uu = self.vertex_array[topo[:, 1], :] - self.vertex_array[topo[:, 0], :]
        vv = self.vertex_array[topo[:, 2], :] - self.vertex_array[topo[:, 0], :]
        return cross(uu, vv)


    def vertex_normals(self, s: float = 0.05) -&gt; Array:
        &#34;&#34;&#34;
        Add vertex list to batch for rendering
        &#34;&#34;&#34;
        f = self.cell_normals()
        assert f.shape == (self.topology.size, 3)
        v = f[self.topology, :]
        assert v.shape[:2] == (self.topology.size, 3)
        assert 3 &lt;= v.shape[2] &lt;= 4
        v_avg = v.mean(axis=1)
        assert v_avg.shape == (self.vertex_array.size, 3)
        return vstack((self.vertex_array, s * normal(v_avg) + self.vertex_array))


    def adjacency(self) -&gt; Array:
        # type: (Array, Array) -&gt; Array
        &#34;&#34;&#34;
        Calculate adjacent vertices
        &#34;&#34;&#34;
        adj = []
        for ii, _ in enumerate(self.vertex_array):
            rows, _ = where(self.topology == ii)
            uni = unique(self.topology[rows, :])
            new_adj = uni[where(uni != ii)]
            adj.append(new_adj)
        return adj


class ObjectStorage(Minio):
    def __init__(self, bucket_name: str, endpoint: str, prefix: str = None, **kwargs):
        self.bucket_name = bucket_name
        self.prefix = prefix
        self.endpoint = endpoint
        
        super().__init__(endpoint, **kwargs)
        if not self.bucket_exists(bucket_name):
            self.make_bucket(bucket_name)

    @property
    def locked(self) -&gt; bool:
        return self.stat_object(&#34;lock.json&#34;) is not None

    def publish_events(self, pubsub_channel: str):
        fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
        with StrictRedis() as queue:
            for event in self.listen_bucket_notification(
                self.bucket_name, &#34;&#34;, None, fcns
            ):
                queue.publish(pubsub_channel, str(event))

    def stat_object(self, object_name: str):
        &#34;&#34;&#34;
        Determine whether an object key exists
        &#34;&#34;&#34;
        try:
            return super().stat_object(self.bucket_name, object_name)
        except NoSuchKey:
            return None

    def list_objects(self, prefix: str = None):
        return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))

    def put_object(
        self,
        object_name: str,
        data: dict or bytes,
        metadata: dict = None,
        codec: str = &#34;utf-8&#34;,
    ) -&gt; str:
        &#34;&#34;&#34;
        Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

        :param label: label for file
        :param data: data to serialize
        :param metadata: headers
        :param codec: how to encode strings
        &#34;&#34;&#34;
        if isinstance(data, dict):
            content_type = &#34;application/json&#34;
            buffer = bytes(dumps(data).encode(codec))
        elif isinstance(data, bytes):
            content_type = &#34;text/plain&#34;
            buffer = data
        else:
            raise TypeError

        accumulate = []
        given_parts = object_name.split(&#34;/&#34;)
        prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
        if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
            for pp in prefix_parts:
                if pp not in given_parts:
                    accumulate.append(pp)
            accumulate.extend(given_parts)
            object_name = &#34;/&#34;.join(accumulate)

        super().put_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            data=BytesIO(buffer),
            length=len(buffer),
            metadata=metadata,
            content_type=content_type,
        )

        return object_name

    def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
        &#34;&#34;&#34;
        Download the data, may be streaming if desired
        &#34;&#34;&#34;
        data = super().get_object(self.bucket_name, object_name)
        if stream:

            def generate():
                for d in data.stream(32 * 1024):
                    yield d

            result = generate
        else:
            result = data

        return Response(result, mimetype=&#34;application/octet-stream&#34;)

    def updateIndex(
        self,
        object_name: str,
        metadata: dict = None,
        entries: [dict] = None,
        props: dict = None,
    ):
        &#34;&#34;&#34;
        Update contents of index metadata
        &#34;&#34;&#34;

        if entries:
            self.put_object(
                object_name=object_name,
                data={
                    **loads(self.get_object(object_name=object_name).data),
                    **(entries or {}),
                    **(props or {}),
                },
                metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
            )
        else:
            self.copy_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                object_source=object_name,
                metadata=metadata,
            )

        return self

    def delete(
        self, 
        prefix: str, 
        batch: int = 10, 
        conditions: dict = None
    ) -&gt; (Any):
        &#34;&#34;&#34;
        Delete all objects within a subdirectory or abstract collection.

        The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
        default, and therefore needs to be iterated through before returning any errors. 

        :param prefic: file prefix/dataset
        :param batch:  number to delete at a time
        &#34;&#34;&#34;
        remove = ()
        errors = ()
        
        objects_iter = self.list_objects(prefix=prefix)
        stop = False
        while not stop:
            try:
                object_name = next(objects_iter).object_name
            except StopIteration:
                stop = True
            else:
                stat = self.stat_object(object_name)
                if isinstance(conditions, dict):
                    if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                        remove += (object_name,)
                else:
                    remove += (object_name,)

            if len(remove) &gt;= batch or stop:
                for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                    errors += (error,)
                return errors


    @staticmethod
    def metadata_template(
        file_type: str = None, parent: str = None, headers: dict = None
    ) -&gt; dict:

        accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

        return {
            &#34;x-amz-acl&#34;: accessControl,
            &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
            &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
            &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
            &#34;x-amz-meta-service-file-type&#34;: file_type,
            **(headers or {}),
        }

    def unlock(self, object_name: str, session: str = None,) -&gt; bool:
        &#34;&#34;&#34;
        Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
        &#34;&#34;&#34;
        try:
            self.remove_object(self.bucket_name, object_name)
        except NoSuchKey:
            return False
        return True

    def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
        &#34;&#34;&#34;
        Object storage locking decorator for functions.

        When used this implements a mutex lock on the object path,
        which will block competing operations until it is cleared.

        Locks will not block read operations except in special cases. 
        &#34;&#34;&#34;

        # index = load_json(self.get_object(object_name=index_file))

        headers = {}
        session_id = uuid4().hex
        name = &#34;bathysphere&#34;
        lock_file = f&#34;{name}/lock.json&#34;
        # index_file = f&#34;{name}/index.json&#34;

        def decorator(fcn):
            &#34;&#34;&#34;
            Methods applied to the wrapped function
            &#34;&#34;&#34;

            def wrapper(*args, **kwargs):
                &#34;&#34;&#34;
                Actual wrapper that calls the decorated function
                &#34;&#34;&#34;
                if self.stat_object(lock_file):
                    return &#34;Lock in place&#34;, 500
                try:
                    self.put_object(
                        object_name=lock_file,
                        data={&#34;session&#34;: session_id},
                        metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                    )
                except NoSuchKey:
                    return &#34;Could not lock repository&#34;, 500
                try:
                    result = fcn(*args, **kwargs)
                except Exception as ex:
                    result = f&#34;{ex}&#34;, 500
                finally:
                    if lock and not self.unlock(object_name=lock):
                        result = &#34;Failed to unlock&#34;, 500
                return result

            return wrapper

        return decorator


class PostgresType(Enum):
    Numerical = &#34;DOUBLE PRECISION NULL&#34;
    TimeStamp = &#34;TIMESTAMP NOT NULL&#34;
    Geography = &#34;GEOGRAPHY NOT NULL&#34;
    IntIdentity = &#34;INT PRIMARY KEY&#34;
    NullString = &#34;VARCHAR(100) NULL&#34;


@attr.s
class Query:

    sql: str = attr.ib()
    parser: Callable = attr.ib()


@attr.s
class Reactor:

    systems: () = attr.ib()
    mesh: None = attr.ib()

    def update(self, volume: array):
        &#34;&#34;&#34;
        Transfer mass from difference equation to conservative arrays
        &#34;&#34;&#34;
        assert all(each.transfer(conversion=volume) for each in self.systems.values())

    def integrate(
        self, anomaly, nutrients, carbon, oxygen, phyto_c=0.0, phyto_n=0.0, volume=1.0
    ) -&gt; None:
        &#34;&#34;&#34;
            
        Update difference equations for internal, temperature-dependent chemistry.

        :param anomaly: temperature anomaly (usually T-20)
        :param carbon: required chemistry instance
        :param oxygen: required chemistry instance
        :param nutrients: optional list of nutrients to track
        :param phyto_c: carbon supplied by biology
        :param phyto_n: nitrogen supplied by biology
        &#34;&#34;&#34;
        limit = carbon.integrate(
            anomaly, oxygen, phyto_c
        )  # available carbon as proxy, consumes oxygen
        assert oxygen.integrate(limit, anomaly)  # oxygen consumption

        assert all(nutrient.mineralize(limit, anomaly) for nutrient in nutrients)

        for each in nutrients:
            if each.__class__.__name__ == &#34;Nitrogen&#34;:
                assert each.integrate(
                    oxygen, carbon, phyto_n, anomaly
                )  # consumes oxygen and carbon
                break

        self.update(volume)


@attr.s
class RecurrentNeuralNetwork:

    hidden = attr.ib()  # hidden layers
    periods = attr.ib()  # previous steps to use for prediction
    epochs = attr.ib()  # number of training cycles
    rate = attr.ib()  # learning rate
    model = attr.ib(default=None)
    output = attr.ib(default=1)  # out put dimensions
    horizon = attr.ib(default=1)  # future steps to predict
    input = attr.ib(default=1)
    file = attr.ib(default=None)  # path for persisting data

    # saver = tf.train.Saver

    @property
    def shape(self):
        return (-1, self.periods, 1)

    @staticmethod
    def from_cache(fcn):
        def wrapper(*args, **kwargs):

            cache = kwargs.pop(&#34;cache&#34;)
            key = kwargs.pop(&#34;objectKey&#34;)

            # check connection
            # check key

            try:
                request.model = tf.keras.models.load_model(
                    &#34;/&#34;.join([&#34;&#34;, cache, key])
                )  # load and inject object
            except:
                return 500, {&#34;message&#34;: &#34;Server error while loading model&#34;}

            return fcn(*args, **kwargs)

        return wrapper

    @classmethod
    def createAndCache(
        cls,
        stateful: bool,
        horizon: int,
        layers: int,
        batch_size: int,
        cache: str,
        key: str,
    ):
        &#34;&#34;&#34;
        Create and cache the model structure
        &#34;&#34;&#34;
        neural_net = cls.create(stateful, horizon, layers, batch_size)
        neural_net.save(&#34;/&#34;.join([&#34;&#34;, cache, key]))
        return 200, {&#34;message&#34;: &#34;model created and cached&#34;}

    @classmethod
    def train_cached_model(
        cls,
        datastream,
        window: int,
        horizon: int,
        batch_size: int,
        ratio: float,
        periods: int,
        epochs: int,
    ):
        &#34;&#34;&#34;
        Load and feed the model training data
        &#34;&#34;&#34;
        datasets = datastream.partition(window, horizon, batch_size, ratio, periods)

        cls.train(
            request.model,
            batch_size=batch_size,
            training=datasets.get(&#34;training&#34;),
            epochs=epochs,
            validation=datasets.get(&#34;validation&#34;),
        )

        return 200, {&#34;message&#34;: &#34;model trained and cached&#34;}

    @staticmethod
    def get_prediction(datastream: object, batch_size: int = 32):
        &#34;&#34;&#34;

        :param datastream:
        :param batch_size: Number of samples per gradient update
        :return:
        &#34;&#34;&#34;
        predicted = request.model.predict(
            datastream, batch_size=batch_size, workers=1, use_multiprocessing=False
        )
        return 200, {&#34;payload&#34;: predicted.flatten()}

    @staticmethod
    def train(model, batch_size: int, training: dict, validation: dict, epochs: int):

        for i in range(epochs):
            print(&#34;Epoch&#34;, i + 1, &#34;/&#34;, epochs)
            model.fit(
                training[&#34;x&#34;],
                training[&#34;y&#34;],
                batch_size=batch_size,
                epochs=1,  # different &#34;epochs&#34;
                verbose=False,
                validation_data=(validation[&#34;x&#34;], validation[&#34;y&#34;]),
                shuffle=False,  # order is important
                workers=1,
                use_multiprocessing=False,
            )

            model.reset_states()

    @staticmethod
    def create(
        stateful: bool, horizon: int, units: int, batch_size: int, variables: int = 1
    ):
        &#34;&#34;&#34;
        Create the Keras-Tensorflow model

        :param horizon: sequence length for training each output point
        :param stateful: boolean, better if true
        :param units: number of LSTM
        :param batch_size:
        :param variables: number of input variables

        :return: model instance
        &#34;&#34;&#34;

        model = tf.keras.models.Sequential()
        model.add(
            tf.keras.layers.LSTM(
                units=units,
                input_shape=(horizon, variables),
                batch_size=batch_size,
                stateful=stateful,
            )
        )
        model.add(tf.keras.layers.Dense(1))
        model.compile(loss=&#34;mse&#34;, optimizer=&#34;adam&#34;)

        return model

    @staticmethod
    def series(n=365, show=False):
        &#34;&#34;&#34;
        Create a synthetic training data set

        :param n: steps
        :param show: plot for time series

        :return: numpy array of magnitudes
        &#34;&#34;&#34;
        rng = date_range(start=&#34;2018&#34;, periods=n, freq=&#34;D&#34;)
        data = Series(random.normal(0, 0.5, size=len(rng)), rng).cumsum()
        return array(data)

    def test(self, data):
        &#34;&#34;&#34;
        Split data for prediction period.

        :param data:
        :return:
        &#34;&#34;&#34;
        start = self.periods + self.horizon
        setup = data[-start:]
        x = setup[: self.periods].reshape(self.shape)
        y = data[-self.periods :].reshape(self.shape)
        return x, y

    def x_train(self, opt, feed, loss, verb):
        &#34;&#34;&#34;
        Initialize and train the neural net.

        :param opt: optimizer
        :param feed:
        :param loss: skill assessment, in this case mean squared error
        :param verb: verbose mode

        :return: time series of error terms
        &#34;&#34;&#34;
        init = tf.global_variables_initializer()
        err = []

        with Session() as session:
            init.run()

            for ep in range(self.epochs):
                session.run(opt, feed_dict=feed)

                if ep % 100 == 0:
                    mse = loss.eval(feed_dict=feed)
                    err.append(mse)
                    mse = loss.eval(feed_dict=feed)
                    if verb:
                        print(ep, &#34;\tMSE:&#34;, mse)

            self.model.saver().save(session, self.file)
            return err

    def predict(self, predictor, feed):
        &#34;&#34;&#34;
        Restore from file, and make a prediction

        :param predictor: handle for neural net
        :param feed: prediction feed
        :return:
        &#34;&#34;&#34;
        with Session() as session:
            self.model.saver().restore(session, self.file)
            return session.run(predictor, feed_dict=feed)

    def x(self, data):
        &#34;&#34;&#34;
        X data

        :param data:
        :return:
        &#34;&#34;&#34;
        n = len(data)
        end = n - n % self.periods
        subset = data[:end]
        return subset.reshape(self.shape)

    def y(self, data):
        &#34;&#34;&#34;
        Y data

        :param data:
        :return:
        &#34;&#34;&#34;
        n = len(data)
        end = n - n % self.periods

        subset = data[1 : end + self.horizon]
        return subset.reshape(self.shape)

    def predictor(self, X, lstm=True):
        &#34;&#34;&#34;
        Create graph for recurrent neural network

        :param X:
        :param lstm: use long short-term memory cells
        :return:
        &#34;&#34;&#34;

        cell = (
            tf.keras.layers.LSTMCell(units=self.hidden)
            if lstm
            else tf.keras.layers.SimpleRNNCell(units=self.hidden)
        )

        out, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
        stacked = tf.reshape(out, [-1, self.hidden])
        layers = tf.layers.dense(stacked, self.output)
        return tf.reshape(layers, [-1, self.periods, self.output])

    def optimizer(self, err):
        &#34;&#34;&#34;
        Stochastic gradient descent algorithm

        :param err: loss/error function tensor node
        :return:
        &#34;&#34;&#34;
        opt = tf.train.AdamOptimizer(learning_rate=self.rate)
        return opt.minimize(err)

    def nodes(self):
        &#34;&#34;&#34;
        X and y tensor graph nodes

        :return:
        &#34;&#34;&#34;

        X = placeholder(tf.float32, [None, self.periods, self.input])
        Y = placeholder(tf.float32, [None, self.periods, self.output])
        return X, Y

    def feed(self, ts: array, x: array, y: array, xp: array) -&gt; dict:
        &#34;&#34;&#34;
        Format data dictionaries to feed into model

        :param ts: time series
        :param x: X tensor nodes
        :param y: Y tensor nodes
        :param xp: x data for prediction
        :return:
        &#34;&#34;&#34;
        return {&#34;train&#34;: {x: self.x(ts), y: self.y(ts)}, &#34;predict&#34;: {x: xp}}

    @classmethod
    def run(cls, config, lstm: bool = True, verb: bool = True):
        &#34;&#34;&#34;
        Create and run the model

        :param config:
        :param lstm: use memory for drop out

        :return:
        &#34;&#34;&#34;

        network = cls(**config)  # create the neural net

        ts = cls.series()  # synthetic time series

        x, y = network.nodes()  # tensor graph nodes
        xt, yt = network.test(ts)  # split data for skill test

        feed = network.feed(ts, x, y, xt)  # feeds for training and prediction
        predictor = network.predictor(x, lstm=lstm)  # predictor model
        loss = reduce_sum(square(predictor - y))  # mean square error tensor node
        optimizer = network.optimizer(loss)  # learning model — minimize error

        network.model.train(
            optimizer, feed[&#34;train&#34;], loss, verb
        )  # train and get error series over epochs
        prediction = network.predict(predictor, feed[&#34;predict&#34;])  # make prediction
        return yt, prediction


class RelationshipLabels(Enum):
    Self = 1
    Root = 2
    Parent = 3
    Collection = 4
    Derived = 5


@attr.s
class Schema:
    fields: [Field] = attr.ib(default=attr.Factory(list))


@attr.s
class State:

    orientation: array = attr.ib()  # facing
    axis: array = attr.ib()  # rotation
    speed: float = attr.ib(default=0.0)
    state3: array = zeros((1, 3), dtype=float)  # 3-axis rotation state
    state4: array = zeros((1, 4), dtype=float)  # 3-axis rotation state
    increment: array = zeros((1, 3), dtype=float)  # transformation increment


@attr.s
class Table:

    name: str = attr.ib()
    schema: Schema = attr.ib(default=Schema())

    @staticmethod
    def _unwrap(x):
        &#34;&#34;&#34;
        Some queries return iterables that need to be unpacked
        &#34;&#34;&#34;
        return {&#34;record&#34;: x[0]}

    def declare(self) -&gt; Query:
        &#34;&#34;&#34;
        Generate a query to create a new table but do not execute
        &#34;&#34;&#34;
        fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
        queryString = f&#34;&#34;&#34;
        CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
        &#34;&#34;&#34;
        return Query(queryString, None)

    def insert(self, data: ()) -&gt; Query:
        &#34;&#34;&#34;
        Generate the query to insert new rows into database.
        &#34;&#34;&#34;
        _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
        columns, values = map(
            join, ((field.name for field in self.schema.fields), _parsedValues)
        )

        queryString = f&#34;&#34;&#34;
        INSERT INTO {self.name} ({columns}) VALUES {values};
        &#34;&#34;&#34;
        return Query(queryString, None)

    def select(
        self,
        order_by: str = None,
        limit: int = 100,
        fields: (str) = (&#34;*&#34;,),
        order: str = &#34;DESC&#34;,
        conditions: ((str)) = (),
    ) -&gt; Query:
        &#34;&#34;&#34;
        Read back values/rows.
        &#34;&#34;&#34;
        _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
        _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

        queryString = f&#34;&#34;&#34;
        SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
        &#34;&#34;&#34;

        return Query(queryString, Table._unwrap)

    def drop(self) -&gt; Query:
        &#34;&#34;&#34;
        Drop the entire table
        &#34;&#34;&#34;
        return Query(f&#34;DROP TABLE {self.name};&#34;, None)


@attr.s
class TimeStamp:
    @staticmethod
    def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
        &#34;&#34;&#34;
        Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
        &#34;&#34;&#34;
        assert len(buffer) == 7
        yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
            int.from_bytes(buffer[:3], byteorder=byteorder),
            int.from_bytes(buffer[3:], byteorder=byteorder),
        )
        return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)


@attr.s
class Topology:

    cells: array = attr.ib(default=None)


    def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
        &#34;&#34;&#34;
        Get element neighbors
        &#34;&#34;&#34;
        queue = dict()
        while indices:
            cell = indices.pop()
            nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
            buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
            key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
            queue[key][cell] = buffer

        return queue


    @classmethod
    def read(path: str, indexed: bool = True) -&gt; dict:
        &#34;&#34;&#34;
        Read in grid topology of unstructured triangular grid
        &#34;&#34;&#34;
        if path[-3:] == &#34;.nc&#34;:
            fid = Dataset(path)
            topo = fid.variables[&#34;nv&#34;][:].T
        else:
            fid = open(path, &#34;r&#34;)
            df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
            topo = df.__array__()

        n = len(topo)

        basis = 0
        enforce = 1
        minimum = topo.min()
        if (minimum != enforce) if enforce else True:
            topo -= minimum + basis  # zero-index
        
        return {
            &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
            &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
        }

    @property
    def adjacency(self):
        &#34;&#34;&#34;
        Get node parents and node neighbors from topology

        :param topology:
        :return:
        &#34;&#34;&#34;
        _parents = dict()
        _neighbors = dict()

        for element in range(len(self.cells)):
            vertices = self.cells[element]
            for node in vertices:
                try:
                    p = _parents[node]
                except KeyError:
                    p = _parents[node] = []
                p.append(element)  # add element to parents, no possible duplicates

                try:
                    n = _neighbors[node]
                except KeyError:
                    n = _neighbors[node] = []
                (mask,) = where(node != vertices)
                others = vertices[mask]

                for neighbor in others:
                    if neighbor not in n:
                        n.append(neighbor)  # add current element to parents

        solid = zeros(n, dtype=bool)
        for node in range(n):
            difference = _neighbors[node].__len__() - _parents[node].__len__()
            if difference == 1:
                solid[node] = True
            elif difference != 0:
                print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)

    @staticmethod
    def deduplicate(self, process: bool = False) -&gt; array:

        n = len(self.cells)
        flag = zeros(n, dtype=bool)
        ordered = sorted(self.cells)

        for ii in range(n - 1):
            match = ordered[ii, :] == ordered[ii + 1 :, :]
            (rows,) = where(match)
            rows += ii + 1
            flag[rows] = True

        if process and flag.any():
            self.cells = self.cells[~flag]
            assert len(self.cells) == n - flag.sum()

        return self


@attr.s
class Trie:

    word = attr.ib(default=None)
    children = attr.ib(default=attr.Factory(dict))

    def insert(self, key: str) -&gt; None:
        node = self
        for letter in key:
            if letter not in node.children:
                node.children[letter] = Trie()
            node = node.children[letter]
        node.word = key

    @staticmethod
    def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
        _filter = lambda x: len(x)
        row = (previous[0] + 1,)
        for column in range(1, len(pattern) + 1):
            row += (
                min(
                    row[column - 1] + 1,
                    previous[column] + 1,
                    previous[column - 1] + int(pattern[column - 1] != symbol),
                ),
            )

        return (
            ((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()
        ) + tuple(
            chain(
                *filter(
                    _filter,
                    tuple(
                        Trie.searchRecursive(v, k, pattern, row, cost)
                        for k, v in node.children.items()
                    ),
                )
            )
            if min(row) &lt;= cost
            else ()
        )

    @staticmethod
    def levenshteinDistance(word1: str, word2: str) -&gt; int:
        columns = len(word1) + 1
        rows = len(word2) + 1

        # build first row
        currentRow = [0]
        for column in range(1, columns):
            currentRow.append(currentRow[column - 1] + 1)

        for row in range(1, rows):
            previousRow = currentRow
            currentRow = [previousRow[0] + 1]

            for column in range(1, columns):

                insertCost = currentRow[column - 1] + 1
                deleteCost = previousRow[column] + 1

                if word1[column - 1] != word2[row - 1]:
                    replaceCost = previousRow[column - 1] + 1
                else:
                    replaceCost = previousRow[column - 1]

                currentRow.append(min(insertCost, deleteCost, replaceCost))

        return currentRow[-1]

    @staticmethod
    def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
        _results = ()
        for word in words:
            cost = Trie.levenshteinDistance(pattern, word)
            if cost &lt;= maxCost:
                _results += ((word, cost),)
        return _results


@attr.s
class Unit:
    name: str = attr.ib(default=None)  # canonical reference name
    symbol: Any = attr.ib(default=None)  # symbol when displayed
    definition: str = attr.ib(
        default=None
    )  # reference to an external formal definition


@attr.s
class VertexArray:

    data: array = attr.ib(default=None)

    def deduplicate(self, topology: Topology = None, threshold: float = 0.00001):
        &#34;&#34;&#34;
        Scan vertex array for duplicates. If topology is also provided, swap later indices for their lower-index
        equivalents. Can be very expensive!

        :param topology:
        :param threshold:

        :return: deletion flags and modified topology array
        &#34;&#34;&#34;
        assert self.data.shape[1], &#34;Must have explicit dimensionality &gt;= 1&#34;
        flag = zeros(self.data.shape[0], dtype=bool)  # mask for indexing
        delta = zeros(self.data.shape, dtype=float)

        for ii in range(self.data.shape[0] - 1):
            if flag[ii]:  # already processed
                continue
            # distance for unchecked vertices
            delta[ii + 1, :] = self.data[ii, :] - self.data[ii + 1 :, :]
            distance = norm(delta[ii + 1, :])  # magnitude of difference vec is distance
            (rows,) = where(distance &lt; threshold)  # indices of points within threshold
            rows += ii + 1
            flag[rows] = True  # set look-ahead flags true for deletion

            if topology:
                for jj in rows:
                    # get rows and columns indices of duplicates
                    re, ce = where(topology == jj)
                    topology[re, ce] = ii  # replace duplicate indices

        if flag.any():  # there are duplicates
            (retain,) = where(~flag)  # first occurrences
            # reversed un-flagged points
            iterator = zip(retain, flip(retain, axis=0)[0 : len(retain)])
            for first, last in iterator:
                if first &gt; last:
                    break  # no swaps left

                self.data[first, :], self.data[last, :] = (
                    self.data[last, :],
                    self.data[first, :],
                )
                ri, ci = where(topology == first)
                rj, cj = where(topology == last)
                topology[ri, ci] = last
                topology[rj, cj] = first

            self.data = self.data[0 : len(retain), :]
        return self, topology



class View:
    count = 0

    def __init__(self, style, extent=None):
        # type: (dict, (float,)) -&gt; View
        &#34;&#34;&#34;
        Setup and return figure and axis instances
        &#34;&#34;&#34;
        rc(&#34;text&#34;, usetex=False)
        # rc(&#34;font&#34;, **{&#34;family&#34;: &#34;sans-serif&#34;, &#34;sans-serif&#34;: [&#34;Arial&#34;]})
        rc(&#34;mathtext&#34;, default=&#34;sf&#34;)
        rc(&#34;lines&#34;, markeredgewidth=1, linewidth=style[&#34;line&#34;])
        rc(&#34;axes&#34;, labelsize=style[&#34;text&#34;], linewidth=(style[&#34;line&#34;] + 1) // 2)
        rc(&#34;xtick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;ytick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;xtick.major&#34;, pad=5)
        rc(&#34;ytick.major&#34;, pad=5)

        self.style = style
        self.extent = extent
        self.fig, self.ax = subplots(
            facecolor=style[&#34;bg&#34;], figsize=(style[&#34;width&#34;], style[&#34;height&#34;])
        )
        padding = style[&#34;padding&#34;]
        subplots_adjust(
            left=padding[0], bottom=padding[1], right=1 - padding[2], top=1 - padding[3]
        )

    def format(self, bg: str, contrast: str, **kwargs):
        &#34;&#34;&#34;
        Setup color styles for figure
        &#34;&#34;&#34;
        self.ax.patch.set_facecolor(bg)  # background colors
        self.ax.edgecolor = contrast  # plotting area border
        self.format_axis(&#34;x&#34;, contrast, **kwargs)
        self.format_axis(&#34;y&#34;, contrast, **kwargs)

    def format_axis(
        self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
    ):
        if axis in (&#34;x&#34;, &#34;X&#34;):
            apply = self.ax.xaxis
            spines = (&#34;left&#34;, &#34;right&#34;)
        elif axis in (&#34;y&#34;, &#34;Y&#34;):
            apply = self.ax.yaxis
            spines = (&#34;top&#34;, &#34;bottom&#34;)
        else:
            raise ValueError

        apply.label.set_color(label)
        self.ax.tick_params(axis=axis.lower(), colors=label)
        for each in spines:
            self.ax.spines[each].set_color(contrast)
        apply.grid(grid)

    def pre_push(self):
        self.fig.canvas.draw()
        self.format(**self.style)
        self.ax.set_frame_on(True)

    def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
        # type: (str, bool, dict) -&gt; BytesIO
        buffer = BytesIO()
        self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
        buffer.seek(0)
        return buffer

    def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
        &#34;&#34;&#34;
        Format figure legend

        Kwargs:
            loc, str -- location on plotting area
            fc, str/arr -- string or RGBA color for face
            ec, str/arr -- string or RGBA color for edges

        Returns: matplotlib legend object
        &#34;&#34;&#34;
        legend = self.ax.legend(loc=loc)
        frame = legend.get_frame()
        frame.set_facecolor(fc)
        frame.set_edgecolor(ec)

        for text in legend.get_texts():
            text.set_color(self.style[&#34;contrast&#34;])

@attr.s
class Wind:
    &#34;&#34;&#34;
    Simulate wind speed and mixing.
    &#34;&#34;&#34;

    speed: float or array = attr.ib(default=0.0)
    delta: float or array = attr.ib(default=0.0)
    validRange: (float, float) = attr.ib(default=0.0)

    @property
    def simpleMixing(self,) -&gt; float or array:
        &#34;&#34;&#34;
        Basic wind mixing rate.
        &#34;&#34;&#34;
        return 0.728 * self.speed ** 0.5 - 0.317 * self.speed + 0.0372 * self.speed ** 2

    def update(self, dt: float):
        &#34;&#34;&#34;
        Update velocity shear due to wind forcing
        &#34;&#34;&#34;
        self.speed += self.delta * dt
        return self

    @staticmethod
    def shear(velocity: array, topology: array, precision: type = float) -&gt; array:
        &#34;&#34;&#34;
        Calculate current velocity shear vector for selected cells

        :param velocity: velocity field, assumed to be already subset to surface and U, V components
        :param topology: tuple of parents of the node
        :param precision:

        :return:
        &#34;&#34;&#34;

        n = len(topology)
        vectors = zeros((n, 2), dtype=precision)  # shear vectors

        for ii in range(n):
            parents = topology[ii]
            sq = (
                velocity[parents, :, :].mean(axis=0) ** 2
            )  # shape is (points, 3, layers, dim)
            vectors[ii] += (
                sq.sum(axis=0) ** 0.5
            )  # reduce to root of the sums, shape is (dim)

        return abs(vectors[:, 0] - vectors[:, 1])

    def dynamicMixing(
        self,
        mapping: ((int), (int)),
        velocity: array,
        diffusivity: float or array = 0.0,
    ) -&gt; array:
        &#34;&#34;&#34;

        :param nodes: object
        :param layers: object
        :param velocity: water velocity field
        :param diffusivity: of oxygen across air-water interface, m2/day

        :return: mixing rate
        &#34;&#34;&#34;
        nodes, layers = mapping
        depth = nodes.depth * layers.z[:2].mean()
        subset = velocity[:, :2, :2]
        return ((diffusivity * Wind.shear(subset, nodes.parents)) / depth ** 0.5).clip(
            min=self.validRange[0]
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="bathysphere.datatypes.ConvexHull"><code class="name flex">
<span>def <span class="ident">ConvexHull</span></span>(<span>points)</span>
</code></dt>
<dd>
<div class="desc"><p>Convex hulls are used to speed up spatial relation queries</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ConvexHull(points):
    &#34;&#34;&#34;
    Convex hulls are used to speed up spatial relation queries
    &#34;&#34;&#34;
    def segment(u, v, indices, points):
        &#34;&#34;&#34;Bisect the points&#34;&#34;&#34;
        if indices.shape[0] == 0:
            return array([], dtype=int)

        def crossProduct(i, j):
            &#34;&#34;&#34;Calculate angles&#34;&#34;&#34;
            return cross(points[indices, :] - points[i, :], points[j, :] - points[i, :])

        w = indices[argmin(crossProduct(u, v))]
        a = indices[argwhere(crossProduct(w, v) &lt; 0).flatten()]
        b = indices[argwhere(crossProduct(u, w) &lt; 0).flatten()]

        return hstack((segment(w, v, a, points), w, segment(u, w, b, points)))

    u = argmin(points[:, 0])
    v = argmax(points[:, 0])
    indices = arange(0, points.shape[0])
    parted = cross(points[indices, :] - points[u, :], points[v, :] - points[u, :]) &lt; 0

    a = indices[argwhere(~parted)]
    b = indices[argwhere(parted)]

    return hstack((u, segment(v, u, a, points), v, segment(u, v, b, points), u))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="bathysphere.datatypes.Array"><code class="flex name class">
<span>class <span class="ident">Array</span></span>
<span>(</span><span>data: array = None, gpu: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Encapsulates ND Array IO and operations, using either
numpy or arrayfire as a backend.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Array:
    &#34;&#34;&#34;
    Encapsulates ND Array IO and operations, using either
    numpy or arrayfire as a backend. 
    &#34;&#34;&#34;
    data: array = attr.ib(default=None)
    gpu: bool = attr.ib(default=False)

    @property
    def interval(self) -&gt; Interval:
        &#34;&#34;&#34;
        Get range of an array, which may be in GPU memory
        &#34;&#34;&#34;
        if self.gpu:
            tex = af.np_to_af_array(self.data)
            mn = af.min(tex)
            mx = af.max(tex)
        else:
            mn = min(self.data)
            mx = max(self.data)
        return mn, mx

    @property
    def range(self) -&gt; float:
        &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
        return self.data.max() - self.data.min()


    @property
    def normalized(self) -&gt; Array:
        &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
        return (self.data - self.data.min()) / self.range


    @property
    def colorize(self) -&gt; Array:
        &#34;&#34;&#34;
        Convert data field to color and transparency components
        &#34;&#34;&#34;
        normalized = self.normalized
        colors = zeros((*self.data.shape, 4), dtype=int) + 255
        colors[:, :, :, 0] *= normalized  # red
        colors[:, :, :, 1] *= 0  # green
        colors[:, :, :, 2] *= 1 - normalized  # blue
        colors[:, :, :, 3] *= 0.5 * normalized  # alpha
        return colors</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Array.colorize"><code class="name">var <span class="ident">colorize</span> : <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></dt>
<dd>
<div class="desc"><p>Convert data field to color and transparency components</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def colorize(self) -&gt; Array:
    &#34;&#34;&#34;
    Convert data field to color and transparency components
    &#34;&#34;&#34;
    normalized = self.normalized
    colors = zeros((*self.data.shape, 4), dtype=int) + 255
    colors[:, :, :, 0] *= normalized  # red
    colors[:, :, :, 1] *= 0  # green
    colors[:, :, :, 2] *= 1 - normalized  # blue
    colors[:, :, :, 3] *= 0.5 * normalized  # alpha
    return colors</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Array.interval"><code class="name">var <span class="ident">interval</span> : <a title="bathysphere.datatypes.Interval" href="#bathysphere.datatypes.Interval">Interval</a></code></dt>
<dd>
<div class="desc"><p>Get range of an array, which may be in GPU memory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def interval(self) -&gt; Interval:
    &#34;&#34;&#34;
    Get range of an array, which may be in GPU memory
    &#34;&#34;&#34;
    if self.gpu:
        tex = af.np_to_af_array(self.data)
        mn = af.min(tex)
        mx = af.max(tex)
    else:
        mn = min(self.data)
        mx = max(self.data)
    return mn, mx</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Array.normalized"><code class="name">var <span class="ident">normalized</span> : <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></dt>
<dd>
<div class="desc"><p>Transform to (0,1) range</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def normalized(self) -&gt; Array:
    &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
    return (self.data - self.data.min()) / self.range</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Array.range"><code class="name">var <span class="ident">range</span> : float</code></dt>
<dd>
<div class="desc"><p>Calculate range of data, used in other properties and functions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def range(self) -&gt; float:
    &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
    return self.data.max() - self.data.min()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Bound"><code class="flex name class">
<span>class <span class="ident">Bound</span></span>
<span>(</span><span>value: Any, closed: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A bound is on an interval, may ne upper or lower, closed or open</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Bound:
    &#34;&#34;&#34;
    A bound is on an interval, may ne upper or lower, closed or open
    &#34;&#34;&#34;
    value: Any = attr.ib()
    closed: bool = attr.ib(default=False)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.BoundingBox"><code class="flex name class">
<span>class <span class="ident">BoundingBox</span></span>
<span>(</span><span>lower_left: (float, float), uppper_right: (float, float))</span>
</code></dt>
<dd>
<div class="desc"><p>A bounding box is similar to an extent, but is define by two points instead of intervals</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BoundingBox:
    &#34;&#34;&#34;
    A bounding box is similar to an extent, but is define by two points instead of intervals
    &#34;&#34;&#34;
    lower_left: (float, float) = attr.ib()
    uppper_right: (float, float) =  attr.ib()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem"><code class="flex name class">
<span>class <span class="ident">ChemicalSystem</span></span>
</code></dt>
<dd>
<div class="desc"><p>ChemicalSystems encapuslate conservative mass transport for a single
tracked species of reactant</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ChemicalSystem:
    &#34;&#34;&#34;
    ChemicalSystems encapuslate conservative mass transport for a single
    tracked species of reactant
    &#34;&#34;&#34;
    sources = None
    value = None
    massAdded: Array = None
    symbol = None
    validRange = (0.0, None)

    @property
    def flux(self) -&gt; None:
        &#34;&#34;&#34;
        Transfer of concentration between control volumes
        &#34;&#34;&#34;
        return None

    @property
    def mass(self) -&gt; None:
        &#34;&#34;&#34;Calculate mass from concentration&#34;&#34;&#34;
        return None

    @property
    def delta(self):
        &#34;&#34;&#34;Current rate of changed, dynamically calculated&#34;&#34;&#34;
        return 0.0

    def __add__(self, other):
        &#34;&#34;&#34;Add two systems&#34;&#34;&#34;
        try:
            return self.value + other.value
        except:
            return self.value + other

    def __truediv__(self, other):
        &#34;&#34;&#34;Divide, such as for unit conversion&#34;&#34;&#34;
        try:
            return self.value / other.value
        except:
            return self.value / other

    def __lt__(self, other):
        &#34;&#34;&#34;Array compare&#34;&#34;&#34;
        return self.value &lt; other

    def __gt__(self, other):
        &#34;&#34;&#34;Array compare&#34;&#34;&#34;
        return self.value &gt; other

    def clamp(
        self,
        future: array,
        volume: array
    ):
        &#34;&#34;&#34;
        Enforce range

        :param concentration:
        :param future:
        :param volume:
        &#34;&#34;&#34;
        nodes, layers = where(self.value &lt; self.validRange[0])
        self.massAdded[nodes, layers] += volume * (self.value - future)
        return future.clip(max=self.validRange[1])

    def transfer(self, conversion: float = 1.0):
        &#34;&#34;&#34;
        :param conversion:

        :return:
        &#34;&#34;&#34;
        # Transport.horizontal(mesh, reactor, self.key)  # Mass flux, advection and diffusion
        # Transport.vertical(mesh, reactor, self.key)  # Mass flux, vertical sigma velocity
        self.mass += self.delta * conversion  # update state from reaction equations</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.ChemicalSystem.massAdded"><code class="name">var <span class="ident">massAdded</span> : <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.sources"><code class="name">var <span class="ident">sources</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.symbol"><code class="name">var <span class="ident">symbol</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.validRange"><code class="name">var <span class="ident">validRange</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.value"><code class="name">var <span class="ident">value</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.ChemicalSystem.delta"><code class="name">var <span class="ident">delta</span></code></dt>
<dd>
<div class="desc"><p>Current rate of changed, dynamically calculated</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def delta(self):
    &#34;&#34;&#34;Current rate of changed, dynamically calculated&#34;&#34;&#34;
    return 0.0</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.flux"><code class="name">var <span class="ident">flux</span> : NoneType</code></dt>
<dd>
<div class="desc"><p>Transfer of concentration between control volumes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def flux(self) -&gt; None:
    &#34;&#34;&#34;
    Transfer of concentration between control volumes
    &#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.mass"><code class="name">var <span class="ident">mass</span> : NoneType</code></dt>
<dd>
<div class="desc"><p>Calculate mass from concentration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mass(self) -&gt; None:
    &#34;&#34;&#34;Calculate mass from concentration&#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.ChemicalSystem.clamp"><code class="name flex">
<span>def <span class="ident">clamp</span></span>(<span>self, future: array, volume: array)</span>
</code></dt>
<dd>
<div class="desc"><p>Enforce range</p>
<p>:param concentration:
:param future:
:param volume:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clamp(
    self,
    future: array,
    volume: array
):
    &#34;&#34;&#34;
    Enforce range

    :param concentration:
    :param future:
    :param volume:
    &#34;&#34;&#34;
    nodes, layers = where(self.value &lt; self.validRange[0])
    self.massAdded[nodes, layers] += volume * (self.value - future)
    return future.clip(max=self.validRange[1])</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ChemicalSystem.transfer"><code class="name flex">
<span>def <span class="ident">transfer</span></span>(<span>self, conversion: float = 1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>:param conversion:</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transfer(self, conversion: float = 1.0):
    &#34;&#34;&#34;
    :param conversion:

    :return:
    &#34;&#34;&#34;
    # Transport.horizontal(mesh, reactor, self.key)  # Mass flux, advection and diffusion
    # Transport.vertical(mesh, reactor, self.key)  # Mass flux, vertical sigma velocity
    self.mass += self.delta * conversion  # update state from reaction equations</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Clock"><code class="flex name class">
<span>class <span class="ident">Clock</span></span>
<span>(</span><span>dt, start: int, elapsed: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Timekeeper object with integer clock</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clock:
    &#34;&#34;&#34;
    Timekeeper object with integer clock
    &#34;&#34;&#34;

    dt = attr.ib()
    start: int = attr.ib()  # time in seconds
    elapsed: int = attr.ib(default=0)

    @property
    def yd(self):
        return self.days % 365

    @property
    def time(self) -&gt; float:
        return self.days % 1.0

    @property
    def days(self) -&gt; float:
        return (self.start + self.elapsed) / SEC2DAY  # current time in days

    @property
    def next(self) -&gt; int:
        return self.start + self.elapsed + SEC2DAY

    def tick(self, dt: int = None) -&gt; int:
        &#34;&#34;&#34;
        Update clock

        :param dt: Optional parameter to assign new time step (integer seconds)
        :return: None
        &#34;&#34;&#34;
        if dt is not None:
            self.dt = dt
        self.elapsed += self.dt
        return self.start + self.elapsed</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Clock.days"><code class="name">var <span class="ident">days</span> : float</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def days(self) -&gt; float:
    return (self.start + self.elapsed) / SEC2DAY  # current time in days</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Clock.next"><code class="name">var <span class="ident">next</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def next(self) -&gt; int:
    return self.start + self.elapsed + SEC2DAY</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Clock.time"><code class="name">var <span class="ident">time</span> : float</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def time(self) -&gt; float:
    return self.days % 1.0</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Clock.yd"><code class="name">var <span class="ident">yd</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def yd(self):
    return self.days % 365</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Clock.tick"><code class="name flex">
<span>def <span class="ident">tick</span></span>(<span>self, dt: int = None) -> int</span>
</code></dt>
<dd>
<div class="desc"><p>Update clock</p>
<p>:param dt: Optional parameter to assign new time step (integer seconds)
:return: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tick(self, dt: int = None) -&gt; int:
    &#34;&#34;&#34;
    Update clock

    :param dt: Optional parameter to assign new time step (integer seconds)
    :return: None
    &#34;&#34;&#34;
    if dt is not None:
        self.dt = dt
    self.elapsed += self.dt
    return self.start + self.elapsed</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.CloudSQL"><code class="flex name class">
<span>class <span class="ident">CloudSQL</span></span>
<span>(</span><span>auth: (str, str), instance: str, port: int = 5432, pool_size: int = 4, max_overflow: int = 2, pool_timeout: int = 5, pool_recycle: int = 1800)</span>
</code></dt>
<dd>
<div class="desc"><p>This class encapsulates a connection pool to a cloud based PostgreSQL provider.
By default it expects a Google CloudSQL database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CloudSQL:
    &#34;&#34;&#34;
    This class encapsulates a connection pool to a cloud based PostgreSQL provider.
    By default it expects a Google CloudSQL database. 
    &#34;&#34;&#34;

    auth: (str, str) = attr.ib()
    instance: str = attr.ib()
    port: int = attr.ib(default=5432)
    pool_size: int = attr.ib(default=4)
    max_overflow: int = attr.ib(default=2)
    pool_timeout: int = attr.ib(default=5)
    pool_recycle: int = attr.ib(default=1800)

    @property
    def engine(self) -&gt; Engine:
        &#34;&#34;&#34;
        The engine property will be used only once per request, so
        can safely be generated as a property. 
        &#34;&#34;&#34;
        user, password = self.auth
        return create_engine(
            URL(
                drivername=&#34;postgres+pg8000&#34;,
                username=user,
                password=password,
                database=&#34;postgres&#34;,
                query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
            ),
            pool_size=self.pool_size,
            max_overflow=self.max_overflow,
            pool_timeout=self.pool_timeout,
            pool_recycle=self.pool_recycle,
        )

    def query(self, table, **kwargs) -&gt; [dict]:
        &#34;&#34;&#34;
        Execute an arbitrary query.
        &#34;&#34;&#34;
        with self.engine.connect() as cursor:
            query: Query = table.select(**kwargs)
            return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]

    def handle(self, request: Request) -&gt; ResponseJSON:
        &#34;&#34;&#34;
        Do some postgres stuff
        &#34;&#34;&#34;
        # pylint: disable=broad-exception
        conf = request.body[&#34;table&#34;]
        fields = [
            Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
        ]
        table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

        try:
            records = self.query(table=table)
        except Exception as ex:  
            return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

        try:
            return (
                dumps(
                    {
                        &#34;count&#34;: len(records),
                        &#34;data&#34;: records,
                        &#34;method&#34;: str(request.method),
                        &#34;query_string&#34;: str(request.query_string),
                    }
                ),
                200,
            )
        except Exception as ex:
            return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.CloudSQL.engine"><code class="name">var <span class="ident">engine</span> : sqlalchemy.engine.base.Engine</code></dt>
<dd>
<div class="desc"><p>The engine property will be used only once per request, so
can safely be generated as a property.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def engine(self) -&gt; Engine:
    &#34;&#34;&#34;
    The engine property will be used only once per request, so
    can safely be generated as a property. 
    &#34;&#34;&#34;
    user, password = self.auth
    return create_engine(
        URL(
            drivername=&#34;postgres+pg8000&#34;,
            username=user,
            password=password,
            database=&#34;postgres&#34;,
            query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
        ),
        pool_size=self.pool_size,
        max_overflow=self.max_overflow,
        pool_timeout=self.pool_timeout,
        pool_recycle=self.pool_recycle,
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.CloudSQL.handle"><code class="name flex">
<span>def <span class="ident">handle</span></span>(<span>self, request: Request) -> 'ResponseJSON'</span>
</code></dt>
<dd>
<div class="desc"><p>Do some postgres stuff</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle(self, request: Request) -&gt; ResponseJSON:
    &#34;&#34;&#34;
    Do some postgres stuff
    &#34;&#34;&#34;
    # pylint: disable=broad-exception
    conf = request.body[&#34;table&#34;]
    fields = [
        Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
    ]
    table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

    try:
        records = self.query(table=table)
    except Exception as ex:  
        return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

    try:
        return (
            dumps(
                {
                    &#34;count&#34;: len(records),
                    &#34;data&#34;: records,
                    &#34;method&#34;: str(request.method),
                    &#34;query_string&#34;: str(request.query_string),
                }
            ),
            200,
        )
    except Exception as ex:
        return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, table, **kwargs) -> '[dict]'</span>
</code></dt>
<dd>
<div class="desc"><p>Execute an arbitrary query.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(self, table, **kwargs) -&gt; [dict]:
    &#34;&#34;&#34;
    Execute an arbitrary query.
    &#34;&#34;&#34;
    with self.engine.connect() as cursor:
        query: Query = table.select(**kwargs)
        return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Condition"><code class="flex name class">
<span>class <span class="ident">Condition</span></span>
<span>(</span><span>value: array, shape: int, mapping: (array, array), scale: float = 1.0, mass: float = 0.0, next: float = None, last: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Conditions are a base class for BOUNDARY and SOURCE types.</p>
<p>:param nodes: optional node indices, if None same value applied universally (non-point)
:param layers: optional layer indices, if None same value applied over column</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Condition:
    &#34;&#34;&#34;
    Conditions are a base class for BOUNDARY and SOURCE types.

    :param nodes: optional node indices, if None same value applied universally (non-point)
    :param layers: optional layer indices, if None same value applied over column
    &#34;&#34;&#34;

    value: array = attr.ib()
    shape: (int) = attr.ib()
    mapping: (array, array) = attr.ib()
    scale: float = attr.ib(default=1.0)
    mass: float = attr.ib(default=0.0)
    next: float = attr.ib(default=None)
    last: float = attr.ib(default=None)

    @property
    def delta(self):
        &#34;&#34;&#34;
        Convenient property to auto
        &#34;&#34;&#34;
        return self.value * self.scale if self.scale is not None else self.value

    def boundary(self, system) -&gt; None:
        &#34;&#34;&#34;
        Boundaries are conditions which override the current state, and impose a new value. They may be a time-varying
        function, constant, or may be controlled by an external simulation.
        &#34;&#34;&#34;
        system[self.mapping] = self.value
        return system

    def mark(self, nodes):
        &#34;&#34;&#34;
        flag nodes as source

        :param nodes:
        :return:
        &#34;&#34;&#34;
        nodes.source[self.mapping] = True

    def update(self, dt: float):
        &#34;&#34;&#34;
        Update values from slope, and calculate new slope

        :param dt:
        :return:
        &#34;&#34;&#34;

        self.value += self.delta * dt
        return self

    def read(self, path: str, conversion: float = 1000):
        &#34;&#34;&#34;
        Read forcing conditions from CSV file, and update difference equation.
        Will fail silently if condition was declared constant

        :param path: path to CSV file
        :param conversion: unit conversion factor

        :return: success
        &#34;&#34;&#34;

        try:
            fid = open(path, &#34;r&#34;)
            data = array(fid.readline().split(&#34;,&#34;)).astype(float)
            fid.close()

            self.last, self.next = (
                self.next,
                data[0],
            )  # simulation time or reads in integer seconds
            self.delta = (data[1:] * conversion * self.scale - self.value) / (
                self.next - self.last
            )
        except AttributeError:
            return False
        else:
            return True

    def source(self, system: ChemicalSystem) -&gt; None:
        &#34;&#34;&#34;
        Source are a type of condition. They are added to their parent state array.

        :param system: chemistry instance
        :param key: internal pool key of tracer
        :param scale: optional conversion factor, used primarily for surface area correction
        &#34;&#34;&#34;
        system.mass[self.mapping] += self.delta
        self.mass += self.delta.sum()  # add to mass balance counter
        return system

    @classmethod
    def NonPointSource(cls):
        &#34;&#34;&#34;
        Uniform by default. Can also be vertically or horizontally uniform if desired.

        Atmospheric and sediment sources are special cases.
        &#34;&#34;&#34;
        return cls()

    @classmethod
    def PointSource(cls, nodes: (int) = None, layers: (int) = None):
        &#34;&#34;&#34;
        Point source loads are defined at some but not all nodes. Points which are not part of the mesh model
        (locations that are not nodes, or location that ARE elements) are divided amongst nearest neighbors.
        This is also true when mass is released between sigma layers,
        such as Lagrangian particle models with vertical dynamics.
        &#34;&#34;&#34;
        return cls(mapping=(nodes, layers))

    @classmethod
    def Surface(cls, nodes: (int) = None):
        &#34;&#34;&#34;
        Atmospheric loads are non-point sources. They may vary in space.
        &#34;&#34;&#34;
        return cls(mapping=(nodes, (0,)))

    @classmethod
    def FallLine(cls, nodes: (int), layers: (int) = None):
        &#34;&#34;&#34;
        Fall-line loads occur where mass enters the system at a boundary, usually a well-mixed freshwater discharge.
        The same concentration is added along a node-defined path, composed of at least two points on the shoreline,
        which are joined by edges either transecting the discharge stream, or following the shoreline (e.g. ground
        water).

        They are a special type of point source.
        &#34;&#34;&#34;
        cls(mapping=(nodes, layers))</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Condition.FallLine"><code class="name flex">
<span>def <span class="ident">FallLine</span></span>(<span>nodes: int, layers: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fall-line loads occur where mass enters the system at a boundary, usually a well-mixed freshwater discharge.
The same concentration is added along a node-defined path, composed of at least two points on the shoreline,
which are joined by edges either transecting the discharge stream, or following the shoreline (e.g. ground
water).</p>
<p>They are a special type of point source.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def FallLine(cls, nodes: (int), layers: (int) = None):
    &#34;&#34;&#34;
    Fall-line loads occur where mass enters the system at a boundary, usually a well-mixed freshwater discharge.
    The same concentration is added along a node-defined path, composed of at least two points on the shoreline,
    which are joined by edges either transecting the discharge stream, or following the shoreline (e.g. ground
    water).

    They are a special type of point source.
    &#34;&#34;&#34;
    cls(mapping=(nodes, layers))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.NonPointSource"><code class="name flex">
<span>def <span class="ident">NonPointSource</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Uniform by default. Can also be vertically or horizontally uniform if desired.</p>
<p>Atmospheric and sediment sources are special cases.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def NonPointSource(cls):
    &#34;&#34;&#34;
    Uniform by default. Can also be vertically or horizontally uniform if desired.

    Atmospheric and sediment sources are special cases.
    &#34;&#34;&#34;
    return cls()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.PointSource"><code class="name flex">
<span>def <span class="ident">PointSource</span></span>(<span>nodes: int = None, layers: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Point source loads are defined at some but not all nodes. Points which are not part of the mesh model
(locations that are not nodes, or location that ARE elements) are divided amongst nearest neighbors.
This is also true when mass is released between sigma layers,
such as Lagrangian particle models with vertical dynamics.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def PointSource(cls, nodes: (int) = None, layers: (int) = None):
    &#34;&#34;&#34;
    Point source loads are defined at some but not all nodes. Points which are not part of the mesh model
    (locations that are not nodes, or location that ARE elements) are divided amongst nearest neighbors.
    This is also true when mass is released between sigma layers,
    such as Lagrangian particle models with vertical dynamics.
    &#34;&#34;&#34;
    return cls(mapping=(nodes, layers))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.Surface"><code class="name flex">
<span>def <span class="ident">Surface</span></span>(<span>nodes: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Atmospheric loads are non-point sources. They may vary in space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def Surface(cls, nodes: (int) = None):
    &#34;&#34;&#34;
    Atmospheric loads are non-point sources. They may vary in space.
    &#34;&#34;&#34;
    return cls(mapping=(nodes, (0,)))</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Condition.delta"><code class="name">var <span class="ident">delta</span></code></dt>
<dd>
<div class="desc"><p>Convenient property to auto</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def delta(self):
    &#34;&#34;&#34;
    Convenient property to auto
    &#34;&#34;&#34;
    return self.value * self.scale if self.scale is not None else self.value</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Condition.boundary"><code class="name flex">
<span>def <span class="ident">boundary</span></span>(<span>self, system) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Boundaries are conditions which override the current state, and impose a new value. They may be a time-varying
function, constant, or may be controlled by an external simulation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boundary(self, system) -&gt; None:
    &#34;&#34;&#34;
    Boundaries are conditions which override the current state, and impose a new value. They may be a time-varying
    function, constant, or may be controlled by an external simulation.
    &#34;&#34;&#34;
    system[self.mapping] = self.value
    return system</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.mark"><code class="name flex">
<span>def <span class="ident">mark</span></span>(<span>self, nodes)</span>
</code></dt>
<dd>
<div class="desc"><p>flag nodes as source</p>
<p>:param nodes:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mark(self, nodes):
    &#34;&#34;&#34;
    flag nodes as source

    :param nodes:
    :return:
    &#34;&#34;&#34;
    nodes.source[self.mapping] = True</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, path: str, conversion: float = 1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Read forcing conditions from CSV file, and update difference equation.
Will fail silently if condition was declared constant</p>
<p>:param path: path to CSV file
:param conversion: unit conversion factor</p>
<p>:return: success</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, path: str, conversion: float = 1000):
    &#34;&#34;&#34;
    Read forcing conditions from CSV file, and update difference equation.
    Will fail silently if condition was declared constant

    :param path: path to CSV file
    :param conversion: unit conversion factor

    :return: success
    &#34;&#34;&#34;

    try:
        fid = open(path, &#34;r&#34;)
        data = array(fid.readline().split(&#34;,&#34;)).astype(float)
        fid.close()

        self.last, self.next = (
            self.next,
            data[0],
        )  # simulation time or reads in integer seconds
        self.delta = (data[1:] * conversion * self.scale - self.value) / (
            self.next - self.last
        )
    except AttributeError:
        return False
    else:
        return True</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.source"><code class="name flex">
<span>def <span class="ident">source</span></span>(<span>self, system: ChemicalSystem) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Source are a type of condition. They are added to their parent state array.</p>
<p>:param system: chemistry instance
:param key: internal pool key of tracer
:param scale: optional conversion factor, used primarily for surface area correction</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def source(self, system: ChemicalSystem) -&gt; None:
    &#34;&#34;&#34;
    Source are a type of condition. They are added to their parent state array.

    :param system: chemistry instance
    :param key: internal pool key of tracer
    :param scale: optional conversion factor, used primarily for surface area correction
    &#34;&#34;&#34;
    system.mass[self.mapping] += self.delta
    self.mass += self.delta.sum()  # add to mass balance counter
    return system</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Condition.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, dt: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Update values from slope, and calculate new slope</p>
<p>:param dt:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, dt: float):
    &#34;&#34;&#34;
    Update values from slope, and calculate new slope

    :param dt:
    :return:
    &#34;&#34;&#34;

    self.value += self.delta * dt
    return self</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.CoordinateSystem"><code class="flex name class">
<span>class <span class="ident">CoordinateSystem</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Well Known Coordinate Systems</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoordinateSystem(Enum):
    &#34;&#34;&#34;Well Known Coordinate Systems&#34;&#34;&#34;
    Sigma = 1
    Cartesian = 2
    Gaussian = 3
    Spherical = 4
    Periodic = 5</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.CoordinateSystem.Cartesian"><code class="name">var <span class="ident">Cartesian</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CoordinateSystem.Gaussian"><code class="name">var <span class="ident">Gaussian</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CoordinateSystem.Periodic"><code class="name">var <span class="ident">Periodic</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CoordinateSystem.Sigma"><code class="name">var <span class="ident">Sigma</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CoordinateSystem.Spherical"><code class="name">var <span class="ident">Spherical</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Coordinates"><code class="flex name class">
<span>class <span class="ident">Coordinates</span></span>
<span>(</span><span>x: float, y: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Point coordinates for spatial applications</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Coordinates:
    &#34;&#34;&#34;Point coordinates for spatial applications&#34;&#34;&#34;

    x: float = attr.ib()
    y: float = attr.ib()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DataFormat"><code class="flex name class">
<span>class <span class="ident">DataFormat</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Well Knonw NDArray formats</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataFormat(Enum):
    &#34;&#34;&#34;Well Knonw NDArray formats&#34;&#34;&#34;
    NETCDF3_CLASSIC = 1
    NETCDF4 = 2
    NETCDF5 = 3
    Custom = 4
    Binary = 5
    NumpyArray = 6
    ArrayfireTexture = 7</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.DataFormat.ArrayfireTexture"><code class="name">var <span class="ident">ArrayfireTexture</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.Binary"><code class="name">var <span class="ident">Binary</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.Custom"><code class="name">var <span class="ident">Custom</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.NETCDF3_CLASSIC"><code class="name">var <span class="ident">NETCDF3_CLASSIC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.NETCDF4"><code class="name">var <span class="ident">NETCDF4</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.NETCDF5"><code class="name">var <span class="ident">NETCDF5</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.DataFormat.NumpyArray"><code class="name">var <span class="ident">NumpyArray</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
or drop-outs.</p>
<ul>
<li>Query: Get an array of a single variable</li>
<li>Cache: Save chunk in object storage or local filesystem</li>
</ul>
<p><strong><code>__init__(self, filename, mode="r", clobber=True, diskless=False,
persist=False, keepweakref=False, memory=None, encoding=None,
parallel=False, comm=None, info=None, format='NETCDF4')</code></strong></p>
<p><code>netCDF4.Dataset</code> constructor.</p>
<p><strong><code>filename</code></strong>: Name of netCDF file to hold dataset. Can also
be a python 3 pathlib instance or the URL of an OpenDAP dataset.
When memory is
set this is just used to set the <code>filepath()</code>.</p>
<p><strong><code>mode</code></strong>: access mode. <code>r</code> means read-only; no data can be
modified. <code>w</code> means write; a new file is created, an existing file with
the same name is deleted. <code>a</code> and <code>r+</code> mean append (in analogy with
serial files); an existing file is opened for reading and writing.
Appending <code>s</code> to modes <code>r</code>, <code>w</code>, <code>r+</code> or <code>a</code> will enable unbuffered shared
access to <code>NETCDF3_CLASSIC</code>, <code>NETCDF3_64BIT_OFFSET</code> or
<code>NETCDF3_64BIT_DATA</code> formatted files.
Unbuffered access may be useful even if you don't need shared
access, since it may be faster for programs that don't access data
sequentially. This option is ignored for <code>NETCDF4</code> and <code>NETCDF4_CLASSIC</code>
formatted files.</p>
<p><strong><code>clobber</code></strong>: if <code>True</code> (default), opening a file with <code>mode='w'</code>
will clobber an existing file with the same name.
if <code>False</code>, an
exception will be raised if a file with the same name already exists.</p>
<p><strong><code>format</code></strong>: underlying file format (one of <code>'NETCDF4',
'NETCDF4_CLASSIC', 'NETCDF3_CLASSIC'&lt;code&gt;, &lt;/code&gt;'NETCDF3_64BIT_OFFSET'</code> or
<code>'NETCDF3_64BIT_DATA'</code>.
Only relevant if <code>mode = 'w'</code> (if <code>mode = 'r','a'</code> or <code>'r+'</code> the file format
is automatically detected). Default <code>'NETCDF4'</code>, which means the data is
stored in an HDF5 file, using netCDF 4 API features.
Setting
<code>format='NETCDF4_CLASSIC'</code> will create an HDF5 file, using only netCDF 3
compatible API features. netCDF 3 clients must be recompiled and linked
against the netCDF 4 library to read files in <code>NETCDF4_CLASSIC</code> format.
<code>'NETCDF3_CLASSIC'</code> is the classic netCDF 3 file format that does not
handle 2+ Gb files. <code>'NETCDF3_64BIT_OFFSET'</code> is the 64-bit offset
version of the netCDF 3 file format, which fully supports 2+ GB files, but
is only compatible with clients linked against netCDF version 3.6.0 or
later. <code>'NETCDF3_64BIT_DATA'</code> is the 64-bit data version of the netCDF 3
file format, which supports 64-bit dimension sizes plus unsigned and
64 bit integer data types, but is only compatible with clients linked against
netCDF version 4.4.0 or later.</p>
<p><strong><code>diskless</code></strong>: If <code>True</code>, create diskless (in-core) file.
This is a feature added to the C library after the
netcdf-4.2 release. If you need to access the memory buffer directly,
use the in-memory feature instead (see <code>memory</code> kwarg).</p>
<p><strong><code>persist</code></strong>: if <code>diskless=True</code>, persist file to disk when closed
(default <code>False</code>).</p>
<p><strong><code>keepweakref</code></strong>: if <code>True</code>, child Dimension and Variable instances will keep weak
references to the parent Dataset or Group object.
Default is <code>False</code>, which
means strong references will be kept.
Having Dimension and Variable instances
keep a strong reference to the parent Dataset instance, which in turn keeps a
reference to child Dimension and Variable instances, creates circular references.
Circular references complicate garbage collection, which may mean increased
memory usage for programs that create may Dataset instances with lots of
Variables. It also will result in the Dataset object never being deleted, which
means it may keep open files alive as well. Setting <code>keepweakref=True</code> allows
Dataset instances to be garbage collected as soon as they go out of scope, potentially
reducing memory usage and open file handles.
However, in many cases this is not
desirable, since the associated Variable instances may still be needed, but are
rendered unusable when the parent Dataset instance is garbage collected.</p>
<p><strong><code>_ncstring_attrs__</code></strong>: if <code>_ncstring_attrs__=True</code>, all string attributes will use
the variable length NC_STRING attributes (default <code>False</code>, ascii text
attributes written as NC_CHAR).</p>
<p><strong><code>memory</code></strong>: if not <code>None</code>, create or open an in-memory Dataset.
If mode = 'r', the memory kwarg must contain a memory buffer object
(an object that supports the python buffer interface).
The Dataset will then be created with contents taken from this block of memory.
If mode = 'w', the memory kwarg should contain the anticipated size
of the Dataset in bytes (used only for NETCDF3 files).
A memory
buffer containing a copy of the Dataset is returned by the
<code>Dataset.close</code> method. Requires netcdf-c version 4.4.1 for mode='r,
netcdf-c 4.6.2 for mode='w'. To persist the file to disk, the raw
bytes from the returned buffer can be written into a binary file.
The Dataset can also be re-opened using this memory buffer.</p>
<p><strong><code>encoding</code></strong>: encoding used to encode filename string into bytes.
Default is None (<code>sys.getdefaultfileencoding()</code> is used).</p>
<p><strong><code>parallel</code></strong>: open for parallel access using MPI (requires mpi4py and
parallel-enabled netcdf-c and hdf5 libraries).
Default is <code>False</code>. If
<code>True</code>, <code>comm</code> and <code>info</code> kwargs may also be specified.</p>
<p><strong><code>comm</code></strong>: MPI_Comm object for parallel access. Default <code>None</code>, which
means MPI_COMM_WORLD will be used.
Ignored if <code>parallel=False</code>.</p>
<p><strong><code>info</code></strong>: MPI_Info object for parallel access. Default <code>None</code>, which
means MPI_INFO_NULL will be used.
Ignored if <code>parallel=False</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataset(_Dataset):
    &#34;&#34;&#34;
    Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
    or drop-outs.

    * Query: Get an array of a single variable
    * Cache: Save chunk in object storage or local filesystem
    &#34;&#34;&#34;

    def query(
        self,
        observed_property: str,
        samples: int = None,
        reduce_dim: bool = False,
        kind: str = &#34;float64&#34;,
    ) -&gt; array:
        &#34;&#34;&#34;
        Extract an observedProperty, and optionally extract pixel samples from it.
        :param observed_property: field to extract
        :param samples: buffer of pixel indices to sample
        :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
        :param kind: format for numerical data
        &#34;&#34;&#34;
        simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
        if samples:
            return array(
                self.variables[observed_property][0, i, j].astype(kind)
                for i, j in samples
            )
        return (
            self.variables[observed_property][:, 0].astype(kind)
            if reduce_dim
            else self.variables[observed_property][:].astype(kind)
        )

    def copy(self, path: str, observed_properties: (str) = None):
        &#34;&#34;&#34;
        Copy parts into a new file
        &#34;&#34;&#34;
        fid = _Dataset(path=path)
        if isfile(path=path) and not self.policy():
            return False
        for name, obj in self.dimensions.items():
            fid.createDimension(name, obj)
        for name, obj in self.variables.items():
            if observed_properties and str(name) not in observed_properties:
                continue  # not matching variables in source data
            fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
            fid.variables[name][:] = self.variables[name][:]
        fid.close()
        return fid</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>netCDF4._netCDF4.Dataset</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Dataset.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, path: str, observed_properties: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Copy parts into a new file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, path: str, observed_properties: (str) = None):
    &#34;&#34;&#34;
    Copy parts into a new file
    &#34;&#34;&#34;
    fid = _Dataset(path=path)
    if isfile(path=path) and not self.policy():
        return False
    for name, obj in self.dimensions.items():
        fid.createDimension(name, obj)
    for name, obj in self.variables.items():
        if observed_properties and str(name) not in observed_properties:
            continue  # not matching variables in source data
        fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
        fid.variables[name][:] = self.variables[name][:]
    fid.close()
    return fid</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Dataset.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, observed_property: str, samples: int = None, reduce_dim: bool = False, kind: str = 'float64') -> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Extract an observedProperty, and optionally extract pixel samples from it.
:param observed_property: field to extract
:param samples: buffer of pixel indices to sample
:param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
:param kind: format for numerical data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(
    self,
    observed_property: str,
    samples: int = None,
    reduce_dim: bool = False,
    kind: str = &#34;float64&#34;,
) -&gt; array:
    &#34;&#34;&#34;
    Extract an observedProperty, and optionally extract pixel samples from it.
    :param observed_property: field to extract
    :param samples: buffer of pixel indices to sample
    :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
    :param kind: format for numerical data
    &#34;&#34;&#34;
    simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
    if samples:
        return array(
            self.variables[observed_property][0, i, j].astype(kind)
            for i, j in samples
        )
    return (
        self.variables[observed_property][:, 0].astype(kind)
        if reduce_dim
        else self.variables[observed_property][:].astype(kind)
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Distance"><code class="flex name class">
<span>class <span class="ident">Distance</span></span>
<span>(</span><span>value: float, unit: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Special distance type</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Distance:
    &#34;&#34;&#34;Special distance type&#34;&#34;&#34;
    value: float = attr.ib()
    unit: str = attr.ib()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList"><code class="flex name class">
<span>class <span class="ident">DoublyLinkedList</span></span>
<span>(</span><span>data: (float,) = ())</span>
</code></dt>
<dd>
<div class="desc"><p>LL abstraction</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoublyLinkedList(LinkedList):
    prev = None  # only for doubly-linked

    def __init__(self, data: (float,) = ()):
        LinkedList.__init__(self, data)
        cursor = self.head
        while cursor.next is not None:
            cursor.next.prev = cursor

    def k_from_end(self, n: int = None) -&gt; None or LinkedListNode:

        _next = self.tail
        _last = None
        while _next is not None and (n is None or n):
            _last = _next
            _next = _next.prev
            if n:
                n -= 1
        return _last

    def traverse_backward(self) -&gt; None:
        cursor = self.tail
        while cursor is not None:
            print(cursor.value)
            cursor = cursor.prev

    def push_front(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.next = self.head
        self.head.prev = n
        self.head = n

    def push_back(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.prev = self.tail
        if self.head is None:
            self.head = n
        if self.tail is not None:
            self.tail.next = n
        self.tail = n

    def insert_after(self, insert: LinkedListNode, ref: LinkedListNode):
        ...

    def insert_before(self, insert: LinkedListNode, ref: LinkedListNode):
        ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bathysphere.datatypes.LinkedList" href="#bathysphere.datatypes.LinkedList">LinkedList</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.DoublyLinkedList.prev"><code class="name">var <span class="ident">prev</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.DoublyLinkedList.insert_after"><code class="name flex">
<span>def <span class="ident">insert_after</span></span>(<span>self, insert: LinkedListNode, ref: LinkedListNode)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert_after(self, insert: LinkedListNode, ref: LinkedListNode):
    ...</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList.insert_before"><code class="name flex">
<span>def <span class="ident">insert_before</span></span>(<span>self, insert: LinkedListNode, ref: LinkedListNode)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert_before(self, insert: LinkedListNode, ref: LinkedListNode):
    ...</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList.k_from_end"><code class="name flex">
<span>def <span class="ident">k_from_end</span></span>(<span>self, n: int = None) -> <a title="bathysphere.datatypes.LinkedListNode" href="#bathysphere.datatypes.LinkedListNode">LinkedListNode</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def k_from_end(self, n: int = None) -&gt; None or LinkedListNode:

    _next = self.tail
    _last = None
    while _next is not None and (n is None or n):
        _last = _next
        _next = _next.prev
        if n:
            n -= 1
    return _last</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList.push_back"><code class="name flex">
<span>def <span class="ident">push_back</span></span>(<span>self, value: float) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_back(self, value: float) -&gt; None:
    n = LinkedListNode(value)
    n.prev = self.tail
    if self.head is None:
        self.head = n
    if self.tail is not None:
        self.tail.next = n
    self.tail = n</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList.push_front"><code class="name flex">
<span>def <span class="ident">push_front</span></span>(<span>self, value: float) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_front(self, value: float) -&gt; None:
    n = LinkedListNode(value)
    n.next = self.head
    self.head.prev = n
    self.head = n</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.DoublyLinkedList.traverse_backward"><code class="name flex">
<span>def <span class="ident">traverse_backward</span></span>(<span>self) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def traverse_backward(self) -&gt; None:
    cursor = self.tail
    while cursor is not None:
        print(cursor.value)
        cursor = cursor.prev</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="bathysphere.datatypes.LinkedList" href="#bathysphere.datatypes.LinkedList">LinkedList</a></b></code>:
<ul class="hlist">
<li><code><a title="bathysphere.datatypes.LinkedList.deduplicate" href="#bathysphere.datatypes.LinkedList.deduplicate">deduplicate</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.k_from_head" href="#bathysphere.datatypes.LinkedList.k_from_head">k_from_head</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.traverse" href="#bathysphere.datatypes.LinkedList.traverse">traverse</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="bathysphere.datatypes.Extent"><code class="flex name class">
<span>class <span class="ident">Extent</span></span>
<span>(</span><span>value: ExtentType)</span>
</code></dt>
<dd>
<div class="desc"><p>Extents speed up relational queries</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Extent:
    &#34;&#34;&#34;Extents speed up relational queries&#34;&#34;&#34;
    value: ExtentType = attr.ib()

    def __call__(self):
        &#34;&#34;&#34;Unwrap the extent value when calling instance&#34;&#34;&#34;
        return self.value

    @property
    def vertex_array(self):
        &#34;&#34;&#34;
        Convert an Extent to a VertexArray
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])

    @property
    def bounding_box(self):
        &#34;&#34;&#34;
        Convert an Extent to a BoundingBox
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[3]]])

    @property
    def path(self) -&gt; Path:
        &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
        ext = self.value
        xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
        return Path(xy)

    @property
    def intervals(self):
        &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
        return (
            Interval(Bound(self.value[0]), Bound(self.value[1])),
            Interval(Bound(self.value[2]), Bound(self.value[3]))
        )


    def __add__(self, other: Extent) -&gt; Extent:
        &#34;&#34;&#34;
        Reduce extents through addition
        &#34;&#34;&#34;
        dat = zip(self.value, other.value)
        return min(next(dat)), max(next(dat)), min(next(dat)), max(next(dat))


    def overlaps(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        def _mapped(item: (Extent, Extent)):
            a, b = item
            return a.overlaps(b)

        return all(map(_mapped, zip(self.intervals, other.intervals)))


    def __contains__(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly contains B
        &#34;&#34;&#34;
        a, b = self.intervals
        c, d = other.intervals

        return c in a and d in b</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Extent.bounding_box"><code class="name">var <span class="ident">bounding_box</span></code></dt>
<dd>
<div class="desc"><p>Convert an Extent to a BoundingBox</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bounding_box(self):
    &#34;&#34;&#34;
    Convert an Extent to a BoundingBox
    &#34;&#34;&#34;
    e = self.value
    return array([[e[0], e[2]], [e[1], e[3]]])</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Extent.intervals"><code class="name">var <span class="ident">intervals</span></code></dt>
<dd>
<div class="desc"><p>Split extent into two intervals for easier parametric comparison</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def intervals(self):
    &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
    return (
        Interval(Bound(self.value[0]), Bound(self.value[1])),
        Interval(Bound(self.value[2]), Bound(self.value[3]))
    )</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Extent.path"><code class="name">var <span class="ident">path</span> : matplotlib.path.Path</code></dt>
<dd>
<div class="desc"><p>Get extent as a closed Path</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def path(self) -&gt; Path:
    &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
    ext = self.value
    xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
    return Path(xy)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Extent.vertex_array"><code class="name">var <span class="ident">vertex_array</span></code></dt>
<dd>
<div class="desc"><p>Convert an Extent to a VertexArray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def vertex_array(self):
    &#34;&#34;&#34;
    Convert an Extent to a VertexArray
    &#34;&#34;&#34;
    e = self.value
    return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Extent.overlaps"><code class="name flex">
<span>def <span class="ident">overlaps</span></span>(<span>self, other: Extent) -> bool</span>
</code></dt>
<dd>
<div class="desc"><p>A wholly or partially contains B</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overlaps(self, other: Extent) -&gt; bool:
    &#34;&#34;&#34;
    A wholly or partially contains B
    &#34;&#34;&#34;
    def _mapped(item: (Extent, Extent)):
        a, b = item
        return a.overlaps(b)

    return all(map(_mapped, zip(self.intervals, other.intervals)))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Feature"><code class="flex name class">
<span>class <span class="ident">Feature</span></span>
</code></dt>
<dd>
<div class="desc"><p>Format as GeoJSON feature</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Feature:
    &#34;&#34;&#34;
    Format as GeoJSON feature
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    geometry: [[float]] = attr.Factory(list)
    properties: dict = attr.Factory(dict)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Feature.geometry"><code class="name">var <span class="ident">geometry</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Feature.properties"><code class="name">var <span class="ident">properties</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Feature.type"><code class="name">var <span class="ident">type</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection"><code class="flex name class">
<span>class <span class="ident">FeatureCollection</span></span>
</code></dt>
<dd>
<div class="desc"><p>GeoJSON feature collection</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureCollection:
    &#34;&#34;&#34;
    GeoJSON feature collection
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    features: [Feature] = attr.Factory(list)
    properties: dict = attr.Factory(dict)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FeatureCollection.features"><code class="name">var <span class="ident">features</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection.properties"><code class="name">var <span class="ident">properties</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection.type"><code class="name">var <span class="ident">type</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Field"><code class="flex name class">
<span>class <span class="ident">Field</span></span>
<span>(</span><span>name: Any, type: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Column for Postgres table</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Field:
    &#34;&#34;&#34;Column for Postgres table&#34;&#34;&#34;

    name: Any = attr.ib()
    type: str = attr.ib()

    @staticmethod
    def autoCorrect(
        key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
    ) -&gt; str:
        &#34;&#34;&#34;
        Match fieldnames probabilistically
        &#34;&#34;&#34;
        fields = lookup.keys()
        seq = SequenceMatcher(isjunk=None, autojunk=False)

        def _score(x):
            seq.set_seqs(key.lower(), x.lower())
            return seq.ratio()

        def _reduce(a, b):
            return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

        return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))

    @staticmethod
    def restore(final, units):
        # type: ((str, ), (str, )) -&gt; (str,)
        &#34;&#34;&#34;
        Get the original header name back by reversing clean_fields() operation.
        &#34;&#34;&#34;
        names = map(
            lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
        )
        return tuple(
            map(
                lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
                zip(names, units),
            )
        )

    @staticmethod
    def clean(fields: (str,)) -&gt; ((str,), (str,)):
        &#34;&#34;&#34;
        Make friendly formats for object and table naming. The inverse is restore_fields().
        &#34;&#34;&#34;

        def _clean(x):
            return (
                x.strip()
                .replace(&#34; &#34;, &#34;_&#34;)
                .replace(&#34;%&#34;, &#34;_percent&#34;)
                .replace(&#34;+&#34;, &#34;_plus&#34;)
                .replace(&#34;-&#34;, &#34;_minus&#34;)
            )

        return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Field.autoCorrect"><code class="name flex">
<span>def <span class="ident">autoCorrect</span></span>(<span>key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25) -> str</span>
</code></dt>
<dd>
<div class="desc"><p>Match fieldnames probabilistically</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def autoCorrect(
    key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
) -&gt; str:
    &#34;&#34;&#34;
    Match fieldnames probabilistically
    &#34;&#34;&#34;
    fields = lookup.keys()
    seq = SequenceMatcher(isjunk=None, autojunk=False)

    def _score(x):
        seq.set_seqs(key.lower(), x.lower())
        return seq.ratio()

    def _reduce(a, b):
        return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

    return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Field.clean"><code class="name flex">
<span>def <span class="ident">clean</span></span>(<span>fields: (str,)) -> '((str,), (str,))'</span>
</code></dt>
<dd>
<div class="desc"><p>Make friendly formats for object and table naming. The inverse is restore_fields().</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clean(fields: (str,)) -&gt; ((str,), (str,)):
    &#34;&#34;&#34;
    Make friendly formats for object and table naming. The inverse is restore_fields().
    &#34;&#34;&#34;

    def _clean(x):
        return (
            x.strip()
            .replace(&#34; &#34;, &#34;_&#34;)
            .replace(&#34;%&#34;, &#34;_percent&#34;)
            .replace(&#34;+&#34;, &#34;_plus&#34;)
            .replace(&#34;-&#34;, &#34;_minus&#34;)
        )

    return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Field.restore"><code class="name flex">
<span>def <span class="ident">restore</span></span>(<span>final, units)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the original header name back by reversing clean_fields() operation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def restore(final, units):
    # type: ((str, ), (str, )) -&gt; (str,)
    &#34;&#34;&#34;
    Get the original header name back by reversing clean_fields() operation.
    &#34;&#34;&#34;
    names = map(
        lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
    )
    return tuple(
        map(
            lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
            zip(names, units),
        )
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.File"><code class="flex name class">
<span>class <span class="ident">File</span></span>
<span>(</span><span>name: str = '', sn: int = None, url: str = None, time: datetime = None, ts: datetime = NOTHING, kb: float = 0.0, encoding: str = None, content: Any = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Originally used for Satlantic files, repurposed as general file system object.</p>
<p>Very similar to Assets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class File:
    &#34;&#34;&#34;
    Originally used for Satlantic files, repurposed as general file system object.

    Very similar to Assets.
    &#34;&#34;&#34;
    name: str = attr.ib(default=&#34;&#34;)
    sn: int = attr.ib(default=None)
    url: str = attr.ib(default=None)
    time: datetime = attr.ib(default=None)
    ts: datetime = attr.ib(default=attr.Factory(datetime.now))
    kb: float = attr.ib(default=0.0)
    encoding: str = attr.ib(default=None)
    content: Any = attr.ib(default=None)

    def __repr__(self):
        &#34;&#34;&#34;Print formatting&#34;&#34;&#34;
        return &#34;{} ({}): {}&#34;.format(self.__class__.__name__, self.encoding, self.name)

    def __cmp__(self, other):
        &#34;&#34;&#34;Compare wrapper&#34;&#34;&#34;
        if hasattr(other, &#34;sort_key&#34;):
            return self.sort_key().__cmp__(other.sort_key())

    def serialize(self):
        &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
        return {
            &#34;url&#34;: self.url,
            &#34;ts&#34;: self.ts,
            &#34;kb&#34;: self.kb,
            &#34;encoding&#34;: self.encoding,
            &#34;content&#34;: self.content,
        }

    def sort_key(self):
        &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
        return self.time

    async def get_and_decode(self, headers, auth, span=32, sn=None):
        # type: (dict, str, int, int) -&gt; str or list
        &#34;&#34;&#34;
        Run a batch of jobs with processor pool and collapse results. This will use the file descriptions to
        retrieve and format object remote raw file, with partially parsed observations in binary buffer.

        Split the binary buffer into labeled frames. A frame corresponds to data from a single sensor/process,
        which may become mingled by the data-logger before serialization and transmission.
        &#34;&#34;&#34;
        response = get(self.url, auth=auth)
        if not response.ok:
            return response

        if self.encoding == FileType.XML:
            self.content = response.content.decode()  #

        elif self.encoding == FileType.Config:
            parts = (&#34;sensor&#34;, &#34;frame&#34;, &#34;parameter&#34;)
            self.content = deque(
                dict(zip(parts, each.decode().split(&#34;:&#34;)))
                for each in response.content.split(b&#34;\n&#34;)
                if each is not b&#34;&#34;
            )  # frames

        elif self.encoding == FileType.Raw:
            breaks = dict()
            for key in headers.keys():
                bkey = key.encode()
                cursor = response.content.find(bkey)
                while cursor != -1:
                    breaks[cursor] = key
                    cursor = response.content.find(bkey, cursor + span)

            breaks = list(breaks.keys())
            breaks.append(len(response.content))
            sorted_breaks = sorted(breaks)  # add end of buffer as the last stop

            self.content = deque(
                Frame(
                    data=response.content[start:end],
                    key=breaks[start],
                    sn=sn,
                    headers=headers,
                )
                for start, end in zip(sorted_breaks[:-1], sorted_breaks[1:])
            )

        self.content = response.content

    @classmethod
    def metadata(cls, url: str, filename: str, ts: str, size: str):
        &#34;&#34;&#34;
        Create a file metadata object
        &#34;&#34;&#34;
        fields = filename.split(&#34;.&#34;)
        encoding = None
        if len(fields) &gt; 1:
            fmt = fields.pop()
            if &#34;sensors&#34; == fmt:
                encoding = FileType.Config
            elif &#34;xml&#34; == fmt:
                encoding = FileType.Schema
            elif &#34;raw&#34; == fmt:
                encoding = FileType.Raw
            elif &#34;txt&#34; == fmt:
                if fields[-1] == &#34;raw&#34;:
                    fields.pop()  # convention is to have &#34;.raw.txt&#34;
                encoding = FileType.Log

        time = None
        if len(fields) &gt; 1:  # dated files
            ft = fields.pop()
            try:
                dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
                time = datetime.strptime(ft, dt_fmt)
            except ValueError:
                pass

        try:
            sn = int(fields.pop())
        except ValueError:
            sn = None

        path = url + filename

        return cls(
            name=filename,
            sn=sn,  # maybe None
            url=path,  # retrieval path
            time=time,  # file time from name, maybe None
            ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
            kb=_parse_str_to_float(size),  # float kilobytes
            encoding=encoding,
        )

    def _match(self, fmt=None, identity=None):
        # type: (File, set, set) -&gt; bool
        &#34;&#34;&#34;Filter for file objects&#34;&#34;&#34;
        return (not identity or self.sn in identity) and (
            not fmt or self.encoding in fmt
        )

    @staticmethod
    async def metadata_promise(url, auth):
        # type: (str, str) -&gt; tuple
        &#34;&#34;&#34;
        Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
        &#34;&#34;&#34;
        response = get(url, auth=auth)
        if not response.ok:
            return response.content

        df = read_html(response.content, skiprows=3)[0]
        return tuple(
            File.metadata(url, *r)
            for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.File.metadata"><code class="name flex">
<span>def <span class="ident">metadata</span></span>(<span>url: str, filename: str, ts: str, size: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a file metadata object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def metadata(cls, url: str, filename: str, ts: str, size: str):
    &#34;&#34;&#34;
    Create a file metadata object
    &#34;&#34;&#34;
    fields = filename.split(&#34;.&#34;)
    encoding = None
    if len(fields) &gt; 1:
        fmt = fields.pop()
        if &#34;sensors&#34; == fmt:
            encoding = FileType.Config
        elif &#34;xml&#34; == fmt:
            encoding = FileType.Schema
        elif &#34;raw&#34; == fmt:
            encoding = FileType.Raw
        elif &#34;txt&#34; == fmt:
            if fields[-1] == &#34;raw&#34;:
                fields.pop()  # convention is to have &#34;.raw.txt&#34;
            encoding = FileType.Log

    time = None
    if len(fields) &gt; 1:  # dated files
        ft = fields.pop()
        try:
            dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
            time = datetime.strptime(ft, dt_fmt)
        except ValueError:
            pass

    try:
        sn = int(fields.pop())
    except ValueError:
        sn = None

    path = url + filename

    return cls(
        name=filename,
        sn=sn,  # maybe None
        url=path,  # retrieval path
        time=time,  # file time from name, maybe None
        ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
        kb=_parse_str_to_float(size),  # float kilobytes
        encoding=encoding,
    )</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.File.metadata_promise"><code class="name flex">
<span>async def <span class="ident">metadata_promise</span></span>(<span>url, auth)</span>
</code></dt>
<dd>
<div class="desc"><p>Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
async def metadata_promise(url, auth):
    # type: (str, str) -&gt; tuple
    &#34;&#34;&#34;
    Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
    &#34;&#34;&#34;
    response = get(url, auth=auth)
    if not response.ok:
        return response.content

    df = read_html(response.content, skiprows=3)[0]
    return tuple(
        File.metadata(url, *r)
        for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.File.get_and_decode"><code class="name flex">
<span>async def <span class="ident">get_and_decode</span></span>(<span>self, headers, auth, span=32, sn=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a batch of jobs with processor pool and collapse results. This will use the file descriptions to
retrieve and format object remote raw file, with partially parsed observations in binary buffer.</p>
<p>Split the binary buffer into labeled frames. A frame corresponds to data from a single sensor/process,
which may become mingled by the data-logger before serialization and transmission.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_and_decode(self, headers, auth, span=32, sn=None):
    # type: (dict, str, int, int) -&gt; str or list
    &#34;&#34;&#34;
    Run a batch of jobs with processor pool and collapse results. This will use the file descriptions to
    retrieve and format object remote raw file, with partially parsed observations in binary buffer.

    Split the binary buffer into labeled frames. A frame corresponds to data from a single sensor/process,
    which may become mingled by the data-logger before serialization and transmission.
    &#34;&#34;&#34;
    response = get(self.url, auth=auth)
    if not response.ok:
        return response

    if self.encoding == FileType.XML:
        self.content = response.content.decode()  #

    elif self.encoding == FileType.Config:
        parts = (&#34;sensor&#34;, &#34;frame&#34;, &#34;parameter&#34;)
        self.content = deque(
            dict(zip(parts, each.decode().split(&#34;:&#34;)))
            for each in response.content.split(b&#34;\n&#34;)
            if each is not b&#34;&#34;
        )  # frames

    elif self.encoding == FileType.Raw:
        breaks = dict()
        for key in headers.keys():
            bkey = key.encode()
            cursor = response.content.find(bkey)
            while cursor != -1:
                breaks[cursor] = key
                cursor = response.content.find(bkey, cursor + span)

        breaks = list(breaks.keys())
        breaks.append(len(response.content))
        sorted_breaks = sorted(breaks)  # add end of buffer as the last stop

        self.content = deque(
            Frame(
                data=response.content[start:end],
                key=breaks[start],
                sn=sn,
                headers=headers,
            )
            for start, end in zip(sorted_breaks[:-1], sorted_breaks[1:])
        )

    self.content = response.content</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.File.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Format as JSON style dictionary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self):
    &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
    return {
        &#34;url&#34;: self.url,
        &#34;ts&#34;: self.ts,
        &#34;kb&#34;: self.kb,
        &#34;encoding&#34;: self.encoding,
        &#34;content&#34;: self.content,
    }</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.File.sort_key"><code class="name flex">
<span>def <span class="ident">sort_key</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compare by time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_key(self):
    &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
    return self.time</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FileSystem"><code class="flex name class">
<span>class <span class="ident">FileSystem</span></span>
</code></dt>
<dd>
<div class="desc"><p>File systems are made up of files!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FileSystem:
    &#34;&#34;&#34;
    File systems are made up of files!
    &#34;&#34;&#34;
    @attr.s
    class OverwritePolicy:
        &#34;&#34;&#34;
        Basic logical unit for allowing/preventing mutability
        &#34;&#34;&#34;
        policy: str = attr.ib(default=&#34;never&#34;)

        def __call__(self, *args, **kwargs):
            if self == &#34;always&#34;:
                return True
            if self == &#34;prompt&#34;:
                print(&#34;Cache already exists. Overwrite? [y/N]&#34;)
                return input() in (&#34;Y&#34;, &#34;y&#34;)
            return False

    policy = OverwritePolicy(policy=&#34;never&#34;)

    @staticmethod
    def load_year_cache(local, years):
        # type: (str, (int, )) -&gt; dict
        &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
        combined = dict()
        for year in years:
            fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
            new = unpickle(fid)
            for key in new.keys():
                try:
                    combined[key] = append(combined[key], new[key])
                except KeyError:
                    combined[key] = array([])
                    combined[key] = append(combined[key], new[key])
        return combined

    @staticmethod
    def indexFileMetadata(url, year, auth=None):
        # type: (str, int, (str,)) -&gt; deque
        &#34;&#34;&#34;
        Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
        that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
        for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
        cached at a leisurely interactive pace.
        &#34;&#34;&#34;
        collector = deque()
        for record in resolveTaskTree(
            FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
        ):
            path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
            collector.append(
                {
                    &#34;date&#34;: date(*record),
                    &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                    &#34;url&#34;: path,
                    &#34;files&#34;: File.metadata_promise(path, auth=auth),
                }
            )
        return collector

    @staticmethod
    async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
        # type: (str, int, int, int, (str, )) -&gt; datetime or None
        &#34;&#34;&#34;
        Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

        Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
        into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
        using the `render()` method, until the specified depth is reached.
        &#34;&#34;&#34;

        def __parse(value):
            &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
            return value if type(value) == int else int(value[:-1])

        if count == depth:
            return enum, None

        try:
            formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
            insert = __parse(enum)
        except TypeError:
            return enum, None

        sublevel = formatter.format(url, insert)
        response = get(sublevel, auth=auth)
        if not response.ok:
            return enum, None

        collector = deque()
        for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
            collector.append(
                FileSystem.indexTaskTree(
                    url=sublevel,
                    enum=__parse(record),  # name
                    count=count + 1,
                    depth=depth,
                    auth=auth,
                )
            )

        return enum, collector

    @staticmethod
    def search(pattern, filesystem):
        # type: (str, dict) -&gt; None or str
        &#34;&#34;&#34;
        Recursively search a directory structure for a key.
        Call this on the result of `index`

        :param filesystem: paths
        :param pattern: search key
        :return:
        &#34;&#34;&#34;
        for key, level in filesystem.items():
            if key == pattern:
                return key
            try:
                result = FileSystem._search(pattern, level)
            except AttributeError:
                result = None
            if result:
                return f&#34;{key}/{result}&#34;
        return None

    @staticmethod
    def _search(
        queue: deque,
        pool: Pool,
        fmt: set = None,
        identity: set = None,
        ts: datetime = None
    ) -&gt; list or None:
        &#34;&#34;&#34;
        Get all XML and configuration files within a directory

        Find configurations from metadata by serial number and date.

        The files can be:
        - On a remote server
        - In the bathysphere_functions_cache
        - Supplied as a list of dictionaries
        &#34;&#34;&#34;
        iterators = []
        if identity:
            iterators.append(repeat(identity))
        if fmt:
            iterators.append(repeat(fmt))
        if ts:
            iterators.append(repeat(ts))

        def _chrono(x: File, ts: datetime = None):
            &#34;&#34;&#34;Chronoloigcal sorting method&#34;&#34;&#34;
            return (
                (x.time is None if ts else x.time is not None),
                (ts - x.time if ts else x.time),
            )

        queue = sorted(queue, key=_chrono, reverse=(False if ts else True))
        if fmt or identity:
            matching = pool.starmap(self._match, zip(queue, *iterators))
            queue = deque(queue)
        else:
            return {}, queue

        collector = dict()
        for condition in matching:
            if not condition:
                queue.rotate(1)
                continue
            file = queue.popleft()
            if not collector.get(file.sn, None):
                collector[file.sn] = deque()
            if (
                not ts or len(collector[file.sn]) == 0
            ):  # limit to length 1 for getting most recent
                collector[file.sn].append(file)
                continue

            queue.append(file)  # put the file back if unused

        return collector, queue

    @staticmethod
    def get_files(
        queue: deque, 
        pool: Pool, 
        **kwargs
    ):
        &#34;&#34;&#34;
        Create and process a day of raw files
        &#34;&#34;&#34;
        extracted, queue = FileSystem.search(
            queue=queue, pool=pool, **kwargs
        )  # get active configuration files
        headers = dict()
        for sn, files in extracted.keys():
            headers[sn] = deque()
            for file in files:
                synchronous(file.get_and_decode())
                if file.encoding == FileType.Config:
                    headers[sn].append(file.frames)

        return extracted, headers, queue

    @staticmethod
    def download(url, prefix=&#34;&#34;):
        # type: (str, str) -&gt; str
        &#34;&#34;&#34;
        Download a file accessible through HTTP/S.
        :param url: location of remote data
        :param prefix: local file path
        &#34;&#34;&#34;
        response = get(url, stream=True)
        filename = url.split(&#34;/&#34;).pop()
        if not response.ok:
            raise ConnectionError
        with open(f&#34;{prefix}{filename}&#34;, &#34;wb&#34;) as fid:
            copyfileobj(response.raw, fid)
        return filename

    def get(
        self,
        observed_properties,
        path=None,
        transpose=True,
        dataset=None,
        kind=&#34;float64&#34;,
        date=None,
    ):
        # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
        &#34;&#34;&#34;
        Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
        by name, resulting in an array. For previously processed internal data, arrays are stored as
        binary data in either `.pkl` or `.bathysphere_functions_cache` files.

        :param observed_properties: lookup field names
        :param path: path to local files if loading
        :param transpose: transpose the array before saving, makes join later easier
        :param dataset: NetCDF reference as in-memory object
        :param kind: numerical format for arrays
        :param date: specific timestamp to sample
        &#34;&#34;&#34;
        result = dict()

        if isinstance(observed_properties, str):
            fields = keys = [observed_properties]
        elif isinstance(observed_properties, dict):
            keys = observed_properties.keys()
            fields = observed_properties.values()
        else:
            fields = keys = observed_properties
        iterator = zip(*(keys, fields))

        for key, rename in iterator:
            if path:
                try:
                    fid = open(key, &#34;rb&#34;)
                except FileNotFoundError:
                    continue
                data = self.load_year_cache(fid).transpose() if transpose else self.load_year_cache(fid)
                fid.close()

            elif dataset:
                data = dataset.variables[key][:].astype(kind)
                self.set(date, data, key)
            else:
                data = None

            result[rename] = data

        return result

    @staticmethod
    def syncFtp(ftp, remote, local, filesystem=None):
        # type: (FTP, str, str, dict) -&gt; int
        &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
        path = FileSystem.search(pattern=remote, filesystem=filesystem)
        with open(local, &#34;wb+&#34;) as fid:
            return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))

    @staticmethod
    def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
        # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
        &#34;&#34;&#34;
        Build directory structure recursively.

        :param ftp: persistent ftp connection
        :param node: node in current working directory
        :param depth: current depth, do not set
        :param limit: maximum depth,
        :param metadata: pass the object metadata down one level
        :param parent:
        :return:
        &#34;&#34;&#34;

        body = loads(req)
        host = body.get(&#34;host&#34;, None)
        root = body.get(&#34;root&#34;, None)
        ftp = FTP(host, timeout=4)
        assert &#34;230&#34; in ftp.login()  # attach if no open socket
        assert ftp.sock
        if root is not None:
            _ = ftp.cwd(root)

        def _map(rec):
            values = rec.split()
            key = values.pop().strip()
            return {key: values}

        if depth == 0 and parent is None:
            parent = create(
                db=graph,
                obj=Locations(
                    **{&#34;name&#34;: &#34;FTP Server&#34;, &#34;description&#34;: &#34;Autogenerated FTP Server&#34;}
                ),
            )

        if limit is None or depth &lt;= limit:
            try:
                _ = ftp.cwd(node)  # target is a file
            except:
                create(
                    db=graph,
                    obj=Proxy(
                        **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                    ),
                    links=[parent],
                )

            else:
                collection = create(
                    db=graph,
                    obj=Proxy(
                        **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                    ),
                    links=[parent],
                )

                files = []
                ftp.retrlines(&#34;LIST&#34;, files.append)
                for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                    indexFtp(
                        ftp=ftp,
                        graph=graph,
                        node=k,
                        depth=depth + 1,
                        limit=limit,
                        metadata=v,
                        parent=collection,
                    )

                if node != &#34;.&#34;:
                    _ = ftp.cwd(&#34;..&#34;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.OverwritePolicy"><code class="name">var <span class="ident">OverwritePolicy</span></code></dt>
<dd>
<div class="desc"><p>Basic logical unit for allowing/preventing mutability</p></div>
</dd>
<dt id="bathysphere.datatypes.FileSystem.policy"><code class="name">var <span class="ident">policy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>url, prefix='')</span>
</code></dt>
<dd>
<div class="desc"><p>Download a file accessible through HTTP/S.
:param url: location of remote data
:param prefix: local file path</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def download(url, prefix=&#34;&#34;):
    # type: (str, str) -&gt; str
    &#34;&#34;&#34;
    Download a file accessible through HTTP/S.
    :param url: location of remote data
    :param prefix: local file path
    &#34;&#34;&#34;
    response = get(url, stream=True)
    filename = url.split(&#34;/&#34;).pop()
    if not response.ok:
        raise ConnectionError
    with open(f&#34;{prefix}{filename}&#34;, &#34;wb&#34;) as fid:
        copyfileobj(response.raw, fid)
    return filename</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.get_files"><code class="name flex">
<span>def <span class="ident">get_files</span></span>(<span>queue: deque, pool: Pool, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and process a day of raw files</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_files(
    queue: deque, 
    pool: Pool, 
    **kwargs
):
    &#34;&#34;&#34;
    Create and process a day of raw files
    &#34;&#34;&#34;
    extracted, queue = FileSystem.search(
        queue=queue, pool=pool, **kwargs
    )  # get active configuration files
    headers = dict()
    for sn, files in extracted.keys():
        headers[sn] = deque()
        for file in files:
            synchronous(file.get_and_decode())
            if file.encoding == FileType.Config:
                headers[sn].append(file.frames)

    return extracted, headers, queue</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexFileMetadata"><code class="name flex">
<span>def <span class="ident">indexFileMetadata</span></span>(<span>url, year, auth=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
that contains a <coroutine> in the place of file meta_data. This only takes a few seconds, compared to minutes
for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
cached at a leisurely interactive pace.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def indexFileMetadata(url, year, auth=None):
    # type: (str, int, (str,)) -&gt; deque
    &#34;&#34;&#34;
    Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
    that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
    for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
    cached at a leisurely interactive pace.
    &#34;&#34;&#34;
    collector = deque()
    for record in resolveTaskTree(
        FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
    ):
        path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
        collector.append(
            {
                &#34;date&#34;: date(*record),
                &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                &#34;url&#34;: path,
                &#34;files&#34;: File.metadata_promise(path, auth=auth),
            }
        )
    return collector</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexFtp"><code class="name flex">
<span>def <span class="ident">indexFtp</span></span>(<span>req, node='.', depth=0, limit=None, metadata=None, parent=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Build directory structure recursively.</p>
<p>:param ftp: persistent ftp connection
:param node: node in current working directory
:param depth: current depth, do not set
:param limit: maximum depth,
:param metadata: pass the object metadata down one level
:param parent:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
    # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
    &#34;&#34;&#34;
    Build directory structure recursively.

    :param ftp: persistent ftp connection
    :param node: node in current working directory
    :param depth: current depth, do not set
    :param limit: maximum depth,
    :param metadata: pass the object metadata down one level
    :param parent:
    :return:
    &#34;&#34;&#34;

    body = loads(req)
    host = body.get(&#34;host&#34;, None)
    root = body.get(&#34;root&#34;, None)
    ftp = FTP(host, timeout=4)
    assert &#34;230&#34; in ftp.login()  # attach if no open socket
    assert ftp.sock
    if root is not None:
        _ = ftp.cwd(root)

    def _map(rec):
        values = rec.split()
        key = values.pop().strip()
        return {key: values}

    if depth == 0 and parent is None:
        parent = create(
            db=graph,
            obj=Locations(
                **{&#34;name&#34;: &#34;FTP Server&#34;, &#34;description&#34;: &#34;Autogenerated FTP Server&#34;}
            ),
        )

    if limit is None or depth &lt;= limit:
        try:
            _ = ftp.cwd(node)  # target is a file
        except:
            create(
                db=graph,
                obj=Proxy(
                    **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                ),
                links=[parent],
            )

        else:
            collection = create(
                db=graph,
                obj=Proxy(
                    **{&#34;name&#34;: node, &#34;description&#34;: &#34;Autogenerated&#34;, &#34;url&#34;: node}
                ),
                links=[parent],
            )

            files = []
            ftp.retrlines(&#34;LIST&#34;, files.append)
            for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                indexFtp(
                    ftp=ftp,
                    graph=graph,
                    node=k,
                    depth=depth + 1,
                    limit=limit,
                    metadata=v,
                    parent=collection,
                )

            if node != &#34;.&#34;:
                _ = ftp.cwd(&#34;..&#34;)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexTaskTree"><code class="name flex">
<span>async def <span class="ident">indexTaskTree</span></span>(<span>url, enum, count=0, depth=2, auth=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Private method is used by <code>metadata()</code> to build a temporal index with multiple levels of resolution on demand.</p>
<p>Recursively <code>GET</code> file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
into nested tuples of (index, <coroutine>). The coroutine is then resolved to another (index, <coroutine>) tuple,
using the <code>render()</code> method, until the specified depth is reached.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
    # type: (str, int, int, int, (str, )) -&gt; datetime or None
    &#34;&#34;&#34;
    Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

    Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
    into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
    using the `render()` method, until the specified depth is reached.
    &#34;&#34;&#34;

    def __parse(value):
        &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
        return value if type(value) == int else int(value[:-1])

    if count == depth:
        return enum, None

    try:
        formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
        insert = __parse(enum)
    except TypeError:
        return enum, None

    sublevel = formatter.format(url, insert)
    response = get(sublevel, auth=auth)
    if not response.ok:
        return enum, None

    collector = deque()
    for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
        collector.append(
            FileSystem.indexTaskTree(
                url=sublevel,
                enum=__parse(record),  # name
                count=count + 1,
                depth=depth,
                auth=auth,
            )
        )

    return enum, collector</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.load_year_cache"><code class="name flex">
<span>def <span class="ident">load_year_cache</span></span>(<span>local, years)</span>
</code></dt>
<dd>
<div class="desc"><p>Load a local binary file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def load_year_cache(local, years):
    # type: (str, (int, )) -&gt; dict
    &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
    combined = dict()
    for year in years:
        fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
        new = unpickle(fid)
        for key in new.keys():
            try:
                combined[key] = append(combined[key], new[key])
            except KeyError:
                combined[key] = array([])
                combined[key] = append(combined[key], new[key])
    return combined</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>pattern, filesystem)</span>
</code></dt>
<dd>
<div class="desc"><p>Recursively search a directory structure for a key.
Call this on the result of <code>index</code></p>
<p>:param filesystem: paths
:param pattern: search key
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def search(pattern, filesystem):
    # type: (str, dict) -&gt; None or str
    &#34;&#34;&#34;
    Recursively search a directory structure for a key.
    Call this on the result of `index`

    :param filesystem: paths
    :param pattern: search key
    :return:
    &#34;&#34;&#34;
    for key, level in filesystem.items():
        if key == pattern:
            return key
        try:
            result = FileSystem._search(pattern, level)
        except AttributeError:
            result = None
        if result:
            return f&#34;{key}/{result}&#34;
    return None</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.syncFtp"><code class="name flex">
<span>def <span class="ident">syncFtp</span></span>(<span>ftp, remote, local, filesystem=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Find and copy a file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def syncFtp(ftp, remote, local, filesystem=None):
    # type: (FTP, str, str, dict) -&gt; int
    &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
    path = FileSystem.search(pattern=remote, filesystem=filesystem)
    with open(local, &#34;wb+&#34;) as fid:
        return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, observed_properties, path=None, transpose=True, dataset=None, kind='float64', date=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
by name, resulting in an array. For previously processed internal data, arrays are stored as
binary data in either <code>.pkl</code> or <code>.bathysphere_functions_cache</code> files.</p>
<p>:param observed_properties: lookup field names
:param path: path to local files if loading
:param transpose: transpose the array before saving, makes join later easier
:param dataset: NetCDF reference as in-memory object
:param kind: numerical format for arrays
:param date: specific timestamp to sample</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(
    self,
    observed_properties,
    path=None,
    transpose=True,
    dataset=None,
    kind=&#34;float64&#34;,
    date=None,
):
    # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
    &#34;&#34;&#34;
    Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
    by name, resulting in an array. For previously processed internal data, arrays are stored as
    binary data in either `.pkl` or `.bathysphere_functions_cache` files.

    :param observed_properties: lookup field names
    :param path: path to local files if loading
    :param transpose: transpose the array before saving, makes join later easier
    :param dataset: NetCDF reference as in-memory object
    :param kind: numerical format for arrays
    :param date: specific timestamp to sample
    &#34;&#34;&#34;
    result = dict()

    if isinstance(observed_properties, str):
        fields = keys = [observed_properties]
    elif isinstance(observed_properties, dict):
        keys = observed_properties.keys()
        fields = observed_properties.values()
    else:
        fields = keys = observed_properties
    iterator = zip(*(keys, fields))

    for key, rename in iterator:
        if path:
            try:
                fid = open(key, &#34;rb&#34;)
            except FileNotFoundError:
                continue
            data = self.load_year_cache(fid).transpose() if transpose else self.load_year_cache(fid)
            fid.close()

        elif dataset:
            data = dataset.variables[key][:].astype(kind)
            self.set(date, data, key)
        else:
            data = None

        result[rename] = data

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FileType"><code class="flex name class">
<span>class <span class="ident">FileType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Well known file types</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FileType(Enum):
    &#34;&#34;&#34;Well known file types&#34;&#34;&#34;
    Schema = 1
    Config = 2
    Log = 3
    Raw = 4
    CSV = 5
    JSON = 6
    XML = 7</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FileType.CSV"><code class="name">var <span class="ident">CSV</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Config"><code class="name">var <span class="ident">Config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.JSON"><code class="name">var <span class="ident">JSON</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Log"><code class="name">var <span class="ident">Log</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Raw"><code class="name">var <span class="ident">Raw</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Schema"><code class="name">var <span class="ident">Schema</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.XML"><code class="name">var <span class="ident">XML</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Frame"><code class="flex name class">
<span>class <span class="ident">Frame</span></span>
<span>(</span><span>data: bytes, label: bytes, headers: dict, sn: int, schema: dict, key: str = None, span: int = 32, ts: datetime = None, dict: dict = NOTHING)</span>
</code></dt>
<dd>
<div class="desc"><p>Data frames are partially parsed binary messages, usually from sensor streams</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Frame(dict):
    &#34;&#34;&#34;
    Data frames are partially parsed binary messages, usually from sensor streams
    &#34;&#34;&#34;
    data: bytes = attr.ib()
    label: bytes = attr.ib()
    headers: dict = attr.ib()
    sn: int = attr.ib()
    schema: dict = attr.ib()
    key: str = attr.ib(default=None)
    span: int = attr.ib(default=32)
    ts: datetime = attr.ib(default=None)
    _dict: dict = attr.ib(default=attr.Factory(dict))

    def goto(self, pattern: bytes, root: str = &#34;SensorFieldGroup&#34;):
        &#34;&#34;&#34;Move cursor&#34;&#34;&#34;
        return [self._dict[key][root] for key in self._dict.keys() if pattern in key][0]

    def wqm(self, keys: (str,)) -&gt; dict:
        &#34;&#34;&#34;
        Decode dataframe form water quality monitor instrument
        &#34;&#34;&#34;
        self.sensor = self.bytes[:9].decode()
        self.time = datetime.strptime(
            self.bytes[20:26].decode() + self.bytes[27:33].decode(), &#34;%m%d%y%H%M%S&#34;
        )
        assert self.time &lt; self.ts  # created date is before arrival time stamp

        self.data = {
            key: value for key, value in zip(keys, self.bytes[34:].decode().split(&#34;,&#34;))
        }
        return self

    def seafet(self, brk: int, keys: list, sep: bytes = b&#34;,&#34;) -&gt; None:
        &#34;&#34;&#34;vendor specific frame&#34;&#34;&#34;
        self.sensor = self.bytes[:brk].decode()
        assert self.sensor[3:6] == &#34;PHA&#34;
        data = self.bytes[brk + 1 :].split(sep)
        self.time = datetime.strptime(data[1].decode(), &#34;%Y%j&#34;) + timedelta(
            hours=float(data[2].decode())
        )
        self.data = {key: value.decode() for key, value in zip(keys, data[3:])}

    def by_key(self, frames: dict, headers: dict):
        &#34;&#34;&#34;sort frames by sensor id&#34;&#34;&#34;
        sn = int(self.label[-4:])
        pattern = self.bytes[:10].decode()
        if pattern[:3] == &#34;SAT&#34;:
            pattern = pattern

        key = [
            headers[sn][key]
            for key in headers[sn].keys()
            if pattern in headers[sn][key]
        ][0]
        loc = self.bytes.find(key.encode())
        buffer = self.bytes[loc + len(key) :]
        fr = [
            frames[sn][key][&#34;SensorFieldGroup&#34;]
            for key in frames[sn].keys()
            if pattern in headers[sn][key]
        ][0]

        binary = ((True for key in each.keys() if (&#34;Binary&#34; in key)) for each in fr)
        dat, extra = (Frame.binary_xml if any(binary) else Frame.ascii_xml)(buffer, fr)
        loc = extra.find(b&#34;\r\n&#34;)

        if self.data is None:
            self.data = []

        self.data = {
            &#34;content&#34;: dat,
            &#34;ts&#34;: TimeStamp.parseBinary(extra[loc + 2 : loc + 9]),
            &#34;type&#34;: &#34;sensor&#34;,
        }
        self.bytes = extra[loc + 9 :]
        self.size: len(self.bytes)

    def analog(self, headers: dict, width: int = 35, key: str = &#34;STORX&#34;):
        &#34;&#34;&#34;
        Parse analog frame

        :param frame: dictionary frame
        :param frames: dataframe description
        :param width: width of frame
        :param key: search pattern
        :return: updated frame
        &#34;&#34;&#34;
        sn = int(self.key[-4:])
        buffer = self.bytes[10:width]
        f = headers[sn].goto(key)
        values, extra = Frame.binary_xml(buffer, f)
        self.update(values)
        self.ts = TimeStamp.parseBinary(extra[:7])
        self.bytes = self.bytes[width:]
        self.size = len(self.bytes)

    def gps(self, headers: dict, key: bytes = b&#34;$GPRMC&#34;):
        &#34;&#34;&#34;Decode bytes as GPGGA or GPRMC location stream&#34;&#34;&#34;
        sn = int(self.key[-4:])
        loc = self.bytes.find(key)
        if loc == -1:
            return
        buffer = self.bytes[loc + len(key) + 1 :]
        f = headers[sn].goto(&#34;MODEM&#34;)
        nav, extra = Frame.ascii_xml(buffer, f)
        self.data = {
            &#34;content&#34;: nav,
            &#34;ts&#34;: TimeStamp.parseBinary(extra[2:9]),
            &#34;type&#34;: &#34;nav&#34;,
        }
        self.bytes = extra[9:]
        self[&#34;size&#34;] = len(self.bytes)

    @staticmethod
    def line(txt: str, bytes_string: bytes):
        &#34;&#34;&#34;Single line&#34;&#34;&#34;
        keys = [b&#34;SAT&#34;, b&#34;WQM&#34;]
        lines = txt.split(bytes_string, keys)
        results = []
        for each in lines:
            result = Frame()
            result.raw = each
            try:
                data, ts = each.split(b&#34;\r\n&#34;)
                result.ts = TimeStamp(ts)
            except ValueError:
                data = each
                result.ts = None

            result.bytes = data
            results.append(result)
        return results

    @staticmethod
    def ascii_xml(buffer: str, frames: list):
        &#34;&#34;&#34;XML&#34;&#34;&#34;
        result = dict()
        offset = 0
        delims = [each[&#34;SensorField&#34;][&#34;Delimiter&#34;].encode() for each in frames]
        for each, sep in zip(frames[:-1], delims[1:]):
            loc = buffer.find(sep, offset)
            count = loc - offset
            wd = count + 1
            name = each[&#34;Name&#34;]
            result[name] = buffer[offset:loc]
            offset += wd
        end = offset + 2
        result[frames[-1][&#34;Name&#34;]] = buffer[offset:end]
        return result, buffer[end:]

    @staticmethod
    def binary_xml(buffer: bytes, frames: list, byteorder: str = &#34;&gt;&#34;) -&gt; (dict, bytes):
        &#34;&#34;&#34;
        Parse raw bytes according to format described in XML frames
        &#34;&#34;&#34;
        result = dict()
        offset = 0
        wc = {&#34;BF&#34;: 4, &#34;BD&#34;: 8, &#34;BS&#34;: 1, &#34;BU&#34;: 1, &#34;AF&#34;: 1, &#34;BULE&#34;: 1, &#34;BSLE&#34;: 1}

        for each in frames:
            keys = each.keys()
            dtype_key = [
                key for key in keys if (&#34;Binary&#34; in key and &#34;Data&#34; in key)
            ].pop()

            if &#34;BinaryFloatingPoint&#34; in dtype_key:
                txt = each[dtype_key]
                wd = wc[txt]
                np_type = byteorder + &#34;f&#34; + str(wd)

            elif &#34;BinaryInteger&#34; in dtype_key:
                txt = each[dtype_key][&#34;Type&#34;]
                wd = wc[txt] * int(each[dtype_key][&#34;Length&#34;])
                np_type = byteorder + &#34;u&#34; + str(wd)

            else:
                break

            name = each[&#34;Name&#34;]
            result[name] = frombuffer(buffer, dtype=np_type, count=1, offset=offset)
            offset += wd

        return result, buffer[offset:]

    def storx(
        self, fields: (Field,), name_length: int = 10, verb: bool = False
    ) -&gt; None:
        &#34;&#34;&#34;
        Decode and process Satlantic sensor frames if format is known, or fail silently

        :param frame: incoming frame dictionary structure
        :param fields: field mappings for known sensor formats
        :param name_length: maximum size for name search pattern, 10 is Satlantic standard
        :param verb: verbose mode

        :return: possibly processed frame
        &#34;&#34;&#34;

        delim = {
            &#34;PHA&#34;: b&#34;,&#34;,
            &#34;CST&#34;: b&#34;\t&#34;,
        }  # SEAFET pH instrument  # CSTAR transmissometer

        brk = self.bytes.find(b&#34;\t&#34;)
        if brk == -1 or brk &gt; name_length:
            brk = self.bytes.find(b&#34;\x00&#34;)
            if brk &gt; name_length:
                print(&#34;Error. Instrument name appears to be too long:&#34;, self.bytes[:32])
                return  # return unmodified frame

        self.sensor = self.bytes[:brk].decode()
        self.time = None
        self.data = None

        sensor = self.sensor[3:6]
        try:
            sep = delim[sensor]
        except KeyError:
            pass  # just copy bytes
        else:
            start = 1
            rest = self.bytes[brk + 1 :].split(sep)
            if sensor == &#34;PHA&#34;:
                self.seafet(brk, fields[sensor])
            else:
                try:
                    keys = fields[sensor]
                except KeyError:
                    self.data = rest[start:]
                else:
                    self.data = {
                        key: value.decode() for key, value in zip(keys, rest[start:])
                    }

        if verb and self.data.__class__.__name__ == &#34;dict&#34;:
            print(self.sensor, &#34;[&#34;, self.time, &#34;] ::&#34;, self.data)

    @staticmethod
    def parse_buffer_queue(
        queue: deque, sequence: list, pool: Pool, frames: list
    ) -&gt; (list, list):
        &#34;&#34;&#34;
        Create a job queue and use pool of workers to process byte strings until consumed
        &#34;&#34;&#34;
        processed = deque()
        for job in sequence:
            queue = pool.starmap(job, zip(queue, repeat(frames, len(queue))))
            processed.append(
                queue.pop(buffer) for buffer in queue if buffer[&#34;size&#34;] == 0
            )

        return processed, queue

    @staticmethod
    def _tree_depth(xml: str) -&gt; int:
        &#34;&#34;&#34;
        Get depth of tree
        &#34;&#34;&#34;

        class _Parser:
            maxDepth = 0
            depth = 0

            def start(self, tag, attrib):
                self.depth += 1
                if self.depth &gt; self.maxDepth:
                    self.maxDepth = self.depth

            def end(self, tag):
                self.depth -= 1

            def close(self):
                return self.maxDepth

        parser = ElementTree.XMLParser(target=_Parser())
        parser.feed(xml)
        return parser.close()

    def parse_xml_frames(
        self,
        config: dict,
        key: str = &#34;sensor&#34;,
        depth: int = 10,
        verb: bool = False
    ) -&gt; dict:
        &#34;&#34;&#34;
        Get frames for all sensors on platform

        :param config: xml style dictionary format with all configuration data for sensor platform
        :param key: key for configured items
        :return: dictionary of with sensors as keys, and dataframe schema as value
        &#34;&#34;&#34;

        def _goto(item):
            &#34;&#34;&#34;
            Start node of frame
            &#34;&#34;&#34;
            sensor = root.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;sensor&#34;] + &#34;&#39;]&#34;)[0]
            frame = sensor.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;frame&#34;] + &#34;&#39;]&#34;)[0]
            if verb:
                print(
                    &#34;Parsing from: . &gt;&#34;,
                    sensor.identifier,
                    &#34;&gt;&#34;,
                    self.identifier,
                )
            return frame

        ns = &#34;{http://www.satlantic.com/instrument}&#34;
        root = ElementTree.fromstring(config[&#34;xml&#34;][&#34;content&#34;])
        return {
            item[key]: Frame._collect(_goto(item), depth=depth, namespace=ns, verb=verb)
            for item in config[&#34;config&#34;][&#34;content&#34;]
        }

    @staticmethod
    def parse_xml(xml, depth=None, verb=False):
        &#34;&#34;&#34;
        Recursively collect XML sensor info as dict
        &#34;&#34;&#34;
        return Frame._collect(
            node=ElementTree.fromstring(xml),
            depth=depth if depth else Frame._tree_depth(xml),
            namespace=&#34;{http://www.satlantic.com/instrument}&#34;,
            verb=verb,
        )

    @staticmethod
    def _collect(
        node: ElementTree,
        depth: int,
        count: int = 0,
        namespace: str = None,
        verb: bool = False,
    ) -&gt; dict or None:
        &#34;&#34;&#34;
        Recursively collect child nodes and info.
        &#34;&#34;&#34;
        collector = dict()
        if count &gt;= depth:
            return None

        for child in node:
            below = Frame._collect(child, depth, count=count + 1, namespace=namespace)
            tag = sub(namespace, &#34;&#34;, child.tag)
            if below is None:
                collector[tag] = child.text
                continue

            queue = collector.get(tag, None)
            if queue is None:
                queue = collector[tag] = []
            queue.append(below)
            if verb:
                print(&#34;\t&#34; * count + &#34;&gt;&#34;, tag + &#34;:&#34;, collector[tag])

        return collector</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Frame.ascii_xml"><code class="name flex">
<span>def <span class="ident">ascii_xml</span></span>(<span>buffer: str, frames: list)</span>
</code></dt>
<dd>
<div class="desc"><p>XML</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def ascii_xml(buffer: str, frames: list):
    &#34;&#34;&#34;XML&#34;&#34;&#34;
    result = dict()
    offset = 0
    delims = [each[&#34;SensorField&#34;][&#34;Delimiter&#34;].encode() for each in frames]
    for each, sep in zip(frames[:-1], delims[1:]):
        loc = buffer.find(sep, offset)
        count = loc - offset
        wd = count + 1
        name = each[&#34;Name&#34;]
        result[name] = buffer[offset:loc]
        offset += wd
    end = offset + 2
    result[frames[-1][&#34;Name&#34;]] = buffer[offset:end]
    return result, buffer[end:]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.binary_xml"><code class="name flex">
<span>def <span class="ident">binary_xml</span></span>(<span>buffer: bytes, frames: list, byteorder: str = &#x27;&gt;&#x27;) -> '(dict, bytes)'</span>
</code></dt>
<dd>
<div class="desc"><p>Parse raw bytes according to format described in XML frames</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def binary_xml(buffer: bytes, frames: list, byteorder: str = &#34;&gt;&#34;) -&gt; (dict, bytes):
    &#34;&#34;&#34;
    Parse raw bytes according to format described in XML frames
    &#34;&#34;&#34;
    result = dict()
    offset = 0
    wc = {&#34;BF&#34;: 4, &#34;BD&#34;: 8, &#34;BS&#34;: 1, &#34;BU&#34;: 1, &#34;AF&#34;: 1, &#34;BULE&#34;: 1, &#34;BSLE&#34;: 1}

    for each in frames:
        keys = each.keys()
        dtype_key = [
            key for key in keys if (&#34;Binary&#34; in key and &#34;Data&#34; in key)
        ].pop()

        if &#34;BinaryFloatingPoint&#34; in dtype_key:
            txt = each[dtype_key]
            wd = wc[txt]
            np_type = byteorder + &#34;f&#34; + str(wd)

        elif &#34;BinaryInteger&#34; in dtype_key:
            txt = each[dtype_key][&#34;Type&#34;]
            wd = wc[txt] * int(each[dtype_key][&#34;Length&#34;])
            np_type = byteorder + &#34;u&#34; + str(wd)

        else:
            break

        name = each[&#34;Name&#34;]
        result[name] = frombuffer(buffer, dtype=np_type, count=1, offset=offset)
        offset += wd

    return result, buffer[offset:]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.line"><code class="name flex">
<span>def <span class="ident">line</span></span>(<span>txt: str, bytes_string: bytes)</span>
</code></dt>
<dd>
<div class="desc"><p>Single line</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def line(txt: str, bytes_string: bytes):
    &#34;&#34;&#34;Single line&#34;&#34;&#34;
    keys = [b&#34;SAT&#34;, b&#34;WQM&#34;]
    lines = txt.split(bytes_string, keys)
    results = []
    for each in lines:
        result = Frame()
        result.raw = each
        try:
            data, ts = each.split(b&#34;\r\n&#34;)
            result.ts = TimeStamp(ts)
        except ValueError:
            data = each
            result.ts = None

        result.bytes = data
        results.append(result)
    return results</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.parse_buffer_queue"><code class="name flex">
<span>def <span class="ident">parse_buffer_queue</span></span>(<span>queue: deque, sequence: list, pool: Pool, frames: list) -> '(list, list)'</span>
</code></dt>
<dd>
<div class="desc"><p>Create a job queue and use pool of workers to process byte strings until consumed</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parse_buffer_queue(
    queue: deque, sequence: list, pool: Pool, frames: list
) -&gt; (list, list):
    &#34;&#34;&#34;
    Create a job queue and use pool of workers to process byte strings until consumed
    &#34;&#34;&#34;
    processed = deque()
    for job in sequence:
        queue = pool.starmap(job, zip(queue, repeat(frames, len(queue))))
        processed.append(
            queue.pop(buffer) for buffer in queue if buffer[&#34;size&#34;] == 0
        )

    return processed, queue</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.parse_xml"><code class="name flex">
<span>def <span class="ident">parse_xml</span></span>(<span>xml, depth=None, verb=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Recursively collect XML sensor info as dict</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parse_xml(xml, depth=None, verb=False):
    &#34;&#34;&#34;
    Recursively collect XML sensor info as dict
    &#34;&#34;&#34;
    return Frame._collect(
        node=ElementTree.fromstring(xml),
        depth=depth if depth else Frame._tree_depth(xml),
        namespace=&#34;{http://www.satlantic.com/instrument}&#34;,
        verb=verb,
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Frame.analog"><code class="name flex">
<span>def <span class="ident">analog</span></span>(<span>self, headers: dict, width: int = 35, key: str = 'STORX')</span>
</code></dt>
<dd>
<div class="desc"><p>Parse analog frame</p>
<p>:param frame: dictionary frame
:param frames: dataframe description
:param width: width of frame
:param key: search pattern
:return: updated frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def analog(self, headers: dict, width: int = 35, key: str = &#34;STORX&#34;):
    &#34;&#34;&#34;
    Parse analog frame

    :param frame: dictionary frame
    :param frames: dataframe description
    :param width: width of frame
    :param key: search pattern
    :return: updated frame
    &#34;&#34;&#34;
    sn = int(self.key[-4:])
    buffer = self.bytes[10:width]
    f = headers[sn].goto(key)
    values, extra = Frame.binary_xml(buffer, f)
    self.update(values)
    self.ts = TimeStamp.parseBinary(extra[:7])
    self.bytes = self.bytes[width:]
    self.size = len(self.bytes)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.by_key"><code class="name flex">
<span>def <span class="ident">by_key</span></span>(<span>self, frames: dict, headers: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>sort frames by sensor id</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def by_key(self, frames: dict, headers: dict):
    &#34;&#34;&#34;sort frames by sensor id&#34;&#34;&#34;
    sn = int(self.label[-4:])
    pattern = self.bytes[:10].decode()
    if pattern[:3] == &#34;SAT&#34;:
        pattern = pattern

    key = [
        headers[sn][key]
        for key in headers[sn].keys()
        if pattern in headers[sn][key]
    ][0]
    loc = self.bytes.find(key.encode())
    buffer = self.bytes[loc + len(key) :]
    fr = [
        frames[sn][key][&#34;SensorFieldGroup&#34;]
        for key in frames[sn].keys()
        if pattern in headers[sn][key]
    ][0]

    binary = ((True for key in each.keys() if (&#34;Binary&#34; in key)) for each in fr)
    dat, extra = (Frame.binary_xml if any(binary) else Frame.ascii_xml)(buffer, fr)
    loc = extra.find(b&#34;\r\n&#34;)

    if self.data is None:
        self.data = []

    self.data = {
        &#34;content&#34;: dat,
        &#34;ts&#34;: TimeStamp.parseBinary(extra[loc + 2 : loc + 9]),
        &#34;type&#34;: &#34;sensor&#34;,
    }
    self.bytes = extra[loc + 9 :]
    self.size: len(self.bytes)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.goto"><code class="name flex">
<span>def <span class="ident">goto</span></span>(<span>self, pattern: bytes, root: str = 'SensorFieldGroup')</span>
</code></dt>
<dd>
<div class="desc"><p>Move cursor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def goto(self, pattern: bytes, root: str = &#34;SensorFieldGroup&#34;):
    &#34;&#34;&#34;Move cursor&#34;&#34;&#34;
    return [self._dict[key][root] for key in self._dict.keys() if pattern in key][0]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.gps"><code class="name flex">
<span>def <span class="ident">gps</span></span>(<span>self, headers: dict, key: bytes = b'$GPRMC')</span>
</code></dt>
<dd>
<div class="desc"><p>Decode bytes as GPGGA or GPRMC location stream</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gps(self, headers: dict, key: bytes = b&#34;$GPRMC&#34;):
    &#34;&#34;&#34;Decode bytes as GPGGA or GPRMC location stream&#34;&#34;&#34;
    sn = int(self.key[-4:])
    loc = self.bytes.find(key)
    if loc == -1:
        return
    buffer = self.bytes[loc + len(key) + 1 :]
    f = headers[sn].goto(&#34;MODEM&#34;)
    nav, extra = Frame.ascii_xml(buffer, f)
    self.data = {
        &#34;content&#34;: nav,
        &#34;ts&#34;: TimeStamp.parseBinary(extra[2:9]),
        &#34;type&#34;: &#34;nav&#34;,
    }
    self.bytes = extra[9:]
    self[&#34;size&#34;] = len(self.bytes)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.parse_xml_frames"><code class="name flex">
<span>def <span class="ident">parse_xml_frames</span></span>(<span>self, config: dict, key: str = 'sensor', depth: int = 10, verb: bool = False) -> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Get frames for all sensors on platform</p>
<p>:param config: xml style dictionary format with all configuration data for sensor platform
:param key: key for configured items
:return: dictionary of with sensors as keys, and dataframe schema as value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_xml_frames(
    self,
    config: dict,
    key: str = &#34;sensor&#34;,
    depth: int = 10,
    verb: bool = False
) -&gt; dict:
    &#34;&#34;&#34;
    Get frames for all sensors on platform

    :param config: xml style dictionary format with all configuration data for sensor platform
    :param key: key for configured items
    :return: dictionary of with sensors as keys, and dataframe schema as value
    &#34;&#34;&#34;

    def _goto(item):
        &#34;&#34;&#34;
        Start node of frame
        &#34;&#34;&#34;
        sensor = root.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;sensor&#34;] + &#34;&#39;]&#34;)[0]
        frame = sensor.findall(&#34;./*/[@identifier=&#39;&#34; + item[&#34;frame&#34;] + &#34;&#39;]&#34;)[0]
        if verb:
            print(
                &#34;Parsing from: . &gt;&#34;,
                sensor.identifier,
                &#34;&gt;&#34;,
                self.identifier,
            )
        return frame

    ns = &#34;{http://www.satlantic.com/instrument}&#34;
    root = ElementTree.fromstring(config[&#34;xml&#34;][&#34;content&#34;])
    return {
        item[key]: Frame._collect(_goto(item), depth=depth, namespace=ns, verb=verb)
        for item in config[&#34;config&#34;][&#34;content&#34;]
    }</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.seafet"><code class="name flex">
<span>def <span class="ident">seafet</span></span>(<span>self, brk: int, keys: list, sep: bytes = b',') -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>vendor specific frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seafet(self, brk: int, keys: list, sep: bytes = b&#34;,&#34;) -&gt; None:
    &#34;&#34;&#34;vendor specific frame&#34;&#34;&#34;
    self.sensor = self.bytes[:brk].decode()
    assert self.sensor[3:6] == &#34;PHA&#34;
    data = self.bytes[brk + 1 :].split(sep)
    self.time = datetime.strptime(data[1].decode(), &#34;%Y%j&#34;) + timedelta(
        hours=float(data[2].decode())
    )
    self.data = {key: value.decode() for key, value in zip(keys, data[3:])}</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.storx"><code class="name flex">
<span>def <span class="ident">storx</span></span>(<span>self, fields: (Field,), name_length: int = 10, verb: bool = False) -> 'None'</span>
</code></dt>
<dd>
<div class="desc"><p>Decode and process Satlantic sensor frames if format is known, or fail silently</p>
<p>:param frame: incoming frame dictionary structure
:param fields: field mappings for known sensor formats
:param name_length: maximum size for name search pattern, 10 is Satlantic standard
:param verb: verbose mode</p>
<p>:return: possibly processed frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def storx(
    self, fields: (Field,), name_length: int = 10, verb: bool = False
) -&gt; None:
    &#34;&#34;&#34;
    Decode and process Satlantic sensor frames if format is known, or fail silently

    :param frame: incoming frame dictionary structure
    :param fields: field mappings for known sensor formats
    :param name_length: maximum size for name search pattern, 10 is Satlantic standard
    :param verb: verbose mode

    :return: possibly processed frame
    &#34;&#34;&#34;

    delim = {
        &#34;PHA&#34;: b&#34;,&#34;,
        &#34;CST&#34;: b&#34;\t&#34;,
    }  # SEAFET pH instrument  # CSTAR transmissometer

    brk = self.bytes.find(b&#34;\t&#34;)
    if brk == -1 or brk &gt; name_length:
        brk = self.bytes.find(b&#34;\x00&#34;)
        if brk &gt; name_length:
            print(&#34;Error. Instrument name appears to be too long:&#34;, self.bytes[:32])
            return  # return unmodified frame

    self.sensor = self.bytes[:brk].decode()
    self.time = None
    self.data = None

    sensor = self.sensor[3:6]
    try:
        sep = delim[sensor]
    except KeyError:
        pass  # just copy bytes
    else:
        start = 1
        rest = self.bytes[brk + 1 :].split(sep)
        if sensor == &#34;PHA&#34;:
            self.seafet(brk, fields[sensor])
        else:
            try:
                keys = fields[sensor]
            except KeyError:
                self.data = rest[start:]
            else:
                self.data = {
                    key: value.decode() for key, value in zip(keys, rest[start:])
                }

    if verb and self.data.__class__.__name__ == &#34;dict&#34;:
        print(self.sensor, &#34;[&#34;, self.time, &#34;] ::&#34;, self.data)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Frame.wqm"><code class="name flex">
<span>def <span class="ident">wqm</span></span>(<span>self, keys: (str,)) -> 'dict'</span>
</code></dt>
<dd>
<div class="desc"><p>Decode dataframe form water quality monitor instrument</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wqm(self, keys: (str,)) -&gt; dict:
    &#34;&#34;&#34;
    Decode dataframe form water quality monitor instrument
    &#34;&#34;&#34;
    self.sensor = self.bytes[:9].decode()
    self.time = datetime.strptime(
        self.bytes[20:26].decode() + self.bytes[27:33].decode(), &#34;%m%d%y%H%M%S&#34;
    )
    assert self.time &lt; self.ts  # created date is before arrival time stamp

    self.data = {
        key: value for key, value in zip(keys, self.bytes[34:].decode().split(&#34;,&#34;))
    }
    return self</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Graph"><code class="flex name class">
<span>class <span class="ident">Graph</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract graphDB</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Graph:
    &#34;&#34;&#34;
    Abstract graphDB 
    &#34;&#34;&#34;
    @staticmethod
    def register(config: dict):

        if config[&#34;join&#34;] and not config[&#34;graph&#34;]:
            hosts = config[&#34;join&#34;].copy()
            while hosts:
                host = hosts.pop()
                response = get(config[&#34;graphHealthcheck&#34;].format(host))
                if response.ok:
                    config[&#34;graph&#34;] = host
                    break

        if config[&#34;graph&#34;] is not None:
            try:
                register = post(
                    config[&#34;graphAuth&#34;].format(config[&#34;graph&#34;]),
                    json={
                        &#34;email&#34;: config[&#34;graphUser&#34;],
                        &#34;password&#34;: config[&#34;graphPassword&#34;],
                        &#34;apiKey&#34;: config[&#34;graphApiKey&#34;],
                    },
                )
            except (ConnectionError, MaxRetryError):
                config[&#34;graph&#34;] = None
            else:
                assert register.ok

    @staticmethod
    def create(cls, obj: dict, url: str, token: str) -&gt; tuple or None:
        url = f&#34;{url}/{cls}&#34;
        return post(url=url, json=obj, headers={&#34;Authorization&#34;: f&#34;Bearer {token}&#34;})</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Graph.create"><code class="name flex">
<span>def <span class="ident">create</span></span>(<span>cls, obj: dict, url: str, token: str) -> tuple</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create(cls, obj: dict, url: str, token: str) -&gt; tuple or None:
    url = f&#34;{url}/{cls}&#34;
    return post(url=url, json=obj, headers={&#34;Authorization&#34;: f&#34;Bearer {token}&#34;})</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Graph.register"><code class="name flex">
<span>def <span class="ident">register</span></span>(<span>config: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def register(config: dict):

    if config[&#34;join&#34;] and not config[&#34;graph&#34;]:
        hosts = config[&#34;join&#34;].copy()
        while hosts:
            host = hosts.pop()
            response = get(config[&#34;graphHealthcheck&#34;].format(host))
            if response.ok:
                config[&#34;graph&#34;] = host
                break

    if config[&#34;graph&#34;] is not None:
        try:
            register = post(
                config[&#34;graphAuth&#34;].format(config[&#34;graph&#34;]),
                json={
                    &#34;email&#34;: config[&#34;graphUser&#34;],
                    &#34;password&#34;: config[&#34;graphPassword&#34;],
                    &#34;apiKey&#34;: config[&#34;graphApiKey&#34;],
                },
            )
        except (ConnectionError, MaxRetryError):
            config[&#34;graph&#34;] = None
        else:
            assert register.ok</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Interval"><code class="flex name class">
<span>class <span class="ident">Interval</span></span>
<span>(</span><span>lower: Bound = None, upper: Bound = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Intervals are convenience data structs for sorting and numerical queries</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Interval:
    &#34;&#34;&#34;Intervals are convenience data structs for sorting and numerical queries&#34;&#34;&#34;
    lower: Bound = attr.ib(default=None)
    upper: Bound = attr.ib(default=None)


    def overlaps(self, other: Interval) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.upper.value and 
            self.upper.value &gt;= other.lower.value
        )


    def __contains__(self, other: Interval):
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.lower.value and 
            self.upper.value &gt;= other.upper.value
        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Interval.overlaps"><code class="name flex">
<span>def <span class="ident">overlaps</span></span>(<span>self, other: Interval) -> bool</span>
</code></dt>
<dd>
<div class="desc"><p>A wholly or partially contains B</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overlaps(self, other: Interval) -&gt; bool:
    &#34;&#34;&#34;
    A wholly or partially contains B
    &#34;&#34;&#34;
    return (
        self.lower.value &lt;= other.upper.value and 
        self.upper.value &gt;= other.lower.value
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper"><code class="flex name class">
<span>class <span class="ident">JSONIOWrapper</span></span>
<span>(</span><span>buffer, encoding=None, errors=None, newline=None, line_buffering=False, write_through=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Use JSON messages piped between between processes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JSONIOWrapper(TextIOWrapper):
    &#34;&#34;&#34;
    Use JSON messages piped between between processes
    &#34;&#34;&#34;
    @staticmethod
    def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
        &#34;&#34;&#34;
        Log notifications.

        :param message: some event notification
        :param data: data that resulted in message
        :param log: log file or interface
        :param arrow: symbol indicating direction of flow

        :return:
        &#34;&#34;&#34;
        timestamp = datetime.now().isoformat(sep=&#34; &#34;)
        string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
        if log is not None:
            log.write((string + &#34;\n&#34;).encode())
            return None
        print(string)

    def receive(self, log: BytesIO) -&gt; dict:
        &#34;&#34;&#34;
        Receive serialized data from command line interface.
        &#34;&#34;&#34;
        json = self.readline()
        self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
        try:
            data = loads(json.rstrip())
        except decoder.JSONDecodeError as decode_error:
            self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
            message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
            return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

        return data

    def send(self, data: dict, log: BytesIO) -&gt; None:
        &#34;&#34;&#34;
        Write serialized data to interface.
        &#34;&#34;&#34;

        def _transform():
            safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
            return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

        json = _transform()
        self.log(message=&#34;Send&#34;, data=json, log=log)
        self.write(f&#34;{json}\n&#34;)

    def dump(self) -&gt; None:
        &#34;&#34;&#34;
        Propagates messages up through C#, subprocess, and control layers.
        &#34;&#34;&#34;
        response = self.readline()
        while response != &#34;&#34;:
            response = self.readline()
            print(response.rstrip())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>_io.TextIOWrapper</li>
<li>_io._TextIOBase</li>
<li>_io._IOBase</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.JSONIOWrapper.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>message: str, data: str, log: BytesIO = None, arrow: str = &#x27;-&gt;&#x27;) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Log notifications.</p>
<p>:param message: some event notification
:param data: data that resulted in message
:param log: log file or interface
:param arrow: symbol indicating direction of flow</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
    &#34;&#34;&#34;
    Log notifications.

    :param message: some event notification
    :param data: data that resulted in message
    :param log: log file or interface
    :param arrow: symbol indicating direction of flow

    :return:
    &#34;&#34;&#34;
    timestamp = datetime.now().isoformat(sep=&#34; &#34;)
    string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
    if log is not None:
        log.write((string + &#34;\n&#34;).encode())
        return None
    print(string)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.JSONIOWrapper.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Propagates messages up through C#, subprocess, and control layers.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self) -&gt; None:
    &#34;&#34;&#34;
    Propagates messages up through C#, subprocess, and control layers.
    &#34;&#34;&#34;
    response = self.readline()
    while response != &#34;&#34;:
        response = self.readline()
        print(response.rstrip())</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper.receive"><code class="name flex">
<span>def <span class="ident">receive</span></span>(<span>self, log: BytesIO) -> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Receive serialized data from command line interface.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def receive(self, log: BytesIO) -&gt; dict:
    &#34;&#34;&#34;
    Receive serialized data from command line interface.
    &#34;&#34;&#34;
    json = self.readline()
    self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
    try:
        data = loads(json.rstrip())
    except decoder.JSONDecodeError as decode_error:
        self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
        message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
        return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

    return data</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper.send"><code class="name flex">
<span>def <span class="ident">send</span></span>(<span>self, data: dict, log: BytesIO) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Write serialized data to interface.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send(self, data: dict, log: BytesIO) -&gt; None:
    &#34;&#34;&#34;
    Write serialized data to interface.
    &#34;&#34;&#34;

    def _transform():
        safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
        return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

    json = _transform()
    self.log(message=&#34;Send&#34;, data=json, log=log)
    self.write(f&#34;{json}\n&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator"><code class="flex name class">
<span>class <span class="ident">KernelDensityEstimator</span></span>
</code></dt>
<dd>
<div class="desc"><p>Predict events in space</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KernelDensityEstimator(KernelDensity):
    &#34;&#34;&#34;Predict events in space&#34;&#34;&#34;
    @staticmethod
    def glm():
        &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
        return LinearRegression()  

    @staticmethod
    def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
        &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
        epsilon = mesh.fields[key]
        field = mesh.nodes.xye(epsilon)
        target = mesh.interp2d(xx, yy, epsilon)  # location suitability

        return field, target

    def intensity(self, field: object):
        &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
        intensity = self.score_samples(field)  # create intensity field
        maximum = intensity.max()
        minimum = intensity.min()
        cost = (intensity - minimum) / (maximum - minimum)

        return intensity, cost

    @staticmethod
    def train(self, target: iter, field: object, xx: iter, yy: iter):
        &#34;&#34;&#34;
        Train kernel density estimator model using a quantized mesh

        :param mesh: Mesh object of the Interpolator super type
        :param key: Spatial field to train on
        :return:
        &#34;&#34;&#34;
        subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
        self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
        return self.intensity(field)

    @staticmethod
    def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
        &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

        xnew = []
        ynew = []

        def prohibit():
            &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
            xtemp = array(xin + xnew)
            ytemp = array(yin + ynew)
            dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
            nearest = dxy.min()
            return nearest &lt; 0.5 * bandwidth

        xmin, ymin = transform(view, native, extent[0], extent[1])
        xmax, ymax = transform(view, native, extent[2], extent[3])

        total = 0
        passes = 0
        while total &lt; count and passes &lt; count * 10:

            sample = kde.sample()
            xx = sample[0][0]
            yy = sample[0][1]

            if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

                if bandwidth is not None and prohibit():
                    xnew.append(xx)
                    ynew.append(yy)
                    total += 1

                else:
                    passes += 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.neighbors._kde.KernelDensity</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh"><code class="name flex">
<span>def <span class="ident">get_epsilon_from_mesh</span></span>(<span>mesh: object, key: str, xx, yy)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve probability field</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
    &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
    epsilon = mesh.fields[key]
    field = mesh.nodes.xye(epsilon)
    target = mesh.interp2d(xx, yy, epsilon)  # location suitability

    return field, target</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.glm"><code class="name flex">
<span>def <span class="ident">glm</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>create linear regression model object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def glm():
    &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
    return LinearRegression()  </code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>extent, count, view, native, kde, xin, yin, bandwidth=1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict new locations based on trained model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
    &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

    xnew = []
    ynew = []

    def prohibit():
        &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
        xtemp = array(xin + xnew)
        ytemp = array(yin + ynew)
        dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
        nearest = dxy.min()
        return nearest &lt; 0.5 * bandwidth

    xmin, ymin = transform(view, native, extent[0], extent[1])
    xmax, ymax = transform(view, native, extent[2], extent[3])

    total = 0
    passes = 0
    while total &lt; count and passes &lt; count * 10:

        sample = kde.sample()
        xx = sample[0][0]
        yy = sample[0][1]

        if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

            if bandwidth is not None and prohibit():
                xnew.append(xx)
                ynew.append(yy)
                total += 1

            else:
                passes += 1</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, target: iter, field: object, xx: iter, yy: iter)</span>
</code></dt>
<dd>
<div class="desc"><p>Train kernel density estimator model using a quantized mesh</p>
<p>:param mesh: Mesh object of the Interpolator super type
:param key: Spatial field to train on
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def train(self, target: iter, field: object, xx: iter, yy: iter):
    &#34;&#34;&#34;
    Train kernel density estimator model using a quantized mesh

    :param mesh: Mesh object of the Interpolator super type
    :param key: Spatial field to train on
    :return:
    &#34;&#34;&#34;
    subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
    self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
    return self.intensity(field)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.KernelDensityEstimator.intensity"><code class="name flex">
<span>def <span class="ident">intensity</span></span>(<span>self, field: object)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate density of observations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intensity(self, field: object):
    &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
    intensity = self.score_samples(field)  # create intensity field
    maximum = intensity.max()
    minimum = intensity.min()
    cost = (intensity - minimum) / (maximum - minimum)

    return intensity, cost</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.LinkedList"><code class="flex name class">
<span>class <span class="ident">LinkedList</span></span>
<span>(</span><span>data: (float,) = ())</span>
</code></dt>
<dd>
<div class="desc"><p>LL abstraction</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinkedList:
    &#34;&#34;&#34;LL abstraction&#34;&#34;&#34;
    def __init__(self, data: (float,) = ()):

        self.head = None
        prev = None
        for value in data:
            n = LinkedListNode(value)
            if prev is None:
                self.head = n
            else:
                prev.next = n
            prev = n

        self.tail = prev

    def traverse(self) -&gt; None:
        &#34;&#34;&#34;Move across nodes&#34;&#34;&#34;
        cursor = self.head
        while cursor is not None:
            print(cursor.value)
            cursor = cursor.next

    def deduplicate(self):
        &#34;&#34;&#34;Remove duplicates&#34;&#34;&#34;
        cursor, last, exists = self.head, None, set()
        while cursor is not None:
            if last is not None and cursor.value in exists:
                last.next = cursor.next.next if cursor.next is not None else None
            else:
                exists |= {cursor.value}
            last, cursor = cursor, cursor.next
        return last

    def k_from_head(self, k: int) -&gt; None or LinkedListNode:
        &#34;&#34;&#34;Get selected&#34;&#34;&#34;
        cursor = self.head
        while cursor.next is not None and k:
            cursor = cursor.next
            k -= 1
        return cursor.value

    def k_from_end(self, k: int) -&gt; None or LinkedListNode:
        cursor = self.head
        total = -k
        while cursor is not None:
            cursor = cursor.next
            total += 1

        assert total &gt; 0

        cursor = self.head
        while cursor is not None and total:
            cursor.next = cursor.next
            total -= 1
        return cursor.value

    def prepend(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        n.next, self.head = self.head, n

    def append(self, value: float) -&gt; None:
        n = LinkedListNode(value)
        if self.head is None:
            self.head = n
        if self.tail is not None:
            self.tail.next = n
        self.tail = n

    def add(self, other):
        ...</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="bathysphere.datatypes.DoublyLinkedList" href="#bathysphere.datatypes.DoublyLinkedList">DoublyLinkedList</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.LinkedList.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, other):
    ...</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, value: float) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, value: float) -&gt; None:
    n = LinkedListNode(value)
    if self.head is None:
        self.head = n
    if self.tail is not None:
        self.tail.next = n
    self.tail = n</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.deduplicate"><code class="name flex">
<span>def <span class="ident">deduplicate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove duplicates</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deduplicate(self):
    &#34;&#34;&#34;Remove duplicates&#34;&#34;&#34;
    cursor, last, exists = self.head, None, set()
    while cursor is not None:
        if last is not None and cursor.value in exists:
            last.next = cursor.next.next if cursor.next is not None else None
        else:
            exists |= {cursor.value}
        last, cursor = cursor, cursor.next
    return last</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.k_from_end"><code class="name flex">
<span>def <span class="ident">k_from_end</span></span>(<span>self, k: int) -> <a title="bathysphere.datatypes.LinkedListNode" href="#bathysphere.datatypes.LinkedListNode">LinkedListNode</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def k_from_end(self, k: int) -&gt; None or LinkedListNode:
    cursor = self.head
    total = -k
    while cursor is not None:
        cursor = cursor.next
        total += 1

    assert total &gt; 0

    cursor = self.head
    while cursor is not None and total:
        cursor.next = cursor.next
        total -= 1
    return cursor.value</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.k_from_head"><code class="name flex">
<span>def <span class="ident">k_from_head</span></span>(<span>self, k: int) -> <a title="bathysphere.datatypes.LinkedListNode" href="#bathysphere.datatypes.LinkedListNode">LinkedListNode</a></span>
</code></dt>
<dd>
<div class="desc"><p>Get selected</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def k_from_head(self, k: int) -&gt; None or LinkedListNode:
    &#34;&#34;&#34;Get selected&#34;&#34;&#34;
    cursor = self.head
    while cursor.next is not None and k:
        cursor = cursor.next
        k -= 1
    return cursor.value</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.prepend"><code class="name flex">
<span>def <span class="ident">prepend</span></span>(<span>self, value: float) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepend(self, value: float) -&gt; None:
    n = LinkedListNode(value)
    n.next, self.head = self.head, n</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.LinkedList.traverse"><code class="name flex">
<span>def <span class="ident">traverse</span></span>(<span>self) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Move across nodes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def traverse(self) -&gt; None:
    &#34;&#34;&#34;Move across nodes&#34;&#34;&#34;
    cursor = self.head
    while cursor is not None:
        print(cursor.value)
        cursor = cursor.next</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.LinkedListNode"><code class="flex name class">
<span>class <span class="ident">LinkedListNode</span></span>
<span>(</span><span>value)</span>
</code></dt>
<dd>
<div class="desc"><p>Node in linked list</p>
<p>create a node</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinkedListNode:
    &#34;&#34;&#34;Node in linked list&#34;&#34;&#34;
    def __init__(self, value):
        &#34;&#34;&#34;create a node&#34;&#34;&#34;
        self.next = None
        self.prev = None
        self.value = value</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory"><code class="flex name class">
<span>class <span class="ident">Memory</span></span>
<span>(</span><span>size, max_size=1000000)</span>
</code></dt>
<dd>
<div class="desc"><p>Memory manager class for allocating and freeing bytes string, only implements contiguous chunks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Memory:
    def __init__(self, size, max_size=int(1e6)):
        # type: (int, int) -&gt; None
        &#34;&#34;&#34;
        Memory manager class for allocating and freeing bytes string, only implements contiguous chunks.
        &#34;&#34;&#34;
        if not isinstance(size, int):
            raise TypeError
        if size &gt; max_size:
            raise MemoryError

        self.buffer = zeros(size, dtype=bytes)
        self.mask = zeros(size, dtype=bool)
        self.map = dict()
        self.remaining = size
        self._count = 0

    def alloc(self, size):
        # type: (int) -&gt; int
        &#34;&#34;&#34;
        Allocate and return a fixed length buffer. Raise error if out of memory.
        &#34;&#34;&#34;
        if self.remaining &lt; size:
            raise MemoryError

        # find indices of sufficient free memory, return pointers
        # optionally shuffle memory to create contiguous blocks
        self._count += 1
        self.remaining -= size

        start = self._find(size)
        if start is None:
            raise MemoryError

        ptr = self.buffer[start : start + size]
        self.map[self._count] = {&#34;mask&#34;: arange(start, start + size), &#34;data&#34;: ptr}
        return self._count

    def set(self, key, values):
        # type: (int or str, bytes) -&gt; None
        &#34;&#34;&#34;
        Set buffer to specified values, or singleton
        &#34;&#34;&#34;
        self.map[key][&#34;data&#34;][:] = values

    def data(self, key):
        # type: (int or str) -&gt; bytes
        &#34;&#34;&#34;Return data&#34;&#34;&#34;
        return self.map[key][&#34;data&#34;]

    def free(self, key):
        # type: (int or str) -&gt; bool
        &#34;&#34;&#34;
        Free previously allocated variable
        &#34;&#34;&#34;
        try:
            indices = self.map[key][&#34;mask&#34;]  # get indices from memory map dict
            # reset mask and increment available memory
            self.mask[indices] = False
            self.remaining += len(indices)
            del key

        except (MemoryError, TypeError):
            return False
        else:
            return True

    def _find(self, size):
        # type: (int) -&gt; int or None
        &#34;&#34;&#34;Find the starting index of the first available contiguous chunk&#34;&#34;&#34;
        start = 0
        while True:
            offset = 1
            if not self.mask[start]:
                while not self.mask[start + offset] and offset &lt;= size:
                    if offset == size:
                        return start
                    else:
                        offset += 1
            else:
                start += 1

            if start == len(self.mask) - size:
                return None


    @staticmethod
    def cache(data, path, free=False):
        # type: (bytes, str, bool) -&gt; int
        fid = open(path, &#34;wb+&#34;)  # open pickled file to read
        dump(data, fid)  # save array
        fid.close()
        if free:
            del data
        return len(data)


    @staticmethod
    def vertex_array_buffer(data, dataset, key, strategy, sequential=False, nb=None, headers=None):
        # type: (deque or (Array, ), str, str, str, bool, float, dict) -&gt; set
        &#34;&#34;&#34;
        Take an iterable of arrays, and chunk them for upload.

        :param data: deque or iterable
        :param dataset: prefix for object storage
        :param key: key for object storage
        :param strategy: how to chunk (aggregate or bisect)
        :param sequential: create an index if False
        :param nb: max number of bytes
        :param headers: headers!
        &#34;&#34;&#34;
        _data = data if isinstance(data, deque) else deque(data)
        if strategy not in (&#34;aggregate&#34;, &#34;bisect&#34;):
            raise ValueError
        if strategy == &#34;aggregate&#34; and nb is None:
            raise ValueError

        last = 0
        indx = 0
        real = len(_data)
        index = set()

        while _data:
            current = int(100 * indx / real)
            if current != last:
                print(current, &#34;%&#34;)

            c = ()
            if strategy == &#34;aggregate&#34;:
                size = 0
                while size &lt; nb and _data:
                    c += (_data.popleft(),)
                    size += c[-1].nbytes
            if strategy == &#34;bisect&#34;:
                c += (_data.popleft(),)

            _key = f&#34;{key}-{indx}&#34; if sequential else None
            ext = reduce(reduce_extent, (extent(*s) for s in c))

            try:
                assert False  # post here
            except SignatureDoesNotMatch:
                to_append = ()
                if strategy == &#34;bisect&#34;:
                    to_append = array_split(c[0], 2, axis=0)
                if strategy == &#34;aggregate&#34;:
                    tilt = len(c) // 2 + 1
                    to_append = c[:tilt], c[tilt:]
                _data.extend(to_append)
                real += 1
            else:
                index |= {_key}
                indx += 1

        return index


    @staticmethod
    def parts(dataset, key):
        part = 0
        result = []
        while True:
            k = f&#34;{dataset}/{key}-{part}&#34;
            stat = head(k)
            if stat is None:
                break
            result.append(k)
            part += 1
        return result


    @staticmethod
    def restore(dataset, key, fcn=None, sequential=True, stack=False, limit=None, **kwargs):
        # type: (str, str, Callable, bool, bool, int, dict) -&gt; (Array, ) or Array
        &#34;&#34;&#34;
        Reconstruct a single or multi-part array dataset

        :param dataset: object storage prefix
        :param key: object name, lat part
        :param fcn: method to perform on
        :param sequential: use a sequential naming scheme rather than an index file
        :param stack: append all array chunks into one
        :param limit: max number to process
        :param kwargs: arguments for the function

        :return: transformed array, or none, if the method return no results
        &#34;&#34;&#34;
        base = f&#34;{dataset}/{key}&#34;
        stat = head(base)
        if stat is None and not sequential:
            raise ValueError

        if stat is not None:
            if sequential:
                for s in unpickle(get(base).content):
                    fcn(s, **kwargs)
                return
        elif sequential:
            raise ValueError

        index = (
            Memory.parts(dataset, key) if sequential else
            tuple(f&#34;{dataset}/{key}&#34; for key in load_json(get(base)))
        )

        if len(index) == 0:
            raise ValueError

        vertex_array_buffer = ()
        part = 0
        for key in index:
            if part &gt; limit:
                break
            c = unpickle(get(key).content)
            if isinstance(c, list):
                c = tuple(c)
            if not isinstance(c, tuple):
                raise TypeError

            part += 1
            if fcn is None:
                vertex_array_buffer += c
                continue

            y = (fcn(x[0] if isinstance(x, tuple) else x, **kwargs) for x in c)
            vertex_array_buffer += tuple(yi for yi in y if yi is not None)

        if not len(vertex_array_buffer):
            return None
        if stack:
            return vstack(vertex_array_buffer)
        return vertex_array_buffer</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Memory.cache"><code class="name flex">
<span>def <span class="ident">cache</span></span>(<span>data, path, free=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def cache(data, path, free=False):
    # type: (bytes, str, bool) -&gt; int
    fid = open(path, &#34;wb+&#34;)  # open pickled file to read
    dump(data, fid)  # save array
    fid.close()
    if free:
        del data
    return len(data)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.parts"><code class="name flex">
<span>def <span class="ident">parts</span></span>(<span>dataset, key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parts(dataset, key):
    part = 0
    result = []
    while True:
        k = f&#34;{dataset}/{key}-{part}&#34;
        stat = head(k)
        if stat is None:
            break
        result.append(k)
        part += 1
    return result</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.restore"><code class="name flex">
<span>def <span class="ident">restore</span></span>(<span>dataset, key, fcn=None, sequential=True, stack=False, limit=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reconstruct a single or multi-part array dataset</p>
<p>:param dataset: object storage prefix
:param key: object name, lat part
:param fcn: method to perform on
:param sequential: use a sequential naming scheme rather than an index file
:param stack: append all array chunks into one
:param limit: max number to process
:param kwargs: arguments for the function</p>
<p>:return: transformed array, or none, if the method return no results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def restore(dataset, key, fcn=None, sequential=True, stack=False, limit=None, **kwargs):
    # type: (str, str, Callable, bool, bool, int, dict) -&gt; (Array, ) or Array
    &#34;&#34;&#34;
    Reconstruct a single or multi-part array dataset

    :param dataset: object storage prefix
    :param key: object name, lat part
    :param fcn: method to perform on
    :param sequential: use a sequential naming scheme rather than an index file
    :param stack: append all array chunks into one
    :param limit: max number to process
    :param kwargs: arguments for the function

    :return: transformed array, or none, if the method return no results
    &#34;&#34;&#34;
    base = f&#34;{dataset}/{key}&#34;
    stat = head(base)
    if stat is None and not sequential:
        raise ValueError

    if stat is not None:
        if sequential:
            for s in unpickle(get(base).content):
                fcn(s, **kwargs)
            return
    elif sequential:
        raise ValueError

    index = (
        Memory.parts(dataset, key) if sequential else
        tuple(f&#34;{dataset}/{key}&#34; for key in load_json(get(base)))
    )

    if len(index) == 0:
        raise ValueError

    vertex_array_buffer = ()
    part = 0
    for key in index:
        if part &gt; limit:
            break
        c = unpickle(get(key).content)
        if isinstance(c, list):
            c = tuple(c)
        if not isinstance(c, tuple):
            raise TypeError

        part += 1
        if fcn is None:
            vertex_array_buffer += c
            continue

        y = (fcn(x[0] if isinstance(x, tuple) else x, **kwargs) for x in c)
        vertex_array_buffer += tuple(yi for yi in y if yi is not None)

    if not len(vertex_array_buffer):
        return None
    if stack:
        return vstack(vertex_array_buffer)
    return vertex_array_buffer</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.vertex_array_buffer"><code class="name flex">
<span>def <span class="ident">vertex_array_buffer</span></span>(<span>data, dataset, key, strategy, sequential=False, nb=None, headers=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Take an iterable of arrays, and chunk them for upload.</p>
<p>:param data: deque or iterable
:param dataset: prefix for object storage
:param key: key for object storage
:param strategy: how to chunk (aggregate or bisect)
:param sequential: create an index if False
:param nb: max number of bytes
:param headers: headers!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def vertex_array_buffer(data, dataset, key, strategy, sequential=False, nb=None, headers=None):
    # type: (deque or (Array, ), str, str, str, bool, float, dict) -&gt; set
    &#34;&#34;&#34;
    Take an iterable of arrays, and chunk them for upload.

    :param data: deque or iterable
    :param dataset: prefix for object storage
    :param key: key for object storage
    :param strategy: how to chunk (aggregate or bisect)
    :param sequential: create an index if False
    :param nb: max number of bytes
    :param headers: headers!
    &#34;&#34;&#34;
    _data = data if isinstance(data, deque) else deque(data)
    if strategy not in (&#34;aggregate&#34;, &#34;bisect&#34;):
        raise ValueError
    if strategy == &#34;aggregate&#34; and nb is None:
        raise ValueError

    last = 0
    indx = 0
    real = len(_data)
    index = set()

    while _data:
        current = int(100 * indx / real)
        if current != last:
            print(current, &#34;%&#34;)

        c = ()
        if strategy == &#34;aggregate&#34;:
            size = 0
            while size &lt; nb and _data:
                c += (_data.popleft(),)
                size += c[-1].nbytes
        if strategy == &#34;bisect&#34;:
            c += (_data.popleft(),)

        _key = f&#34;{key}-{indx}&#34; if sequential else None
        ext = reduce(reduce_extent, (extent(*s) for s in c))

        try:
            assert False  # post here
        except SignatureDoesNotMatch:
            to_append = ()
            if strategy == &#34;bisect&#34;:
                to_append = array_split(c[0], 2, axis=0)
            if strategy == &#34;aggregate&#34;:
                tilt = len(c) // 2 + 1
                to_append = c[:tilt], c[tilt:]
            _data.extend(to_append)
            real += 1
        else:
            index |= {_key}
            indx += 1

    return index</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Memory.alloc"><code class="name flex">
<span>def <span class="ident">alloc</span></span>(<span>self, size)</span>
</code></dt>
<dd>
<div class="desc"><p>Allocate and return a fixed length buffer. Raise error if out of memory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc(self, size):
    # type: (int) -&gt; int
    &#34;&#34;&#34;
    Allocate and return a fixed length buffer. Raise error if out of memory.
    &#34;&#34;&#34;
    if self.remaining &lt; size:
        raise MemoryError

    # find indices of sufficient free memory, return pointers
    # optionally shuffle memory to create contiguous blocks
    self._count += 1
    self.remaining -= size

    start = self._find(size)
    if start is None:
        raise MemoryError

    ptr = self.buffer[start : start + size]
    self.map[self._count] = {&#34;mask&#34;: arange(start, start + size), &#34;data&#34;: ptr}
    return self._count</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.data"><code class="name flex">
<span>def <span class="ident">data</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<div class="desc"><p>Return data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data(self, key):
    # type: (int or str) -&gt; bytes
    &#34;&#34;&#34;Return data&#34;&#34;&#34;
    return self.map[key][&#34;data&#34;]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.free"><code class="name flex">
<span>def <span class="ident">free</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<div class="desc"><p>Free previously allocated variable</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def free(self, key):
    # type: (int or str) -&gt; bool
    &#34;&#34;&#34;
    Free previously allocated variable
    &#34;&#34;&#34;
    try:
        indices = self.map[key][&#34;mask&#34;]  # get indices from memory map dict
        # reset mask and increment available memory
        self.mask[indices] = False
        self.remaining += len(indices)
        del key

    except (MemoryError, TypeError):
        return False
    else:
        return True</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Memory.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, key, values)</span>
</code></dt>
<dd>
<div class="desc"><p>Set buffer to specified values, or singleton</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(self, key, values):
    # type: (int or str, bytes) -&gt; None
    &#34;&#34;&#34;
    Set buffer to specified values, or singleton
    &#34;&#34;&#34;
    self.map[key][&#34;data&#34;][:] = values</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Mesh"><code class="flex name class">
<span>class <span class="ident">Mesh</span></span>
<span>(</span><span>vertex_array: VertexArray = None, topology: Topology = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Meshes are collections of points and their topology</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mesh:
    &#34;&#34;&#34;
    Meshes are collections of points and their topology
    &#34;&#34;&#34;
    vertex_array: VertexArray = attr.ib(default=None)
    topology: Topology = attr.ib(default=None)  # Topology


    def cell_normals(self) -&gt; Array:
        &#34;&#34;&#34;Normals of faces&#34;&#34;&#34;
        topo = self.topology
        uu = self.vertex_array[topo[:, 1], :] - self.vertex_array[topo[:, 0], :]
        vv = self.vertex_array[topo[:, 2], :] - self.vertex_array[topo[:, 0], :]
        return cross(uu, vv)


    def vertex_normals(self, s: float = 0.05) -&gt; Array:
        &#34;&#34;&#34;
        Add vertex list to batch for rendering
        &#34;&#34;&#34;
        f = self.cell_normals()
        assert f.shape == (self.topology.size, 3)
        v = f[self.topology, :]
        assert v.shape[:2] == (self.topology.size, 3)
        assert 3 &lt;= v.shape[2] &lt;= 4
        v_avg = v.mean(axis=1)
        assert v_avg.shape == (self.vertex_array.size, 3)
        return vstack((self.vertex_array, s * normal(v_avg) + self.vertex_array))


    def adjacency(self) -&gt; Array:
        # type: (Array, Array) -&gt; Array
        &#34;&#34;&#34;
        Calculate adjacent vertices
        &#34;&#34;&#34;
        adj = []
        for ii, _ in enumerate(self.vertex_array):
            rows, _ = where(self.topology == ii)
            uni = unique(self.topology[rows, :])
            new_adj = uni[where(uni != ii)]
            adj.append(new_adj)
        return adj</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Mesh.adjacency"><code class="name flex">
<span>def <span class="ident">adjacency</span></span>(<span>self) -> <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></span>
</code></dt>
<dd>
<div class="desc"><p>Calculate adjacent vertices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adjacency(self) -&gt; Array:
    # type: (Array, Array) -&gt; Array
    &#34;&#34;&#34;
    Calculate adjacent vertices
    &#34;&#34;&#34;
    adj = []
    for ii, _ in enumerate(self.vertex_array):
        rows, _ = where(self.topology == ii)
        uni = unique(self.topology[rows, :])
        new_adj = uni[where(uni != ii)]
        adj.append(new_adj)
    return adj</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Mesh.cell_normals"><code class="name flex">
<span>def <span class="ident">cell_normals</span></span>(<span>self) -> <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></span>
</code></dt>
<dd>
<div class="desc"><p>Normals of faces</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cell_normals(self) -&gt; Array:
    &#34;&#34;&#34;Normals of faces&#34;&#34;&#34;
    topo = self.topology
    uu = self.vertex_array[topo[:, 1], :] - self.vertex_array[topo[:, 0], :]
    vv = self.vertex_array[topo[:, 2], :] - self.vertex_array[topo[:, 0], :]
    return cross(uu, vv)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Mesh.vertex_normals"><code class="name flex">
<span>def <span class="ident">vertex_normals</span></span>(<span>self, s: float = 0.05) -> <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></span>
</code></dt>
<dd>
<div class="desc"><p>Add vertex list to batch for rendering</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vertex_normals(self, s: float = 0.05) -&gt; Array:
    &#34;&#34;&#34;
    Add vertex list to batch for rendering
    &#34;&#34;&#34;
    f = self.cell_normals()
    assert f.shape == (self.topology.size, 3)
    v = f[self.topology, :]
    assert v.shape[:2] == (self.topology.size, 3)
    assert 3 &lt;= v.shape[2] &lt;= 4
    v_avg = v.mean(axis=1)
    assert v_avg.shape == (self.vertex_array.size, 3)
    return vstack((self.vertex_array, s * normal(v_avg) + self.vertex_array))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage"><code class="flex name class">
<span>class <span class="ident">ObjectStorage</span></span>
<span>(</span><span>bucket_name: str, endpoint: str, prefix: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Constructs a :class:<code>Minio &lt;Minio&gt;</code>.</p>
<h2 id="examples">Examples</h2>
<p>client = Minio('play.min.io')
client = Minio('s3.amazonaws.com', 'ACCESS_KEY', 'SECRET_KEY')</p>
<h1 id="to-override-auto-bucket-location-discovery">To override auto bucket location discovery.</h1>
<p>client = Minio('play.min.io', 'ACCESS_KEY', 'SECRET_KEY',
region='us-east-1')</p>
<p>:param endpoint: Hostname of the cloud storage server.
:param access_key: Access key to sign self._http.request with.
:param secret_key: Secret key to sign self._http.request with.
:param session_token: Session token to sign self._http.request with.
:param secure: Set this value if wish to make secure requests.
Default is True.
:param region: Set this value to override automatic bucket
location discovery.
:param timeout: Set this value to control how long requests
are allowed to run before being aborted.
:return: :class:<code>Minio &lt;Minio&gt;</code> object</p>
<p><strong>NOTE on concurrent usage:</strong> The <code>Minio</code> object is thread safe when using
the Python <code>threading</code> library. Specifically, it is <strong>NOT</strong> safe to share
it between multiple processes, for example when using
<code>multiprocessing.Pool</code>. The solution is simply to create a new <code>Minio</code>
object in each process, and not share it between processes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ObjectStorage(Minio):
    def __init__(self, bucket_name: str, endpoint: str, prefix: str = None, **kwargs):
        self.bucket_name = bucket_name
        self.prefix = prefix
        self.endpoint = endpoint
        
        super().__init__(endpoint, **kwargs)
        if not self.bucket_exists(bucket_name):
            self.make_bucket(bucket_name)

    @property
    def locked(self) -&gt; bool:
        return self.stat_object(&#34;lock.json&#34;) is not None

    def publish_events(self, pubsub_channel: str):
        fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
        with StrictRedis() as queue:
            for event in self.listen_bucket_notification(
                self.bucket_name, &#34;&#34;, None, fcns
            ):
                queue.publish(pubsub_channel, str(event))

    def stat_object(self, object_name: str):
        &#34;&#34;&#34;
        Determine whether an object key exists
        &#34;&#34;&#34;
        try:
            return super().stat_object(self.bucket_name, object_name)
        except NoSuchKey:
            return None

    def list_objects(self, prefix: str = None):
        return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))

    def put_object(
        self,
        object_name: str,
        data: dict or bytes,
        metadata: dict = None,
        codec: str = &#34;utf-8&#34;,
    ) -&gt; str:
        &#34;&#34;&#34;
        Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

        :param label: label for file
        :param data: data to serialize
        :param metadata: headers
        :param codec: how to encode strings
        &#34;&#34;&#34;
        if isinstance(data, dict):
            content_type = &#34;application/json&#34;
            buffer = bytes(dumps(data).encode(codec))
        elif isinstance(data, bytes):
            content_type = &#34;text/plain&#34;
            buffer = data
        else:
            raise TypeError

        accumulate = []
        given_parts = object_name.split(&#34;/&#34;)
        prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
        if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
            for pp in prefix_parts:
                if pp not in given_parts:
                    accumulate.append(pp)
            accumulate.extend(given_parts)
            object_name = &#34;/&#34;.join(accumulate)

        super().put_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            data=BytesIO(buffer),
            length=len(buffer),
            metadata=metadata,
            content_type=content_type,
        )

        return object_name

    def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
        &#34;&#34;&#34;
        Download the data, may be streaming if desired
        &#34;&#34;&#34;
        data = super().get_object(self.bucket_name, object_name)
        if stream:

            def generate():
                for d in data.stream(32 * 1024):
                    yield d

            result = generate
        else:
            result = data

        return Response(result, mimetype=&#34;application/octet-stream&#34;)

    def updateIndex(
        self,
        object_name: str,
        metadata: dict = None,
        entries: [dict] = None,
        props: dict = None,
    ):
        &#34;&#34;&#34;
        Update contents of index metadata
        &#34;&#34;&#34;

        if entries:
            self.put_object(
                object_name=object_name,
                data={
                    **loads(self.get_object(object_name=object_name).data),
                    **(entries or {}),
                    **(props or {}),
                },
                metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
            )
        else:
            self.copy_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                object_source=object_name,
                metadata=metadata,
            )

        return self

    def delete(
        self, 
        prefix: str, 
        batch: int = 10, 
        conditions: dict = None
    ) -&gt; (Any):
        &#34;&#34;&#34;
        Delete all objects within a subdirectory or abstract collection.

        The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
        default, and therefore needs to be iterated through before returning any errors. 

        :param prefic: file prefix/dataset
        :param batch:  number to delete at a time
        &#34;&#34;&#34;
        remove = ()
        errors = ()
        
        objects_iter = self.list_objects(prefix=prefix)
        stop = False
        while not stop:
            try:
                object_name = next(objects_iter).object_name
            except StopIteration:
                stop = True
            else:
                stat = self.stat_object(object_name)
                if isinstance(conditions, dict):
                    if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                        remove += (object_name,)
                else:
                    remove += (object_name,)

            if len(remove) &gt;= batch or stop:
                for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                    errors += (error,)
                return errors


    @staticmethod
    def metadata_template(
        file_type: str = None, parent: str = None, headers: dict = None
    ) -&gt; dict:

        accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

        return {
            &#34;x-amz-acl&#34;: accessControl,
            &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
            &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
            &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
            &#34;x-amz-meta-service-file-type&#34;: file_type,
            **(headers or {}),
        }

    def unlock(self, object_name: str, session: str = None,) -&gt; bool:
        &#34;&#34;&#34;
        Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
        &#34;&#34;&#34;
        try:
            self.remove_object(self.bucket_name, object_name)
        except NoSuchKey:
            return False
        return True

    def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
        &#34;&#34;&#34;
        Object storage locking decorator for functions.

        When used this implements a mutex lock on the object path,
        which will block competing operations until it is cleared.

        Locks will not block read operations except in special cases. 
        &#34;&#34;&#34;

        # index = load_json(self.get_object(object_name=index_file))

        headers = {}
        session_id = uuid4().hex
        name = &#34;bathysphere&#34;
        lock_file = f&#34;{name}/lock.json&#34;
        # index_file = f&#34;{name}/index.json&#34;

        def decorator(fcn):
            &#34;&#34;&#34;
            Methods applied to the wrapped function
            &#34;&#34;&#34;

            def wrapper(*args, **kwargs):
                &#34;&#34;&#34;
                Actual wrapper that calls the decorated function
                &#34;&#34;&#34;
                if self.stat_object(lock_file):
                    return &#34;Lock in place&#34;, 500
                try:
                    self.put_object(
                        object_name=lock_file,
                        data={&#34;session&#34;: session_id},
                        metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                    )
                except NoSuchKey:
                    return &#34;Could not lock repository&#34;, 500
                try:
                    result = fcn(*args, **kwargs)
                except Exception as ex:
                    result = f&#34;{ex}&#34;, 500
                finally:
                    if lock and not self.unlock(object_name=lock):
                        result = &#34;Failed to unlock&#34;, 500
                return result

            return wrapper

        return decorator</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>minio.api.Minio</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.metadata_template"><code class="name flex">
<span>def <span class="ident">metadata_template</span></span>(<span>file_type: str = None, parent: str = None, headers: dict = None) -> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def metadata_template(
    file_type: str = None, parent: str = None, headers: dict = None
) -&gt; dict:

    accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

    return {
        &#34;x-amz-acl&#34;: accessControl,
        &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
        &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
        &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
        &#34;x-amz-meta-service-file-type&#34;: file_type,
        **(headers or {}),
    }</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.locked"><code class="name">var <span class="ident">locked</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def locked(self) -&gt; bool:
    return self.stat_object(&#34;lock.json&#34;) is not None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.delete"><code class="name flex">
<span>def <span class="ident">delete</span></span>(<span>self, prefix: str, batch: int = 10, conditions: dict = None) -> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Delete all objects within a subdirectory or abstract collection.</p>
<p>The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
default, and therefore needs to be iterated through before returning any errors. </p>
<p>:param prefic: file prefix/dataset
:param batch:
number to delete at a time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete(
    self, 
    prefix: str, 
    batch: int = 10, 
    conditions: dict = None
) -&gt; (Any):
    &#34;&#34;&#34;
    Delete all objects within a subdirectory or abstract collection.

    The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
    default, and therefore needs to be iterated through before returning any errors. 

    :param prefic: file prefix/dataset
    :param batch:  number to delete at a time
    &#34;&#34;&#34;
    remove = ()
    errors = ()
    
    objects_iter = self.list_objects(prefix=prefix)
    stop = False
    while not stop:
        try:
            object_name = next(objects_iter).object_name
        except StopIteration:
            stop = True
        else:
            stat = self.stat_object(object_name)
            if isinstance(conditions, dict):
                if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                    remove += (object_name,)
            else:
                remove += (object_name,)

        if len(remove) &gt;= batch or stop:
            for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                errors += (error,)
            return errors</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.get_object"><code class="name flex">
<span>def <span class="ident">get_object</span></span>(<span>self, object_name: str, stream: bool = False) -> flask.wrappers.Response</span>
</code></dt>
<dd>
<div class="desc"><p>Download the data, may be streaming if desired</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
    &#34;&#34;&#34;
    Download the data, may be streaming if desired
    &#34;&#34;&#34;
    data = super().get_object(self.bucket_name, object_name)
    if stream:

        def generate():
            for d in data.stream(32 * 1024):
                yield d

        result = generate
    else:
        result = data

    return Response(result, mimetype=&#34;application/octet-stream&#34;)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.list_objects"><code class="name flex">
<span>def <span class="ident">list_objects</span></span>(<span>self, prefix: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>List objects in the given bucket.</p>
<h2 id="examples">Examples</h2>
<p>objects = minio.list_objects('foo')
for current_object in objects:
print(current_object)</p>
<h1 id="hello">hello</h1>
<h1 id="hello_1">hello/</h1>
<h1 id="hello_2">hello/</h1>
<h1 id="world">world/</h1>
<p>objects = minio.list_objects('foo', prefix='hello/')
for current_object in objects:
print(current_object)</p>
<h1 id="helloworld">hello/world/</h1>
<p>objects = minio.list_objects('foo', recursive=True)
for current_object in objects:
print(current_object)</p>
<h1 id="helloworld1">hello/world/1</h1>
<h1 id="worldworld2">world/world/2</h1>
<h1 id="_1">&hellip;</h1>
<p>objects = minio.list_objects('foo', prefix='hello/',
recursive=True)
for current_object in objects:
print(current_object)</p>
<h1 id="helloworld1_1">hello/world/1</h1>
<h1 id="helloworld2">hello/world/2</h1>
<p>:param bucket_name: Bucket to list objects from
:param prefix: String specifying objects returned must begin with
:param recursive: If yes, returns all objects for a specified prefix
:return: An iterator of objects in alphabetical order.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_objects(self, prefix: str = None):
    return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.publish_events"><code class="name flex">
<span>def <span class="ident">publish_events</span></span>(<span>self, pubsub_channel: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def publish_events(self, pubsub_channel: str):
    fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
    with StrictRedis() as queue:
        for event in self.listen_bucket_notification(
            self.bucket_name, &#34;&#34;, None, fcns
        ):
            queue.publish(pubsub_channel, str(event))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.put_object"><code class="name flex">
<span>def <span class="ident">put_object</span></span>(<span>self, object_name: str, data: dict or bytes, metadata: dict = None, codec: str = 'utf-8') -> str</span>
</code></dt>
<dd>
<div class="desc"><p>Create an s3 connection if necessary, then create bucket if it doesn't exist.</p>
<p>:param label: label for file
:param data: data to serialize
:param metadata: headers
:param codec: how to encode strings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def put_object(
    self,
    object_name: str,
    data: dict or bytes,
    metadata: dict = None,
    codec: str = &#34;utf-8&#34;,
) -&gt; str:
    &#34;&#34;&#34;
    Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

    :param label: label for file
    :param data: data to serialize
    :param metadata: headers
    :param codec: how to encode strings
    &#34;&#34;&#34;
    if isinstance(data, dict):
        content_type = &#34;application/json&#34;
        buffer = bytes(dumps(data).encode(codec))
    elif isinstance(data, bytes):
        content_type = &#34;text/plain&#34;
        buffer = data
    else:
        raise TypeError

    accumulate = []
    given_parts = object_name.split(&#34;/&#34;)
    prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
    if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
        for pp in prefix_parts:
            if pp not in given_parts:
                accumulate.append(pp)
        accumulate.extend(given_parts)
        object_name = &#34;/&#34;.join(accumulate)

    super().put_object(
        bucket_name=self.bucket_name,
        object_name=object_name,
        data=BytesIO(buffer),
        length=len(buffer),
        metadata=metadata,
        content_type=content_type,
    )

    return object_name</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.session"><code class="name flex">
<span>def <span class="ident">session</span></span>(<span>self, lock: bool = False) -> 'ResponseJSON or ResponseOctet'</span>
</code></dt>
<dd>
<div class="desc"><p>Object storage locking decorator for functions.</p>
<p>When used this implements a mutex lock on the object path,
which will block competing operations until it is cleared.</p>
<p>Locks will not block read operations except in special cases.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
    &#34;&#34;&#34;
    Object storage locking decorator for functions.

    When used this implements a mutex lock on the object path,
    which will block competing operations until it is cleared.

    Locks will not block read operations except in special cases. 
    &#34;&#34;&#34;

    # index = load_json(self.get_object(object_name=index_file))

    headers = {}
    session_id = uuid4().hex
    name = &#34;bathysphere&#34;
    lock_file = f&#34;{name}/lock.json&#34;
    # index_file = f&#34;{name}/index.json&#34;

    def decorator(fcn):
        &#34;&#34;&#34;
        Methods applied to the wrapped function
        &#34;&#34;&#34;

        def wrapper(*args, **kwargs):
            &#34;&#34;&#34;
            Actual wrapper that calls the decorated function
            &#34;&#34;&#34;
            if self.stat_object(lock_file):
                return &#34;Lock in place&#34;, 500
            try:
                self.put_object(
                    object_name=lock_file,
                    data={&#34;session&#34;: session_id},
                    metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                )
            except NoSuchKey:
                return &#34;Could not lock repository&#34;, 500
            try:
                result = fcn(*args, **kwargs)
            except Exception as ex:
                result = f&#34;{ex}&#34;, 500
            finally:
                if lock and not self.unlock(object_name=lock):
                    result = &#34;Failed to unlock&#34;, 500
            return result

        return wrapper

    return decorator</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.stat_object"><code class="name flex">
<span>def <span class="ident">stat_object</span></span>(<span>self, object_name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine whether an object key exists</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stat_object(self, object_name: str):
    &#34;&#34;&#34;
    Determine whether an object key exists
    &#34;&#34;&#34;
    try:
        return super().stat_object(self.bucket_name, object_name)
    except NoSuchKey:
        return None</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.unlock"><code class="name flex">
<span>def <span class="ident">unlock</span></span>(<span>self, object_name: str, session: str = None) -> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unlock(self, object_name: str, session: str = None,) -&gt; bool:
    &#34;&#34;&#34;
    Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
    &#34;&#34;&#34;
    try:
        self.remove_object(self.bucket_name, object_name)
    except NoSuchKey:
        return False
    return True</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.updateIndex"><code class="name flex">
<span>def <span class="ident">updateIndex</span></span>(<span>self, object_name: str, metadata: dict = None, entries: [dict] = None, props: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Update contents of index metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def updateIndex(
    self,
    object_name: str,
    metadata: dict = None,
    entries: [dict] = None,
    props: dict = None,
):
    &#34;&#34;&#34;
    Update contents of index metadata
    &#34;&#34;&#34;

    if entries:
        self.put_object(
            object_name=object_name,
            data={
                **loads(self.get_object(object_name=object_name).data),
                **(entries or {}),
                **(props or {}),
            },
            metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
        )
    else:
        self.copy_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            object_source=object_name,
            metadata=metadata,
        )

    return self</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.PostgresType"><code class="flex name class">
<span>class <span class="ident">PostgresType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PostgresType(Enum):
    Numerical = &#34;DOUBLE PRECISION NULL&#34;
    TimeStamp = &#34;TIMESTAMP NOT NULL&#34;
    Geography = &#34;GEOGRAPHY NOT NULL&#34;
    IntIdentity = &#34;INT PRIMARY KEY&#34;
    NullString = &#34;VARCHAR(100) NULL&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.PostgresType.Geography"><code class="name">var <span class="ident">Geography</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.IntIdentity"><code class="name">var <span class="ident">IntIdentity</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.NullString"><code class="name">var <span class="ident">NullString</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.Numerical"><code class="name">var <span class="ident">Numerical</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.TimeStamp"><code class="name">var <span class="ident">TimeStamp</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Query"><code class="flex name class">
<span>class <span class="ident">Query</span></span>
<span>(</span><span>sql: str, parser: Callable)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Query:

    sql: str = attr.ib()
    parser: Callable = attr.ib()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Reactor"><code class="flex name class">
<span>class <span class="ident">Reactor</span></span>
<span>(</span><span>systems: (), mesh: None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Reactor:

    systems: () = attr.ib()
    mesh: None = attr.ib()

    def update(self, volume: array):
        &#34;&#34;&#34;
        Transfer mass from difference equation to conservative arrays
        &#34;&#34;&#34;
        assert all(each.transfer(conversion=volume) for each in self.systems.values())

    def integrate(
        self, anomaly, nutrients, carbon, oxygen, phyto_c=0.0, phyto_n=0.0, volume=1.0
    ) -&gt; None:
        &#34;&#34;&#34;
            
        Update difference equations for internal, temperature-dependent chemistry.

        :param anomaly: temperature anomaly (usually T-20)
        :param carbon: required chemistry instance
        :param oxygen: required chemistry instance
        :param nutrients: optional list of nutrients to track
        :param phyto_c: carbon supplied by biology
        :param phyto_n: nitrogen supplied by biology
        &#34;&#34;&#34;
        limit = carbon.integrate(
            anomaly, oxygen, phyto_c
        )  # available carbon as proxy, consumes oxygen
        assert oxygen.integrate(limit, anomaly)  # oxygen consumption

        assert all(nutrient.mineralize(limit, anomaly) for nutrient in nutrients)

        for each in nutrients:
            if each.__class__.__name__ == &#34;Nitrogen&#34;:
                assert each.integrate(
                    oxygen, carbon, phyto_n, anomaly
                )  # consumes oxygen and carbon
                break

        self.update(volume)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Reactor.integrate"><code class="name flex">
<span>def <span class="ident">integrate</span></span>(<span>self, anomaly, nutrients, carbon, oxygen, phyto_c=0.0, phyto_n=0.0, volume=1.0) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Update difference equations for internal, temperature-dependent chemistry.</p>
<p>:param anomaly: temperature anomaly (usually T-20)
:param carbon: required chemistry instance
:param oxygen: required chemistry instance
:param nutrients: optional list of nutrients to track
:param phyto_c: carbon supplied by biology
:param phyto_n: nitrogen supplied by biology</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def integrate(
    self, anomaly, nutrients, carbon, oxygen, phyto_c=0.0, phyto_n=0.0, volume=1.0
) -&gt; None:
    &#34;&#34;&#34;
        
    Update difference equations for internal, temperature-dependent chemistry.

    :param anomaly: temperature anomaly (usually T-20)
    :param carbon: required chemistry instance
    :param oxygen: required chemistry instance
    :param nutrients: optional list of nutrients to track
    :param phyto_c: carbon supplied by biology
    :param phyto_n: nitrogen supplied by biology
    &#34;&#34;&#34;
    limit = carbon.integrate(
        anomaly, oxygen, phyto_c
    )  # available carbon as proxy, consumes oxygen
    assert oxygen.integrate(limit, anomaly)  # oxygen consumption

    assert all(nutrient.mineralize(limit, anomaly) for nutrient in nutrients)

    for each in nutrients:
        if each.__class__.__name__ == &#34;Nitrogen&#34;:
            assert each.integrate(
                oxygen, carbon, phyto_n, anomaly
            )  # consumes oxygen and carbon
            break

    self.update(volume)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Reactor.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, volume: array)</span>
</code></dt>
<dd>
<div class="desc"><p>Transfer mass from difference equation to conservative arrays</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, volume: array):
    &#34;&#34;&#34;
    Transfer mass from difference equation to conservative arrays
    &#34;&#34;&#34;
    assert all(each.transfer(conversion=volume) for each in self.systems.values())</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork"><code class="flex name class">
<span>class <span class="ident">RecurrentNeuralNetwork</span></span>
<span>(</span><span>hidden, periods, epochs, rate, model=None, output=1, horizon=1, input=1, file=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecurrentNeuralNetwork:

    hidden = attr.ib()  # hidden layers
    periods = attr.ib()  # previous steps to use for prediction
    epochs = attr.ib()  # number of training cycles
    rate = attr.ib()  # learning rate
    model = attr.ib(default=None)
    output = attr.ib(default=1)  # out put dimensions
    horizon = attr.ib(default=1)  # future steps to predict
    input = attr.ib(default=1)
    file = attr.ib(default=None)  # path for persisting data

    # saver = tf.train.Saver

    @property
    def shape(self):
        return (-1, self.periods, 1)

    @staticmethod
    def from_cache(fcn):
        def wrapper(*args, **kwargs):

            cache = kwargs.pop(&#34;cache&#34;)
            key = kwargs.pop(&#34;objectKey&#34;)

            # check connection
            # check key

            try:
                request.model = tf.keras.models.load_model(
                    &#34;/&#34;.join([&#34;&#34;, cache, key])
                )  # load and inject object
            except:
                return 500, {&#34;message&#34;: &#34;Server error while loading model&#34;}

            return fcn(*args, **kwargs)

        return wrapper

    @classmethod
    def createAndCache(
        cls,
        stateful: bool,
        horizon: int,
        layers: int,
        batch_size: int,
        cache: str,
        key: str,
    ):
        &#34;&#34;&#34;
        Create and cache the model structure
        &#34;&#34;&#34;
        neural_net = cls.create(stateful, horizon, layers, batch_size)
        neural_net.save(&#34;/&#34;.join([&#34;&#34;, cache, key]))
        return 200, {&#34;message&#34;: &#34;model created and cached&#34;}

    @classmethod
    def train_cached_model(
        cls,
        datastream,
        window: int,
        horizon: int,
        batch_size: int,
        ratio: float,
        periods: int,
        epochs: int,
    ):
        &#34;&#34;&#34;
        Load and feed the model training data
        &#34;&#34;&#34;
        datasets = datastream.partition(window, horizon, batch_size, ratio, periods)

        cls.train(
            request.model,
            batch_size=batch_size,
            training=datasets.get(&#34;training&#34;),
            epochs=epochs,
            validation=datasets.get(&#34;validation&#34;),
        )

        return 200, {&#34;message&#34;: &#34;model trained and cached&#34;}

    @staticmethod
    def get_prediction(datastream: object, batch_size: int = 32):
        &#34;&#34;&#34;

        :param datastream:
        :param batch_size: Number of samples per gradient update
        :return:
        &#34;&#34;&#34;
        predicted = request.model.predict(
            datastream, batch_size=batch_size, workers=1, use_multiprocessing=False
        )
        return 200, {&#34;payload&#34;: predicted.flatten()}

    @staticmethod
    def train(model, batch_size: int, training: dict, validation: dict, epochs: int):

        for i in range(epochs):
            print(&#34;Epoch&#34;, i + 1, &#34;/&#34;, epochs)
            model.fit(
                training[&#34;x&#34;],
                training[&#34;y&#34;],
                batch_size=batch_size,
                epochs=1,  # different &#34;epochs&#34;
                verbose=False,
                validation_data=(validation[&#34;x&#34;], validation[&#34;y&#34;]),
                shuffle=False,  # order is important
                workers=1,
                use_multiprocessing=False,
            )

            model.reset_states()

    @staticmethod
    def create(
        stateful: bool, horizon: int, units: int, batch_size: int, variables: int = 1
    ):
        &#34;&#34;&#34;
        Create the Keras-Tensorflow model

        :param horizon: sequence length for training each output point
        :param stateful: boolean, better if true
        :param units: number of LSTM
        :param batch_size:
        :param variables: number of input variables

        :return: model instance
        &#34;&#34;&#34;

        model = tf.keras.models.Sequential()
        model.add(
            tf.keras.layers.LSTM(
                units=units,
                input_shape=(horizon, variables),
                batch_size=batch_size,
                stateful=stateful,
            )
        )
        model.add(tf.keras.layers.Dense(1))
        model.compile(loss=&#34;mse&#34;, optimizer=&#34;adam&#34;)

        return model

    @staticmethod
    def series(n=365, show=False):
        &#34;&#34;&#34;
        Create a synthetic training data set

        :param n: steps
        :param show: plot for time series

        :return: numpy array of magnitudes
        &#34;&#34;&#34;
        rng = date_range(start=&#34;2018&#34;, periods=n, freq=&#34;D&#34;)
        data = Series(random.normal(0, 0.5, size=len(rng)), rng).cumsum()
        return array(data)

    def test(self, data):
        &#34;&#34;&#34;
        Split data for prediction period.

        :param data:
        :return:
        &#34;&#34;&#34;
        start = self.periods + self.horizon
        setup = data[-start:]
        x = setup[: self.periods].reshape(self.shape)
        y = data[-self.periods :].reshape(self.shape)
        return x, y

    def x_train(self, opt, feed, loss, verb):
        &#34;&#34;&#34;
        Initialize and train the neural net.

        :param opt: optimizer
        :param feed:
        :param loss: skill assessment, in this case mean squared error
        :param verb: verbose mode

        :return: time series of error terms
        &#34;&#34;&#34;
        init = tf.global_variables_initializer()
        err = []

        with Session() as session:
            init.run()

            for ep in range(self.epochs):
                session.run(opt, feed_dict=feed)

                if ep % 100 == 0:
                    mse = loss.eval(feed_dict=feed)
                    err.append(mse)
                    mse = loss.eval(feed_dict=feed)
                    if verb:
                        print(ep, &#34;\tMSE:&#34;, mse)

            self.model.saver().save(session, self.file)
            return err

    def predict(self, predictor, feed):
        &#34;&#34;&#34;
        Restore from file, and make a prediction

        :param predictor: handle for neural net
        :param feed: prediction feed
        :return:
        &#34;&#34;&#34;
        with Session() as session:
            self.model.saver().restore(session, self.file)
            return session.run(predictor, feed_dict=feed)

    def x(self, data):
        &#34;&#34;&#34;
        X data

        :param data:
        :return:
        &#34;&#34;&#34;
        n = len(data)
        end = n - n % self.periods
        subset = data[:end]
        return subset.reshape(self.shape)

    def y(self, data):
        &#34;&#34;&#34;
        Y data

        :param data:
        :return:
        &#34;&#34;&#34;
        n = len(data)
        end = n - n % self.periods

        subset = data[1 : end + self.horizon]
        return subset.reshape(self.shape)

    def predictor(self, X, lstm=True):
        &#34;&#34;&#34;
        Create graph for recurrent neural network

        :param X:
        :param lstm: use long short-term memory cells
        :return:
        &#34;&#34;&#34;

        cell = (
            tf.keras.layers.LSTMCell(units=self.hidden)
            if lstm
            else tf.keras.layers.SimpleRNNCell(units=self.hidden)
        )

        out, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
        stacked = tf.reshape(out, [-1, self.hidden])
        layers = tf.layers.dense(stacked, self.output)
        return tf.reshape(layers, [-1, self.periods, self.output])

    def optimizer(self, err):
        &#34;&#34;&#34;
        Stochastic gradient descent algorithm

        :param err: loss/error function tensor node
        :return:
        &#34;&#34;&#34;
        opt = tf.train.AdamOptimizer(learning_rate=self.rate)
        return opt.minimize(err)

    def nodes(self):
        &#34;&#34;&#34;
        X and y tensor graph nodes

        :return:
        &#34;&#34;&#34;

        X = placeholder(tf.float32, [None, self.periods, self.input])
        Y = placeholder(tf.float32, [None, self.periods, self.output])
        return X, Y

    def feed(self, ts: array, x: array, y: array, xp: array) -&gt; dict:
        &#34;&#34;&#34;
        Format data dictionaries to feed into model

        :param ts: time series
        :param x: X tensor nodes
        :param y: Y tensor nodes
        :param xp: x data for prediction
        :return:
        &#34;&#34;&#34;
        return {&#34;train&#34;: {x: self.x(ts), y: self.y(ts)}, &#34;predict&#34;: {x: xp}}

    @classmethod
    def run(cls, config, lstm: bool = True, verb: bool = True):
        &#34;&#34;&#34;
        Create and run the model

        :param config:
        :param lstm: use memory for drop out

        :return:
        &#34;&#34;&#34;

        network = cls(**config)  # create the neural net

        ts = cls.series()  # synthetic time series

        x, y = network.nodes()  # tensor graph nodes
        xt, yt = network.test(ts)  # split data for skill test

        feed = network.feed(ts, x, y, xt)  # feeds for training and prediction
        predictor = network.predictor(x, lstm=lstm)  # predictor model
        loss = reduce_sum(square(predictor - y))  # mean square error tensor node
        optimizer = network.optimizer(loss)  # learning model — minimize error

        network.model.train(
            optimizer, feed[&#34;train&#34;], loss, verb
        )  # train and get error series over epochs
        prediction = network.predict(predictor, feed[&#34;predict&#34;])  # make prediction
        return yt, prediction</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.create"><code class="name flex">
<span>def <span class="ident">create</span></span>(<span>stateful: bool, horizon: int, units: int, batch_size: int, variables: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Create the Keras-Tensorflow model</p>
<p>:param horizon: sequence length for training each output point
:param stateful: boolean, better if true
:param units: number of LSTM
:param batch_size:
:param variables: number of input variables</p>
<p>:return: model instance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create(
    stateful: bool, horizon: int, units: int, batch_size: int, variables: int = 1
):
    &#34;&#34;&#34;
    Create the Keras-Tensorflow model

    :param horizon: sequence length for training each output point
    :param stateful: boolean, better if true
    :param units: number of LSTM
    :param batch_size:
    :param variables: number of input variables

    :return: model instance
    &#34;&#34;&#34;

    model = tf.keras.models.Sequential()
    model.add(
        tf.keras.layers.LSTM(
            units=units,
            input_shape=(horizon, variables),
            batch_size=batch_size,
            stateful=stateful,
        )
    )
    model.add(tf.keras.layers.Dense(1))
    model.compile(loss=&#34;mse&#34;, optimizer=&#34;adam&#34;)

    return model</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.createAndCache"><code class="name flex">
<span>def <span class="ident">createAndCache</span></span>(<span>stateful: bool, horizon: int, layers: int, batch_size: int, cache: str, key: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and cache the model structure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def createAndCache(
    cls,
    stateful: bool,
    horizon: int,
    layers: int,
    batch_size: int,
    cache: str,
    key: str,
):
    &#34;&#34;&#34;
    Create and cache the model structure
    &#34;&#34;&#34;
    neural_net = cls.create(stateful, horizon, layers, batch_size)
    neural_net.save(&#34;/&#34;.join([&#34;&#34;, cache, key]))
    return 200, {&#34;message&#34;: &#34;model created and cached&#34;}</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.from_cache"><code class="name flex">
<span>def <span class="ident">from_cache</span></span>(<span>fcn)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_cache(fcn):
    def wrapper(*args, **kwargs):

        cache = kwargs.pop(&#34;cache&#34;)
        key = kwargs.pop(&#34;objectKey&#34;)

        # check connection
        # check key

        try:
            request.model = tf.keras.models.load_model(
                &#34;/&#34;.join([&#34;&#34;, cache, key])
            )  # load and inject object
        except:
            return 500, {&#34;message&#34;: &#34;Server error while loading model&#34;}

        return fcn(*args, **kwargs)

    return wrapper</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.get_prediction"><code class="name flex">
<span>def <span class="ident">get_prediction</span></span>(<span>datastream: object, batch_size: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>:param datastream:
:param batch_size: Number of samples per gradient update
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_prediction(datastream: object, batch_size: int = 32):
    &#34;&#34;&#34;

    :param datastream:
    :param batch_size: Number of samples per gradient update
    :return:
    &#34;&#34;&#34;
    predicted = request.model.predict(
        datastream, batch_size=batch_size, workers=1, use_multiprocessing=False
    )
    return 200, {&#34;payload&#34;: predicted.flatten()}</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>config, lstm: bool = True, verb: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and run the model</p>
<p>:param config:
:param lstm: use memory for drop out</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def run(cls, config, lstm: bool = True, verb: bool = True):
    &#34;&#34;&#34;
    Create and run the model

    :param config:
    :param lstm: use memory for drop out

    :return:
    &#34;&#34;&#34;

    network = cls(**config)  # create the neural net

    ts = cls.series()  # synthetic time series

    x, y = network.nodes()  # tensor graph nodes
    xt, yt = network.test(ts)  # split data for skill test

    feed = network.feed(ts, x, y, xt)  # feeds for training and prediction
    predictor = network.predictor(x, lstm=lstm)  # predictor model
    loss = reduce_sum(square(predictor - y))  # mean square error tensor node
    optimizer = network.optimizer(loss)  # learning model — minimize error

    network.model.train(
        optimizer, feed[&#34;train&#34;], loss, verb
    )  # train and get error series over epochs
    prediction = network.predict(predictor, feed[&#34;predict&#34;])  # make prediction
    return yt, prediction</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.series"><code class="name flex">
<span>def <span class="ident">series</span></span>(<span>n=365, show=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a synthetic training data set</p>
<p>:param n: steps
:param show: plot for time series</p>
<p>:return: numpy array of magnitudes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def series(n=365, show=False):
    &#34;&#34;&#34;
    Create a synthetic training data set

    :param n: steps
    :param show: plot for time series

    :return: numpy array of magnitudes
    &#34;&#34;&#34;
    rng = date_range(start=&#34;2018&#34;, periods=n, freq=&#34;D&#34;)
    data = Series(random.normal(0, 0.5, size=len(rng)), rng).cumsum()
    return array(data)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>model, batch_size: int, training: dict, validation: dict, epochs: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def train(model, batch_size: int, training: dict, validation: dict, epochs: int):

    for i in range(epochs):
        print(&#34;Epoch&#34;, i + 1, &#34;/&#34;, epochs)
        model.fit(
            training[&#34;x&#34;],
            training[&#34;y&#34;],
            batch_size=batch_size,
            epochs=1,  # different &#34;epochs&#34;
            verbose=False,
            validation_data=(validation[&#34;x&#34;], validation[&#34;y&#34;]),
            shuffle=False,  # order is important
            workers=1,
            use_multiprocessing=False,
        )

        model.reset_states()</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.train_cached_model"><code class="name flex">
<span>def <span class="ident">train_cached_model</span></span>(<span>datastream, window: int, horizon: int, batch_size: int, ratio: float, periods: int, epochs: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Load and feed the model training data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def train_cached_model(
    cls,
    datastream,
    window: int,
    horizon: int,
    batch_size: int,
    ratio: float,
    periods: int,
    epochs: int,
):
    &#34;&#34;&#34;
    Load and feed the model training data
    &#34;&#34;&#34;
    datasets = datastream.partition(window, horizon, batch_size, ratio, periods)

    cls.train(
        request.model,
        batch_size=batch_size,
        training=datasets.get(&#34;training&#34;),
        epochs=epochs,
        validation=datasets.get(&#34;validation&#34;),
    )

    return 200, {&#34;message&#34;: &#34;model trained and cached&#34;}</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.shape"><code class="name">var <span class="ident">shape</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self):
    return (-1, self.periods, 1)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.feed"><code class="name flex">
<span>def <span class="ident">feed</span></span>(<span>self, ts: array, x: array, y: array, xp: array) -> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Format data dictionaries to feed into model</p>
<p>:param ts: time series
:param x: X tensor nodes
:param y: Y tensor nodes
:param xp: x data for prediction
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feed(self, ts: array, x: array, y: array, xp: array) -&gt; dict:
    &#34;&#34;&#34;
    Format data dictionaries to feed into model

    :param ts: time series
    :param x: X tensor nodes
    :param y: Y tensor nodes
    :param xp: x data for prediction
    :return:
    &#34;&#34;&#34;
    return {&#34;train&#34;: {x: self.x(ts), y: self.y(ts)}, &#34;predict&#34;: {x: xp}}</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.nodes"><code class="name flex">
<span>def <span class="ident">nodes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>X and y tensor graph nodes</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nodes(self):
    &#34;&#34;&#34;
    X and y tensor graph nodes

    :return:
    &#34;&#34;&#34;

    X = placeholder(tf.float32, [None, self.periods, self.input])
    Y = placeholder(tf.float32, [None, self.periods, self.output])
    return X, Y</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.optimizer"><code class="name flex">
<span>def <span class="ident">optimizer</span></span>(<span>self, err)</span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic gradient descent algorithm</p>
<p>:param err: loss/error function tensor node
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimizer(self, err):
    &#34;&#34;&#34;
    Stochastic gradient descent algorithm

    :param err: loss/error function tensor node
    :return:
    &#34;&#34;&#34;
    opt = tf.train.AdamOptimizer(learning_rate=self.rate)
    return opt.minimize(err)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, predictor, feed)</span>
</code></dt>
<dd>
<div class="desc"><p>Restore from file, and make a prediction</p>
<p>:param predictor: handle for neural net
:param feed: prediction feed
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, predictor, feed):
    &#34;&#34;&#34;
    Restore from file, and make a prediction

    :param predictor: handle for neural net
    :param feed: prediction feed
    :return:
    &#34;&#34;&#34;
    with Session() as session:
        self.model.saver().restore(session, self.file)
        return session.run(predictor, feed_dict=feed)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.predictor"><code class="name flex">
<span>def <span class="ident">predictor</span></span>(<span>self, X, lstm=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Create graph for recurrent neural network</p>
<p>:param X:
:param lstm: use long short-term memory cells
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predictor(self, X, lstm=True):
    &#34;&#34;&#34;
    Create graph for recurrent neural network

    :param X:
    :param lstm: use long short-term memory cells
    :return:
    &#34;&#34;&#34;

    cell = (
        tf.keras.layers.LSTMCell(units=self.hidden)
        if lstm
        else tf.keras.layers.SimpleRNNCell(units=self.hidden)
    )

    out, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
    stacked = tf.reshape(out, [-1, self.hidden])
    layers = tf.layers.dense(stacked, self.output)
    return tf.reshape(layers, [-1, self.periods, self.output])</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Split data for prediction period.</p>
<p>:param data:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self, data):
    &#34;&#34;&#34;
    Split data for prediction period.

    :param data:
    :return:
    &#34;&#34;&#34;
    start = self.periods + self.horizon
    setup = data[-start:]
    x = setup[: self.periods].reshape(self.shape)
    y = data[-self.periods :].reshape(self.shape)
    return x, y</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.x"><code class="name flex">
<span>def <span class="ident">x</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>X data</p>
<p>:param data:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def x(self, data):
    &#34;&#34;&#34;
    X data

    :param data:
    :return:
    &#34;&#34;&#34;
    n = len(data)
    end = n - n % self.periods
    subset = data[:end]
    return subset.reshape(self.shape)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.x_train"><code class="name flex">
<span>def <span class="ident">x_train</span></span>(<span>self, opt, feed, loss, verb)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize and train the neural net.</p>
<p>:param opt: optimizer
:param feed:
:param loss: skill assessment, in this case mean squared error
:param verb: verbose mode</p>
<p>:return: time series of error terms</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def x_train(self, opt, feed, loss, verb):
    &#34;&#34;&#34;
    Initialize and train the neural net.

    :param opt: optimizer
    :param feed:
    :param loss: skill assessment, in this case mean squared error
    :param verb: verbose mode

    :return: time series of error terms
    &#34;&#34;&#34;
    init = tf.global_variables_initializer()
    err = []

    with Session() as session:
        init.run()

        for ep in range(self.epochs):
            session.run(opt, feed_dict=feed)

            if ep % 100 == 0:
                mse = loss.eval(feed_dict=feed)
                err.append(mse)
                mse = loss.eval(feed_dict=feed)
                if verb:
                    print(ep, &#34;\tMSE:&#34;, mse)

        self.model.saver().save(session, self.file)
        return err</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.RecurrentNeuralNetwork.y"><code class="name flex">
<span>def <span class="ident">y</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Y data</p>
<p>:param data:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def y(self, data):
    &#34;&#34;&#34;
    Y data

    :param data:
    :return:
    &#34;&#34;&#34;
    n = len(data)
    end = n - n % self.periods

    subset = data[1 : end + self.horizon]
    return subset.reshape(self.shape)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.RelationshipLabels"><code class="flex name class">
<span>class <span class="ident">RelationshipLabels</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RelationshipLabels(Enum):
    Self = 1
    Root = 2
    Parent = 3
    Collection = 4
    Derived = 5</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.RelationshipLabels.Collection"><code class="name">var <span class="ident">Collection</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.RelationshipLabels.Derived"><code class="name">var <span class="ident">Derived</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.RelationshipLabels.Parent"><code class="name">var <span class="ident">Parent</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.RelationshipLabels.Root"><code class="name">var <span class="ident">Root</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.RelationshipLabels.Self"><code class="name">var <span class="ident">Self</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Schema"><code class="flex name class">
<span>class <span class="ident">Schema</span></span>
<span>(</span><span>fields: [Field] = NOTHING)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Schema:
    fields: [Field] = attr.ib(default=attr.Factory(list))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.State"><code class="flex name class">
<span>class <span class="ident">State</span></span>
<span>(</span><span>orientation: array, axis: array, speed: float = 0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class State:

    orientation: array = attr.ib()  # facing
    axis: array = attr.ib()  # rotation
    speed: float = attr.ib(default=0.0)
    state3: array = zeros((1, 3), dtype=float)  # 3-axis rotation state
    state4: array = zeros((1, 4), dtype=float)  # 3-axis rotation state
    increment: array = zeros((1, 3), dtype=float)  # transformation increment</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.State.increment"><code class="name">var <span class="ident">increment</span> : <built-in function array></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.State.state3"><code class="name">var <span class="ident">state3</span> : <built-in function array></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.State.state4"><code class="name">var <span class="ident">state4</span> : <built-in function array></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Table"><code class="flex name class">
<span>class <span class="ident">Table</span></span>
<span>(</span><span>name: str, schema: Schema = Schema(fields=[]))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Table:

    name: str = attr.ib()
    schema: Schema = attr.ib(default=Schema())

    @staticmethod
    def _unwrap(x):
        &#34;&#34;&#34;
        Some queries return iterables that need to be unpacked
        &#34;&#34;&#34;
        return {&#34;record&#34;: x[0]}

    def declare(self) -&gt; Query:
        &#34;&#34;&#34;
        Generate a query to create a new table but do not execute
        &#34;&#34;&#34;
        fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
        queryString = f&#34;&#34;&#34;
        CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
        &#34;&#34;&#34;
        return Query(queryString, None)

    def insert(self, data: ()) -&gt; Query:
        &#34;&#34;&#34;
        Generate the query to insert new rows into database.
        &#34;&#34;&#34;
        _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
        columns, values = map(
            join, ((field.name for field in self.schema.fields), _parsedValues)
        )

        queryString = f&#34;&#34;&#34;
        INSERT INTO {self.name} ({columns}) VALUES {values};
        &#34;&#34;&#34;
        return Query(queryString, None)

    def select(
        self,
        order_by: str = None,
        limit: int = 100,
        fields: (str) = (&#34;*&#34;,),
        order: str = &#34;DESC&#34;,
        conditions: ((str)) = (),
    ) -&gt; Query:
        &#34;&#34;&#34;
        Read back values/rows.
        &#34;&#34;&#34;
        _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
        _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

        queryString = f&#34;&#34;&#34;
        SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
        &#34;&#34;&#34;

        return Query(queryString, Table._unwrap)

    def drop(self) -&gt; Query:
        &#34;&#34;&#34;
        Drop the entire table
        &#34;&#34;&#34;
        return Query(f&#34;DROP TABLE {self.name};&#34;, None)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Table.declare"><code class="name flex">
<span>def <span class="ident">declare</span></span>(<span>self) -> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate a query to create a new table but do not execute</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def declare(self) -&gt; Query:
    &#34;&#34;&#34;
    Generate a query to create a new table but do not execute
    &#34;&#34;&#34;
    fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
    queryString = f&#34;&#34;&#34;
    CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
    &#34;&#34;&#34;
    return Query(queryString, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.drop"><code class="name flex">
<span>def <span class="ident">drop</span></span>(<span>self) -> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Drop the entire table</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop(self) -&gt; Query:
    &#34;&#34;&#34;
    Drop the entire table
    &#34;&#34;&#34;
    return Query(f&#34;DROP TABLE {self.name};&#34;, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, data: ()) -> '<a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a>'</span>
</code></dt>
<dd>
<div class="desc"><p>Generate the query to insert new rows into database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, data: ()) -&gt; Query:
    &#34;&#34;&#34;
    Generate the query to insert new rows into database.
    &#34;&#34;&#34;
    _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
    columns, values = map(
        join, ((field.name for field in self.schema.fields), _parsedValues)
    )

    queryString = f&#34;&#34;&#34;
    INSERT INTO {self.name} ({columns}) VALUES {values};
    &#34;&#34;&#34;
    return Query(queryString, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, order_by: str = None, limit: int = 100, fields: str = ('*',), order: str = 'DESC', conditions: str = ()) -> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read back values/rows.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(
    self,
    order_by: str = None,
    limit: int = 100,
    fields: (str) = (&#34;*&#34;,),
    order: str = &#34;DESC&#34;,
    conditions: ((str)) = (),
) -&gt; Query:
    &#34;&#34;&#34;
    Read back values/rows.
    &#34;&#34;&#34;
    _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
    _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

    queryString = f&#34;&#34;&#34;
    SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
    &#34;&#34;&#34;

    return Query(queryString, Table._unwrap)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.TimeStamp"><code class="flex name class">
<span>class <span class="ident">TimeStamp</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TimeStamp:
    @staticmethod
    def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
        &#34;&#34;&#34;
        Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
        &#34;&#34;&#34;
        assert len(buffer) == 7
        yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
            int.from_bytes(buffer[:3], byteorder=byteorder),
            int.from_bytes(buffer[3:], byteorder=byteorder),
        )
        return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.TimeStamp.parseBinary"><code class="name flex">
<span>def <span class="ident">parseBinary</span></span>(<span>buffer: bytes, byteorder: str = 'big') -> datetime.datetime</span>
</code></dt>
<dd>
<div class="desc"><p>Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
    &#34;&#34;&#34;
    Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
    &#34;&#34;&#34;
    assert len(buffer) == 7
    yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
        int.from_bytes(buffer[:3], byteorder=byteorder),
        int.from_bytes(buffer[3:], byteorder=byteorder),
    )
    return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Topology"><code class="flex name class">
<span>class <span class="ident">Topology</span></span>
<span>(</span><span>cells: array = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Topology:

    cells: array = attr.ib(default=None)


    def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
        &#34;&#34;&#34;
        Get element neighbors
        &#34;&#34;&#34;
        queue = dict()
        while indices:
            cell = indices.pop()
            nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
            buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
            key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
            queue[key][cell] = buffer

        return queue


    @classmethod
    def read(path: str, indexed: bool = True) -&gt; dict:
        &#34;&#34;&#34;
        Read in grid topology of unstructured triangular grid
        &#34;&#34;&#34;
        if path[-3:] == &#34;.nc&#34;:
            fid = Dataset(path)
            topo = fid.variables[&#34;nv&#34;][:].T
        else:
            fid = open(path, &#34;r&#34;)
            df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
            topo = df.__array__()

        n = len(topo)

        basis = 0
        enforce = 1
        minimum = topo.min()
        if (minimum != enforce) if enforce else True:
            topo -= minimum + basis  # zero-index
        
        return {
            &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
            &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
        }

    @property
    def adjacency(self):
        &#34;&#34;&#34;
        Get node parents and node neighbors from topology

        :param topology:
        :return:
        &#34;&#34;&#34;
        _parents = dict()
        _neighbors = dict()

        for element in range(len(self.cells)):
            vertices = self.cells[element]
            for node in vertices:
                try:
                    p = _parents[node]
                except KeyError:
                    p = _parents[node] = []
                p.append(element)  # add element to parents, no possible duplicates

                try:
                    n = _neighbors[node]
                except KeyError:
                    n = _neighbors[node] = []
                (mask,) = where(node != vertices)
                others = vertices[mask]

                for neighbor in others:
                    if neighbor not in n:
                        n.append(neighbor)  # add current element to parents

        solid = zeros(n, dtype=bool)
        for node in range(n):
            difference = _neighbors[node].__len__() - _parents[node].__len__()
            if difference == 1:
                solid[node] = True
            elif difference != 0:
                print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)

    @staticmethod
    def deduplicate(self, process: bool = False) -&gt; array:

        n = len(self.cells)
        flag = zeros(n, dtype=bool)
        ordered = sorted(self.cells)

        for ii in range(n - 1):
            match = ordered[ii, :] == ordered[ii + 1 :, :]
            (rows,) = where(match)
            rows += ii + 1
            flag[rows] = True

        if process and flag.any():
            self.cells = self.cells[~flag]
            assert len(self.cells) == n - flag.sum()

        return self</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.deduplicate"><code class="name flex">
<span>def <span class="ident">deduplicate</span></span>(<span>self, process: bool = False) -> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def deduplicate(self, process: bool = False) -&gt; array:

    n = len(self.cells)
    flag = zeros(n, dtype=bool)
    ordered = sorted(self.cells)

    for ii in range(n - 1):
        match = ordered[ii, :] == ordered[ii + 1 :, :]
        (rows,) = where(match)
        rows += ii + 1
        flag[rows] = True

    if process and flag.any():
        self.cells = self.cells[~flag]
        assert len(self.cells) == n - flag.sum()

    return self</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Topology.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>indexed: bool = True) -> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read in grid topology of unstructured triangular grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def read(path: str, indexed: bool = True) -&gt; dict:
    &#34;&#34;&#34;
    Read in grid topology of unstructured triangular grid
    &#34;&#34;&#34;
    if path[-3:] == &#34;.nc&#34;:
        fid = Dataset(path)
        topo = fid.variables[&#34;nv&#34;][:].T
    else:
        fid = open(path, &#34;r&#34;)
        df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
        topo = df.__array__()

    n = len(topo)

    basis = 0
    enforce = 1
    minimum = topo.min()
    if (minimum != enforce) if enforce else True:
        topo -= minimum + basis  # zero-index
    
    return {
        &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
        &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
    }</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.adjacency"><code class="name">var <span class="ident">adjacency</span></code></dt>
<dd>
<div class="desc"><p>Get node parents and node neighbors from topology</p>
<p>:param topology:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def adjacency(self):
    &#34;&#34;&#34;
    Get node parents and node neighbors from topology

    :param topology:
    :return:
    &#34;&#34;&#34;
    _parents = dict()
    _neighbors = dict()

    for element in range(len(self.cells)):
        vertices = self.cells[element]
        for node in vertices:
            try:
                p = _parents[node]
            except KeyError:
                p = _parents[node] = []
            p.append(element)  # add element to parents, no possible duplicates

            try:
                n = _neighbors[node]
            except KeyError:
                n = _neighbors[node] = []
            (mask,) = where(node != vertices)
            others = vertices[mask]

            for neighbor in others:
                if neighbor not in n:
                    n.append(neighbor)  # add current element to parents

    solid = zeros(n, dtype=bool)
    for node in range(n):
        difference = _neighbors[node].__len__() - _parents[node].__len__()
        if difference == 1:
            solid[node] = True
        elif difference != 0:
            print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.cell_adjacency"><code class="name flex">
<span>def <span class="ident">cell_adjacency</span></span>(<span>self, parents: dict, indices: [int]) -> 'dict'</span>
</code></dt>
<dd>
<div class="desc"><p>Get element neighbors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
    &#34;&#34;&#34;
    Get element neighbors
    &#34;&#34;&#34;
    queue = dict()
    while indices:
        cell = indices.pop()
        nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
        buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
        key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
        queue[key][cell] = buffer

    return queue</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Trie"><code class="flex name class">
<span>class <span class="ident">Trie</span></span>
<span>(</span><span>word=None, children=NOTHING)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trie:

    word = attr.ib(default=None)
    children = attr.ib(default=attr.Factory(dict))

    def insert(self, key: str) -&gt; None:
        node = self
        for letter in key:
            if letter not in node.children:
                node.children[letter] = Trie()
            node = node.children[letter]
        node.word = key

    @staticmethod
    def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
        _filter = lambda x: len(x)
        row = (previous[0] + 1,)
        for column in range(1, len(pattern) + 1):
            row += (
                min(
                    row[column - 1] + 1,
                    previous[column] + 1,
                    previous[column - 1] + int(pattern[column - 1] != symbol),
                ),
            )

        return (
            ((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()
        ) + tuple(
            chain(
                *filter(
                    _filter,
                    tuple(
                        Trie.searchRecursive(v, k, pattern, row, cost)
                        for k, v in node.children.items()
                    ),
                )
            )
            if min(row) &lt;= cost
            else ()
        )

    @staticmethod
    def levenshteinDistance(word1: str, word2: str) -&gt; int:
        columns = len(word1) + 1
        rows = len(word2) + 1

        # build first row
        currentRow = [0]
        for column in range(1, columns):
            currentRow.append(currentRow[column - 1] + 1)

        for row in range(1, rows):
            previousRow = currentRow
            currentRow = [previousRow[0] + 1]

            for column in range(1, columns):

                insertCost = currentRow[column - 1] + 1
                deleteCost = previousRow[column] + 1

                if word1[column - 1] != word2[row - 1]:
                    replaceCost = previousRow[column - 1] + 1
                else:
                    replaceCost = previousRow[column - 1]

                currentRow.append(min(insertCost, deleteCost, replaceCost))

        return currentRow[-1]

    @staticmethod
    def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
        _results = ()
        for word in words:
            cost = Trie.levenshteinDistance(pattern, word)
            if cost &lt;= maxCost:
                _results += ((word, cost),)
        return _results</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Trie.levenshteinDistance"><code class="name flex">
<span>def <span class="ident">levenshteinDistance</span></span>(<span>word1: str, word2: str) -> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def levenshteinDistance(word1: str, word2: str) -&gt; int:
    columns = len(word1) + 1
    rows = len(word2) + 1

    # build first row
    currentRow = [0]
    for column in range(1, columns):
        currentRow.append(currentRow[column - 1] + 1)

    for row in range(1, rows):
        previousRow = currentRow
        currentRow = [previousRow[0] + 1]

        for column in range(1, columns):

            insertCost = currentRow[column - 1] + 1
            deleteCost = previousRow[column] + 1

            if word1[column - 1] != word2[row - 1]:
                replaceCost = previousRow[column - 1] + 1
            else:
                replaceCost = previousRow[column - 1]

            currentRow.append(min(insertCost, deleteCost, replaceCost))

    return currentRow[-1]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Trie.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>words: {str}, pattern: str, maxCost: int) -> '(str, int)'</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
    _results = ()
    for word in words:
        cost = Trie.levenshteinDistance(pattern, word)
        if cost &lt;= maxCost:
            _results += ((word, cost),)
    return _results</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Trie.searchRecursive"><code class="name flex">
<span>def <span class="ident">searchRecursive</span></span>(<span>node, symbol: str, pattern: str, previous: (int,), cost: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
    _filter = lambda x: len(x)
    row = (previous[0] + 1,)
    for column in range(1, len(pattern) + 1):
        row += (
            min(
                row[column - 1] + 1,
                previous[column] + 1,
                previous[column - 1] + int(pattern[column - 1] != symbol),
            ),
        )

    return (
        ((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()
    ) + tuple(
        chain(
            *filter(
                _filter,
                tuple(
                    Trie.searchRecursive(v, k, pattern, row, cost)
                    for k, v in node.children.items()
                ),
            )
        )
        if min(row) &lt;= cost
        else ()
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Trie.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, key: str) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, key: str) -&gt; None:
    node = self
    for letter in key:
        if letter not in node.children:
            node.children[letter] = Trie()
        node = node.children[letter]
    node.word = key</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Unit"><code class="flex name class">
<span>class <span class="ident">Unit</span></span>
<span>(</span><span>name: str = None, symbol: Any = None, definition: str = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Unit:
    name: str = attr.ib(default=None)  # canonical reference name
    symbol: Any = attr.ib(default=None)  # symbol when displayed
    definition: str = attr.ib(
        default=None
    )  # reference to an external formal definition</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.VertexArray"><code class="flex name class">
<span>class <span class="ident">VertexArray</span></span>
<span>(</span><span>data: array = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VertexArray:

    data: array = attr.ib(default=None)

    def deduplicate(self, topology: Topology = None, threshold: float = 0.00001):
        &#34;&#34;&#34;
        Scan vertex array for duplicates. If topology is also provided, swap later indices for their lower-index
        equivalents. Can be very expensive!

        :param topology:
        :param threshold:

        :return: deletion flags and modified topology array
        &#34;&#34;&#34;
        assert self.data.shape[1], &#34;Must have explicit dimensionality &gt;= 1&#34;
        flag = zeros(self.data.shape[0], dtype=bool)  # mask for indexing
        delta = zeros(self.data.shape, dtype=float)

        for ii in range(self.data.shape[0] - 1):
            if flag[ii]:  # already processed
                continue
            # distance for unchecked vertices
            delta[ii + 1, :] = self.data[ii, :] - self.data[ii + 1 :, :]
            distance = norm(delta[ii + 1, :])  # magnitude of difference vec is distance
            (rows,) = where(distance &lt; threshold)  # indices of points within threshold
            rows += ii + 1
            flag[rows] = True  # set look-ahead flags true for deletion

            if topology:
                for jj in rows:
                    # get rows and columns indices of duplicates
                    re, ce = where(topology == jj)
                    topology[re, ce] = ii  # replace duplicate indices

        if flag.any():  # there are duplicates
            (retain,) = where(~flag)  # first occurrences
            # reversed un-flagged points
            iterator = zip(retain, flip(retain, axis=0)[0 : len(retain)])
            for first, last in iterator:
                if first &gt; last:
                    break  # no swaps left

                self.data[first, :], self.data[last, :] = (
                    self.data[last, :],
                    self.data[first, :],
                )
                ri, ci = where(topology == first)
                rj, cj = where(topology == last)
                topology[ri, ci] = last
                topology[rj, cj] = first

            self.data = self.data[0 : len(retain), :]
        return self, topology</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.VertexArray.deduplicate"><code class="name flex">
<span>def <span class="ident">deduplicate</span></span>(<span>self, topology: Topology = None, threshold: float = 1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Scan vertex array for duplicates. If topology is also provided, swap later indices for their lower-index
equivalents. Can be very expensive!</p>
<p>:param topology:
:param threshold:</p>
<p>:return: deletion flags and modified topology array</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deduplicate(self, topology: Topology = None, threshold: float = 0.00001):
    &#34;&#34;&#34;
    Scan vertex array for duplicates. If topology is also provided, swap later indices for their lower-index
    equivalents. Can be very expensive!

    :param topology:
    :param threshold:

    :return: deletion flags and modified topology array
    &#34;&#34;&#34;
    assert self.data.shape[1], &#34;Must have explicit dimensionality &gt;= 1&#34;
    flag = zeros(self.data.shape[0], dtype=bool)  # mask for indexing
    delta = zeros(self.data.shape, dtype=float)

    for ii in range(self.data.shape[0] - 1):
        if flag[ii]:  # already processed
            continue
        # distance for unchecked vertices
        delta[ii + 1, :] = self.data[ii, :] - self.data[ii + 1 :, :]
        distance = norm(delta[ii + 1, :])  # magnitude of difference vec is distance
        (rows,) = where(distance &lt; threshold)  # indices of points within threshold
        rows += ii + 1
        flag[rows] = True  # set look-ahead flags true for deletion

        if topology:
            for jj in rows:
                # get rows and columns indices of duplicates
                re, ce = where(topology == jj)
                topology[re, ce] = ii  # replace duplicate indices

    if flag.any():  # there are duplicates
        (retain,) = where(~flag)  # first occurrences
        # reversed un-flagged points
        iterator = zip(retain, flip(retain, axis=0)[0 : len(retain)])
        for first, last in iterator:
            if first &gt; last:
                break  # no swaps left

            self.data[first, :], self.data[last, :] = (
                self.data[last, :],
                self.data[first, :],
            )
            ri, ci = where(topology == first)
            rj, cj = where(topology == last)
            topology[ri, ci] = last
            topology[rj, cj] = first

        self.data = self.data[0 : len(retain), :]
    return self, topology</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.View"><code class="flex name class">
<span>class <span class="ident">View</span></span>
<span>(</span><span>style, extent=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup and return figure and axis instances</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class View:
    count = 0

    def __init__(self, style, extent=None):
        # type: (dict, (float,)) -&gt; View
        &#34;&#34;&#34;
        Setup and return figure and axis instances
        &#34;&#34;&#34;
        rc(&#34;text&#34;, usetex=False)
        # rc(&#34;font&#34;, **{&#34;family&#34;: &#34;sans-serif&#34;, &#34;sans-serif&#34;: [&#34;Arial&#34;]})
        rc(&#34;mathtext&#34;, default=&#34;sf&#34;)
        rc(&#34;lines&#34;, markeredgewidth=1, linewidth=style[&#34;line&#34;])
        rc(&#34;axes&#34;, labelsize=style[&#34;text&#34;], linewidth=(style[&#34;line&#34;] + 1) // 2)
        rc(&#34;xtick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;ytick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;xtick.major&#34;, pad=5)
        rc(&#34;ytick.major&#34;, pad=5)

        self.style = style
        self.extent = extent
        self.fig, self.ax = subplots(
            facecolor=style[&#34;bg&#34;], figsize=(style[&#34;width&#34;], style[&#34;height&#34;])
        )
        padding = style[&#34;padding&#34;]
        subplots_adjust(
            left=padding[0], bottom=padding[1], right=1 - padding[2], top=1 - padding[3]
        )

    def format(self, bg: str, contrast: str, **kwargs):
        &#34;&#34;&#34;
        Setup color styles for figure
        &#34;&#34;&#34;
        self.ax.patch.set_facecolor(bg)  # background colors
        self.ax.edgecolor = contrast  # plotting area border
        self.format_axis(&#34;x&#34;, contrast, **kwargs)
        self.format_axis(&#34;y&#34;, contrast, **kwargs)

    def format_axis(
        self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
    ):
        if axis in (&#34;x&#34;, &#34;X&#34;):
            apply = self.ax.xaxis
            spines = (&#34;left&#34;, &#34;right&#34;)
        elif axis in (&#34;y&#34;, &#34;Y&#34;):
            apply = self.ax.yaxis
            spines = (&#34;top&#34;, &#34;bottom&#34;)
        else:
            raise ValueError

        apply.label.set_color(label)
        self.ax.tick_params(axis=axis.lower(), colors=label)
        for each in spines:
            self.ax.spines[each].set_color(contrast)
        apply.grid(grid)

    def pre_push(self):
        self.fig.canvas.draw()
        self.format(**self.style)
        self.ax.set_frame_on(True)

    def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
        # type: (str, bool, dict) -&gt; BytesIO
        buffer = BytesIO()
        self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
        buffer.seek(0)
        return buffer

    def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
        &#34;&#34;&#34;
        Format figure legend

        Kwargs:
            loc, str -- location on plotting area
            fc, str/arr -- string or RGBA color for face
            ec, str/arr -- string or RGBA color for edges

        Returns: matplotlib legend object
        &#34;&#34;&#34;
        legend = self.ax.legend(loc=loc)
        frame = legend.get_frame()
        frame.set_facecolor(fc)
        frame.set_edgecolor(ec)

        for text in legend.get_texts():
            text.set_color(self.style[&#34;contrast&#34;])</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.View.count"><code class="name">var <span class="ident">count</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.View.format"><code class="name flex">
<span>def <span class="ident">format</span></span>(<span>self, bg: str, contrast: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup color styles for figure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format(self, bg: str, contrast: str, **kwargs):
    &#34;&#34;&#34;
    Setup color styles for figure
    &#34;&#34;&#34;
    self.ax.patch.set_facecolor(bg)  # background colors
    self.ax.edgecolor = contrast  # plotting area border
    self.format_axis(&#34;x&#34;, contrast, **kwargs)
    self.format_axis(&#34;y&#34;, contrast, **kwargs)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.format_axis"><code class="name flex">
<span>def <span class="ident">format_axis</span></span>(<span>self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_axis(
    self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
):
    if axis in (&#34;x&#34;, &#34;X&#34;):
        apply = self.ax.xaxis
        spines = (&#34;left&#34;, &#34;right&#34;)
    elif axis in (&#34;y&#34;, &#34;Y&#34;):
        apply = self.ax.yaxis
        spines = (&#34;top&#34;, &#34;bottom&#34;)
    else:
        raise ValueError

    apply.label.set_color(label)
    self.ax.tick_params(axis=axis.lower(), colors=label)
    for each in spines:
        self.ax.spines[each].set_color(contrast)
    apply.grid(grid)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.legend"><code class="name flex">
<span>def <span class="ident">legend</span></span>(<span>self, loc: str = 'best', fc: str = 'none', ec: str = 'none')</span>
</code></dt>
<dd>
<div class="desc"><p>Format figure legend</p>
<h2 id="kwargs">Kwargs</h2>
<p>loc, str &ndash; location on plotting area
fc, str/arr &ndash; string or RGBA color for face
ec, str/arr &ndash; string or RGBA color for edges</p>
<p>Returns: matplotlib legend object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
    &#34;&#34;&#34;
    Format figure legend

    Kwargs:
        loc, str -- location on plotting area
        fc, str/arr -- string or RGBA color for face
        ec, str/arr -- string or RGBA color for edges

    Returns: matplotlib legend object
    &#34;&#34;&#34;
    legend = self.ax.legend(loc=loc)
    frame = legend.get_frame()
    frame.set_facecolor(fc)
    frame.set_edgecolor(ec)

    for text in legend.get_texts():
        text.set_color(self.style[&#34;contrast&#34;])</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.pre_push"><code class="name flex">
<span>def <span class="ident">pre_push</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_push(self):
    self.fig.canvas.draw()
    self.format(**self.style)
    self.ax.set_frame_on(True)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.push"><code class="name flex">
<span>def <span class="ident">push</span></span>(<span>self, encoding='png', transparent=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
    # type: (str, bool, dict) -&gt; BytesIO
    buffer = BytesIO()
    self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
    buffer.seek(0)
    return buffer</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Wind"><code class="flex name class">
<span>class <span class="ident">Wind</span></span>
<span>(</span><span>speed: float or array = 0.0, delta: float or array = 0.0, validRange: (float, float) = 0.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate wind speed and mixing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Wind:
    &#34;&#34;&#34;
    Simulate wind speed and mixing.
    &#34;&#34;&#34;

    speed: float or array = attr.ib(default=0.0)
    delta: float or array = attr.ib(default=0.0)
    validRange: (float, float) = attr.ib(default=0.0)

    @property
    def simpleMixing(self,) -&gt; float or array:
        &#34;&#34;&#34;
        Basic wind mixing rate.
        &#34;&#34;&#34;
        return 0.728 * self.speed ** 0.5 - 0.317 * self.speed + 0.0372 * self.speed ** 2

    def update(self, dt: float):
        &#34;&#34;&#34;
        Update velocity shear due to wind forcing
        &#34;&#34;&#34;
        self.speed += self.delta * dt
        return self

    @staticmethod
    def shear(velocity: array, topology: array, precision: type = float) -&gt; array:
        &#34;&#34;&#34;
        Calculate current velocity shear vector for selected cells

        :param velocity: velocity field, assumed to be already subset to surface and U, V components
        :param topology: tuple of parents of the node
        :param precision:

        :return:
        &#34;&#34;&#34;

        n = len(topology)
        vectors = zeros((n, 2), dtype=precision)  # shear vectors

        for ii in range(n):
            parents = topology[ii]
            sq = (
                velocity[parents, :, :].mean(axis=0) ** 2
            )  # shape is (points, 3, layers, dim)
            vectors[ii] += (
                sq.sum(axis=0) ** 0.5
            )  # reduce to root of the sums, shape is (dim)

        return abs(vectors[:, 0] - vectors[:, 1])

    def dynamicMixing(
        self,
        mapping: ((int), (int)),
        velocity: array,
        diffusivity: float or array = 0.0,
    ) -&gt; array:
        &#34;&#34;&#34;

        :param nodes: object
        :param layers: object
        :param velocity: water velocity field
        :param diffusivity: of oxygen across air-water interface, m2/day

        :return: mixing rate
        &#34;&#34;&#34;
        nodes, layers = mapping
        depth = nodes.depth * layers.z[:2].mean()
        subset = velocity[:, :2, :2]
        return ((diffusivity * Wind.shear(subset, nodes.parents)) / depth ** 0.5).clip(
            min=self.validRange[0]
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Wind.shear"><code class="name flex">
<span>def <span class="ident">shear</span></span>(<span>velocity: array, topology: array, precision: type = builtins.float) -> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Calculate current velocity shear vector for selected cells</p>
<p>:param velocity: velocity field, assumed to be already subset to surface and U, V components
:param topology: tuple of parents of the node
:param precision:</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def shear(velocity: array, topology: array, precision: type = float) -&gt; array:
    &#34;&#34;&#34;
    Calculate current velocity shear vector for selected cells

    :param velocity: velocity field, assumed to be already subset to surface and U, V components
    :param topology: tuple of parents of the node
    :param precision:

    :return:
    &#34;&#34;&#34;

    n = len(topology)
    vectors = zeros((n, 2), dtype=precision)  # shear vectors

    for ii in range(n):
        parents = topology[ii]
        sq = (
            velocity[parents, :, :].mean(axis=0) ** 2
        )  # shape is (points, 3, layers, dim)
        vectors[ii] += (
            sq.sum(axis=0) ** 0.5
        )  # reduce to root of the sums, shape is (dim)

    return abs(vectors[:, 0] - vectors[:, 1])</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Wind.simpleMixing"><code class="name">var <span class="ident">simpleMixing</span> : float</code></dt>
<dd>
<div class="desc"><p>Basic wind mixing rate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def simpleMixing(self,) -&gt; float or array:
    &#34;&#34;&#34;
    Basic wind mixing rate.
    &#34;&#34;&#34;
    return 0.728 * self.speed ** 0.5 - 0.317 * self.speed + 0.0372 * self.speed ** 2</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Wind.dynamicMixing"><code class="name flex">
<span>def <span class="ident">dynamicMixing</span></span>(<span>self, mapping: (int, int), velocity: array, diffusivity: float or array = 0.0) -> 'array'</span>
</code></dt>
<dd>
<div class="desc"><p>:param nodes: object
:param layers: object
:param velocity: water velocity field
:param diffusivity: of oxygen across air-water interface, m2/day</p>
<p>:return: mixing rate</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dynamicMixing(
    self,
    mapping: ((int), (int)),
    velocity: array,
    diffusivity: float or array = 0.0,
) -&gt; array:
    &#34;&#34;&#34;

    :param nodes: object
    :param layers: object
    :param velocity: water velocity field
    :param diffusivity: of oxygen across air-water interface, m2/day

    :return: mixing rate
    &#34;&#34;&#34;
    nodes, layers = mapping
    depth = nodes.depth * layers.z[:2].mean()
    subset = velocity[:, :2, :2]
    return ((diffusivity * Wind.shear(subset, nodes.parents)) / depth ** 0.5).clip(
        min=self.validRange[0]
    )</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Wind.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, dt: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Update velocity shear due to wind forcing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, dt: float):
    &#34;&#34;&#34;
    Update velocity shear due to wind forcing
    &#34;&#34;&#34;
    self.speed += self.delta * dt
    return self</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="bathysphere" href="index.html">bathysphere</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="bathysphere.datatypes.ConvexHull" href="#bathysphere.datatypes.ConvexHull">ConvexHull</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Array.colorize" href="#bathysphere.datatypes.Array.colorize">colorize</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.interval" href="#bathysphere.datatypes.Array.interval">interval</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.normalized" href="#bathysphere.datatypes.Array.normalized">normalized</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.range" href="#bathysphere.datatypes.Array.range">range</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.BoundingBox" href="#bathysphere.datatypes.BoundingBox">BoundingBox</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.ChemicalSystem" href="#bathysphere.datatypes.ChemicalSystem">ChemicalSystem</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.ChemicalSystem.clamp" href="#bathysphere.datatypes.ChemicalSystem.clamp">clamp</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.delta" href="#bathysphere.datatypes.ChemicalSystem.delta">delta</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.flux" href="#bathysphere.datatypes.ChemicalSystem.flux">flux</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.mass" href="#bathysphere.datatypes.ChemicalSystem.mass">mass</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.massAdded" href="#bathysphere.datatypes.ChemicalSystem.massAdded">massAdded</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.sources" href="#bathysphere.datatypes.ChemicalSystem.sources">sources</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.symbol" href="#bathysphere.datatypes.ChemicalSystem.symbol">symbol</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.transfer" href="#bathysphere.datatypes.ChemicalSystem.transfer">transfer</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.validRange" href="#bathysphere.datatypes.ChemicalSystem.validRange">validRange</a></code></li>
<li><code><a title="bathysphere.datatypes.ChemicalSystem.value" href="#bathysphere.datatypes.ChemicalSystem.value">value</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Clock" href="#bathysphere.datatypes.Clock">Clock</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Clock.days" href="#bathysphere.datatypes.Clock.days">days</a></code></li>
<li><code><a title="bathysphere.datatypes.Clock.next" href="#bathysphere.datatypes.Clock.next">next</a></code></li>
<li><code><a title="bathysphere.datatypes.Clock.tick" href="#bathysphere.datatypes.Clock.tick">tick</a></code></li>
<li><code><a title="bathysphere.datatypes.Clock.time" href="#bathysphere.datatypes.Clock.time">time</a></code></li>
<li><code><a title="bathysphere.datatypes.Clock.yd" href="#bathysphere.datatypes.Clock.yd">yd</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.CloudSQL" href="#bathysphere.datatypes.CloudSQL">CloudSQL</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.CloudSQL.engine" href="#bathysphere.datatypes.CloudSQL.engine">engine</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.handle" href="#bathysphere.datatypes.CloudSQL.handle">handle</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.query" href="#bathysphere.datatypes.CloudSQL.query">query</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Condition" href="#bathysphere.datatypes.Condition">Condition</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.Condition.FallLine" href="#bathysphere.datatypes.Condition.FallLine">FallLine</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.NonPointSource" href="#bathysphere.datatypes.Condition.NonPointSource">NonPointSource</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.PointSource" href="#bathysphere.datatypes.Condition.PointSource">PointSource</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.Surface" href="#bathysphere.datatypes.Condition.Surface">Surface</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.boundary" href="#bathysphere.datatypes.Condition.boundary">boundary</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.delta" href="#bathysphere.datatypes.Condition.delta">delta</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.mark" href="#bathysphere.datatypes.Condition.mark">mark</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.read" href="#bathysphere.datatypes.Condition.read">read</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.source" href="#bathysphere.datatypes.Condition.source">source</a></code></li>
<li><code><a title="bathysphere.datatypes.Condition.update" href="#bathysphere.datatypes.Condition.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.CoordinateSystem" href="#bathysphere.datatypes.CoordinateSystem">CoordinateSystem</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.CoordinateSystem.Cartesian" href="#bathysphere.datatypes.CoordinateSystem.Cartesian">Cartesian</a></code></li>
<li><code><a title="bathysphere.datatypes.CoordinateSystem.Gaussian" href="#bathysphere.datatypes.CoordinateSystem.Gaussian">Gaussian</a></code></li>
<li><code><a title="bathysphere.datatypes.CoordinateSystem.Periodic" href="#bathysphere.datatypes.CoordinateSystem.Periodic">Periodic</a></code></li>
<li><code><a title="bathysphere.datatypes.CoordinateSystem.Sigma" href="#bathysphere.datatypes.CoordinateSystem.Sigma">Sigma</a></code></li>
<li><code><a title="bathysphere.datatypes.CoordinateSystem.Spherical" href="#bathysphere.datatypes.CoordinateSystem.Spherical">Spherical</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Coordinates" href="#bathysphere.datatypes.Coordinates">Coordinates</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.DataFormat" href="#bathysphere.datatypes.DataFormat">DataFormat</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.DataFormat.ArrayfireTexture" href="#bathysphere.datatypes.DataFormat.ArrayfireTexture">ArrayfireTexture</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.Binary" href="#bathysphere.datatypes.DataFormat.Binary">Binary</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.Custom" href="#bathysphere.datatypes.DataFormat.Custom">Custom</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.NETCDF3_CLASSIC" href="#bathysphere.datatypes.DataFormat.NETCDF3_CLASSIC">NETCDF3_CLASSIC</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.NETCDF4" href="#bathysphere.datatypes.DataFormat.NETCDF4">NETCDF4</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.NETCDF5" href="#bathysphere.datatypes.DataFormat.NETCDF5">NETCDF5</a></code></li>
<li><code><a title="bathysphere.datatypes.DataFormat.NumpyArray" href="#bathysphere.datatypes.DataFormat.NumpyArray">NumpyArray</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Dataset" href="#bathysphere.datatypes.Dataset">Dataset</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Dataset.copy" href="#bathysphere.datatypes.Dataset.copy">copy</a></code></li>
<li><code><a title="bathysphere.datatypes.Dataset.query" href="#bathysphere.datatypes.Dataset.query">query</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Distance" href="#bathysphere.datatypes.Distance">Distance</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.DoublyLinkedList" href="#bathysphere.datatypes.DoublyLinkedList">DoublyLinkedList</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.insert_after" href="#bathysphere.datatypes.DoublyLinkedList.insert_after">insert_after</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.insert_before" href="#bathysphere.datatypes.DoublyLinkedList.insert_before">insert_before</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.k_from_end" href="#bathysphere.datatypes.DoublyLinkedList.k_from_end">k_from_end</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.prev" href="#bathysphere.datatypes.DoublyLinkedList.prev">prev</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.push_back" href="#bathysphere.datatypes.DoublyLinkedList.push_back">push_back</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.push_front" href="#bathysphere.datatypes.DoublyLinkedList.push_front">push_front</a></code></li>
<li><code><a title="bathysphere.datatypes.DoublyLinkedList.traverse_backward" href="#bathysphere.datatypes.DoublyLinkedList.traverse_backward">traverse_backward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Extent" href="#bathysphere.datatypes.Extent">Extent</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Extent.bounding_box" href="#bathysphere.datatypes.Extent.bounding_box">bounding_box</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.intervals" href="#bathysphere.datatypes.Extent.intervals">intervals</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.overlaps" href="#bathysphere.datatypes.Extent.overlaps">overlaps</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.path" href="#bathysphere.datatypes.Extent.path">path</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.vertex_array" href="#bathysphere.datatypes.Extent.vertex_array">vertex_array</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Feature" href="#bathysphere.datatypes.Feature">Feature</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Feature.geometry" href="#bathysphere.datatypes.Feature.geometry">geometry</a></code></li>
<li><code><a title="bathysphere.datatypes.Feature.properties" href="#bathysphere.datatypes.Feature.properties">properties</a></code></li>
<li><code><a title="bathysphere.datatypes.Feature.type" href="#bathysphere.datatypes.Feature.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FeatureCollection" href="#bathysphere.datatypes.FeatureCollection">FeatureCollection</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.FeatureCollection.features" href="#bathysphere.datatypes.FeatureCollection.features">features</a></code></li>
<li><code><a title="bathysphere.datatypes.FeatureCollection.properties" href="#bathysphere.datatypes.FeatureCollection.properties">properties</a></code></li>
<li><code><a title="bathysphere.datatypes.FeatureCollection.type" href="#bathysphere.datatypes.FeatureCollection.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Field" href="#bathysphere.datatypes.Field">Field</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Field.autoCorrect" href="#bathysphere.datatypes.Field.autoCorrect">autoCorrect</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.clean" href="#bathysphere.datatypes.Field.clean">clean</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.restore" href="#bathysphere.datatypes.Field.restore">restore</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.File" href="#bathysphere.datatypes.File">File</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.File.get_and_decode" href="#bathysphere.datatypes.File.get_and_decode">get_and_decode</a></code></li>
<li><code><a title="bathysphere.datatypes.File.metadata" href="#bathysphere.datatypes.File.metadata">metadata</a></code></li>
<li><code><a title="bathysphere.datatypes.File.metadata_promise" href="#bathysphere.datatypes.File.metadata_promise">metadata_promise</a></code></li>
<li><code><a title="bathysphere.datatypes.File.serialize" href="#bathysphere.datatypes.File.serialize">serialize</a></code></li>
<li><code><a title="bathysphere.datatypes.File.sort_key" href="#bathysphere.datatypes.File.sort_key">sort_key</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FileSystem" href="#bathysphere.datatypes.FileSystem">FileSystem</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.FileSystem.OverwritePolicy" href="#bathysphere.datatypes.FileSystem.OverwritePolicy">OverwritePolicy</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.download" href="#bathysphere.datatypes.FileSystem.download">download</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.get" href="#bathysphere.datatypes.FileSystem.get">get</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.get_files" href="#bathysphere.datatypes.FileSystem.get_files">get_files</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexFileMetadata" href="#bathysphere.datatypes.FileSystem.indexFileMetadata">indexFileMetadata</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexFtp" href="#bathysphere.datatypes.FileSystem.indexFtp">indexFtp</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexTaskTree" href="#bathysphere.datatypes.FileSystem.indexTaskTree">indexTaskTree</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.load_year_cache" href="#bathysphere.datatypes.FileSystem.load_year_cache">load_year_cache</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.policy" href="#bathysphere.datatypes.FileSystem.policy">policy</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.search" href="#bathysphere.datatypes.FileSystem.search">search</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.syncFtp" href="#bathysphere.datatypes.FileSystem.syncFtp">syncFtp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FileType" href="#bathysphere.datatypes.FileType">FileType</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.FileType.CSV" href="#bathysphere.datatypes.FileType.CSV">CSV</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Config" href="#bathysphere.datatypes.FileType.Config">Config</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.JSON" href="#bathysphere.datatypes.FileType.JSON">JSON</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Log" href="#bathysphere.datatypes.FileType.Log">Log</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Raw" href="#bathysphere.datatypes.FileType.Raw">Raw</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Schema" href="#bathysphere.datatypes.FileType.Schema">Schema</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.XML" href="#bathysphere.datatypes.FileType.XML">XML</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Frame" href="#bathysphere.datatypes.Frame">Frame</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.Frame.analog" href="#bathysphere.datatypes.Frame.analog">analog</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.ascii_xml" href="#bathysphere.datatypes.Frame.ascii_xml">ascii_xml</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.binary_xml" href="#bathysphere.datatypes.Frame.binary_xml">binary_xml</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.by_key" href="#bathysphere.datatypes.Frame.by_key">by_key</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.goto" href="#bathysphere.datatypes.Frame.goto">goto</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.gps" href="#bathysphere.datatypes.Frame.gps">gps</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.line" href="#bathysphere.datatypes.Frame.line">line</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.parse_buffer_queue" href="#bathysphere.datatypes.Frame.parse_buffer_queue">parse_buffer_queue</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.parse_xml" href="#bathysphere.datatypes.Frame.parse_xml">parse_xml</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.parse_xml_frames" href="#bathysphere.datatypes.Frame.parse_xml_frames">parse_xml_frames</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.seafet" href="#bathysphere.datatypes.Frame.seafet">seafet</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.storx" href="#bathysphere.datatypes.Frame.storx">storx</a></code></li>
<li><code><a title="bathysphere.datatypes.Frame.wqm" href="#bathysphere.datatypes.Frame.wqm">wqm</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Graph" href="#bathysphere.datatypes.Graph">Graph</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Graph.create" href="#bathysphere.datatypes.Graph.create">create</a></code></li>
<li><code><a title="bathysphere.datatypes.Graph.register" href="#bathysphere.datatypes.Graph.register">register</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Interval" href="#bathysphere.datatypes.Interval">Interval</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Interval.overlaps" href="#bathysphere.datatypes.Interval.overlaps">overlaps</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.JSONIOWrapper" href="#bathysphere.datatypes.JSONIOWrapper">JSONIOWrapper</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.dump" href="#bathysphere.datatypes.JSONIOWrapper.dump">dump</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.log" href="#bathysphere.datatypes.JSONIOWrapper.log">log</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.receive" href="#bathysphere.datatypes.JSONIOWrapper.receive">receive</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.send" href="#bathysphere.datatypes.JSONIOWrapper.send">send</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.KernelDensityEstimator" href="#bathysphere.datatypes.KernelDensityEstimator">KernelDensityEstimator</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh" href="#bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh">get_epsilon_from_mesh</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.glm" href="#bathysphere.datatypes.KernelDensityEstimator.glm">glm</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.intensity" href="#bathysphere.datatypes.KernelDensityEstimator.intensity">intensity</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.predict" href="#bathysphere.datatypes.KernelDensityEstimator.predict">predict</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.train" href="#bathysphere.datatypes.KernelDensityEstimator.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.LinkedList" href="#bathysphere.datatypes.LinkedList">LinkedList</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.LinkedList.add" href="#bathysphere.datatypes.LinkedList.add">add</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.append" href="#bathysphere.datatypes.LinkedList.append">append</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.deduplicate" href="#bathysphere.datatypes.LinkedList.deduplicate">deduplicate</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.k_from_end" href="#bathysphere.datatypes.LinkedList.k_from_end">k_from_end</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.k_from_head" href="#bathysphere.datatypes.LinkedList.k_from_head">k_from_head</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.prepend" href="#bathysphere.datatypes.LinkedList.prepend">prepend</a></code></li>
<li><code><a title="bathysphere.datatypes.LinkedList.traverse" href="#bathysphere.datatypes.LinkedList.traverse">traverse</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.LinkedListNode" href="#bathysphere.datatypes.LinkedListNode">LinkedListNode</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Memory" href="#bathysphere.datatypes.Memory">Memory</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.Memory.alloc" href="#bathysphere.datatypes.Memory.alloc">alloc</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.cache" href="#bathysphere.datatypes.Memory.cache">cache</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.data" href="#bathysphere.datatypes.Memory.data">data</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.free" href="#bathysphere.datatypes.Memory.free">free</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.parts" href="#bathysphere.datatypes.Memory.parts">parts</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.restore" href="#bathysphere.datatypes.Memory.restore">restore</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.set" href="#bathysphere.datatypes.Memory.set">set</a></code></li>
<li><code><a title="bathysphere.datatypes.Memory.vertex_array_buffer" href="#bathysphere.datatypes.Memory.vertex_array_buffer">vertex_array_buffer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Mesh" href="#bathysphere.datatypes.Mesh">Mesh</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Mesh.adjacency" href="#bathysphere.datatypes.Mesh.adjacency">adjacency</a></code></li>
<li><code><a title="bathysphere.datatypes.Mesh.cell_normals" href="#bathysphere.datatypes.Mesh.cell_normals">cell_normals</a></code></li>
<li><code><a title="bathysphere.datatypes.Mesh.vertex_normals" href="#bathysphere.datatypes.Mesh.vertex_normals">vertex_normals</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.ObjectStorage" href="#bathysphere.datatypes.ObjectStorage">ObjectStorage</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.ObjectStorage.delete" href="#bathysphere.datatypes.ObjectStorage.delete">delete</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.get_object" href="#bathysphere.datatypes.ObjectStorage.get_object">get_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.list_objects" href="#bathysphere.datatypes.ObjectStorage.list_objects">list_objects</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.locked" href="#bathysphere.datatypes.ObjectStorage.locked">locked</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.metadata_template" href="#bathysphere.datatypes.ObjectStorage.metadata_template">metadata_template</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.publish_events" href="#bathysphere.datatypes.ObjectStorage.publish_events">publish_events</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.put_object" href="#bathysphere.datatypes.ObjectStorage.put_object">put_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.session" href="#bathysphere.datatypes.ObjectStorage.session">session</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.stat_object" href="#bathysphere.datatypes.ObjectStorage.stat_object">stat_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.unlock" href="#bathysphere.datatypes.ObjectStorage.unlock">unlock</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.updateIndex" href="#bathysphere.datatypes.ObjectStorage.updateIndex">updateIndex</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.PostgresType" href="#bathysphere.datatypes.PostgresType">PostgresType</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.PostgresType.Geography" href="#bathysphere.datatypes.PostgresType.Geography">Geography</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.IntIdentity" href="#bathysphere.datatypes.PostgresType.IntIdentity">IntIdentity</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.NullString" href="#bathysphere.datatypes.PostgresType.NullString">NullString</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.Numerical" href="#bathysphere.datatypes.PostgresType.Numerical">Numerical</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.TimeStamp" href="#bathysphere.datatypes.PostgresType.TimeStamp">TimeStamp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Reactor" href="#bathysphere.datatypes.Reactor">Reactor</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Reactor.integrate" href="#bathysphere.datatypes.Reactor.integrate">integrate</a></code></li>
<li><code><a title="bathysphere.datatypes.Reactor.update" href="#bathysphere.datatypes.Reactor.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork" href="#bathysphere.datatypes.RecurrentNeuralNetwork">RecurrentNeuralNetwork</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.create" href="#bathysphere.datatypes.RecurrentNeuralNetwork.create">create</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.createAndCache" href="#bathysphere.datatypes.RecurrentNeuralNetwork.createAndCache">createAndCache</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.feed" href="#bathysphere.datatypes.RecurrentNeuralNetwork.feed">feed</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.from_cache" href="#bathysphere.datatypes.RecurrentNeuralNetwork.from_cache">from_cache</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.get_prediction" href="#bathysphere.datatypes.RecurrentNeuralNetwork.get_prediction">get_prediction</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.nodes" href="#bathysphere.datatypes.RecurrentNeuralNetwork.nodes">nodes</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.optimizer" href="#bathysphere.datatypes.RecurrentNeuralNetwork.optimizer">optimizer</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.predict" href="#bathysphere.datatypes.RecurrentNeuralNetwork.predict">predict</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.predictor" href="#bathysphere.datatypes.RecurrentNeuralNetwork.predictor">predictor</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.run" href="#bathysphere.datatypes.RecurrentNeuralNetwork.run">run</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.series" href="#bathysphere.datatypes.RecurrentNeuralNetwork.series">series</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.shape" href="#bathysphere.datatypes.RecurrentNeuralNetwork.shape">shape</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.test" href="#bathysphere.datatypes.RecurrentNeuralNetwork.test">test</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.train" href="#bathysphere.datatypes.RecurrentNeuralNetwork.train">train</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.train_cached_model" href="#bathysphere.datatypes.RecurrentNeuralNetwork.train_cached_model">train_cached_model</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.x" href="#bathysphere.datatypes.RecurrentNeuralNetwork.x">x</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.x_train" href="#bathysphere.datatypes.RecurrentNeuralNetwork.x_train">x_train</a></code></li>
<li><code><a title="bathysphere.datatypes.RecurrentNeuralNetwork.y" href="#bathysphere.datatypes.RecurrentNeuralNetwork.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.RelationshipLabels" href="#bathysphere.datatypes.RelationshipLabels">RelationshipLabels</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.RelationshipLabels.Collection" href="#bathysphere.datatypes.RelationshipLabels.Collection">Collection</a></code></li>
<li><code><a title="bathysphere.datatypes.RelationshipLabels.Derived" href="#bathysphere.datatypes.RelationshipLabels.Derived">Derived</a></code></li>
<li><code><a title="bathysphere.datatypes.RelationshipLabels.Parent" href="#bathysphere.datatypes.RelationshipLabels.Parent">Parent</a></code></li>
<li><code><a title="bathysphere.datatypes.RelationshipLabels.Root" href="#bathysphere.datatypes.RelationshipLabels.Root">Root</a></code></li>
<li><code><a title="bathysphere.datatypes.RelationshipLabels.Self" href="#bathysphere.datatypes.RelationshipLabels.Self">Self</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Schema" href="#bathysphere.datatypes.Schema">Schema</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.State" href="#bathysphere.datatypes.State">State</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.State.increment" href="#bathysphere.datatypes.State.increment">increment</a></code></li>
<li><code><a title="bathysphere.datatypes.State.state3" href="#bathysphere.datatypes.State.state3">state3</a></code></li>
<li><code><a title="bathysphere.datatypes.State.state4" href="#bathysphere.datatypes.State.state4">state4</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Table" href="#bathysphere.datatypes.Table">Table</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Table.declare" href="#bathysphere.datatypes.Table.declare">declare</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.drop" href="#bathysphere.datatypes.Table.drop">drop</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.insert" href="#bathysphere.datatypes.Table.insert">insert</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.select" href="#bathysphere.datatypes.Table.select">select</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.TimeStamp" href="#bathysphere.datatypes.TimeStamp">TimeStamp</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.TimeStamp.parseBinary" href="#bathysphere.datatypes.TimeStamp.parseBinary">parseBinary</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Topology" href="#bathysphere.datatypes.Topology">Topology</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Topology.adjacency" href="#bathysphere.datatypes.Topology.adjacency">adjacency</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.cell_adjacency" href="#bathysphere.datatypes.Topology.cell_adjacency">cell_adjacency</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.deduplicate" href="#bathysphere.datatypes.Topology.deduplicate">deduplicate</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.read" href="#bathysphere.datatypes.Topology.read">read</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Trie" href="#bathysphere.datatypes.Trie">Trie</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Trie.insert" href="#bathysphere.datatypes.Trie.insert">insert</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.levenshteinDistance" href="#bathysphere.datatypes.Trie.levenshteinDistance">levenshteinDistance</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.search" href="#bathysphere.datatypes.Trie.search">search</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.searchRecursive" href="#bathysphere.datatypes.Trie.searchRecursive">searchRecursive</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Unit" href="#bathysphere.datatypes.Unit">Unit</a></code></h4>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.VertexArray" href="#bathysphere.datatypes.VertexArray">VertexArray</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.VertexArray.deduplicate" href="#bathysphere.datatypes.VertexArray.deduplicate">deduplicate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.View" href="#bathysphere.datatypes.View">View</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.View.count" href="#bathysphere.datatypes.View.count">count</a></code></li>
<li><code><a title="bathysphere.datatypes.View.format" href="#bathysphere.datatypes.View.format">format</a></code></li>
<li><code><a title="bathysphere.datatypes.View.format_axis" href="#bathysphere.datatypes.View.format_axis">format_axis</a></code></li>
<li><code><a title="bathysphere.datatypes.View.legend" href="#bathysphere.datatypes.View.legend">legend</a></code></li>
<li><code><a title="bathysphere.datatypes.View.pre_push" href="#bathysphere.datatypes.View.pre_push">pre_push</a></code></li>
<li><code><a title="bathysphere.datatypes.View.push" href="#bathysphere.datatypes.View.push">push</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Wind" href="#bathysphere.datatypes.Wind">Wind</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Wind.dynamicMixing" href="#bathysphere.datatypes.Wind.dynamicMixing">dynamicMixing</a></code></li>
<li><code><a title="bathysphere.datatypes.Wind.shear" href="#bathysphere.datatypes.Wind.shear">shear</a></code></li>
<li><code><a title="bathysphere.datatypes.Wind.simpleMixing" href="#bathysphere.datatypes.Wind.simpleMixing">simpleMixing</a></code></li>
<li><code><a title="bathysphere.datatypes.Wind.update" href="#bathysphere.datatypes.Wind.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>