<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>bathysphere.datatypes API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>bathysphere.datatypes</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=line-too-long,invalid-name
from __future__ import annotations
from enum import Enum
from typing import Callable, Any
from datetime import datetime, date, timedelta
from json import dumps, loads, decoder
from collections import deque
from uuid import uuid4
from os import getpid
from os.path import isfile
from io import BytesIO, TextIOWrapper
from difflib import SequenceMatcher
from functools import reduce
from ftplib import FTP
from pickle import loads as unpickle
from re import sub
from itertools import repeat, chain
from multiprocessing import Pool
from warnings import simplefilter

from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.engine.url import URL

from bidict import bidict
import attr
from minio import Minio
from minio.error import NoSuchKey
from requests import get, post
from flask import Response, Request, request
from redis import StrictRedis

from numpy import (
    array,
    append,
    argmax,
    argmin,
    random,
    where,
    isnan,
    cross,
    argwhere,
    arange,
    array,
    hstack,
    vstack,
    repeat,
    zeros,
    unique,
)
from numpy.linalg import norm
from netCDF4 import Dataset as _Dataset # pylint: disable=no-name-in-module
from pandas import read_html, read_csv, Series

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KernelDensity
from pyproj import transform
from matplotlib import rc
from matplotlib.pyplot import subplots, subplots_adjust

# Use ArrayFire for multiple GPU bindings if available, else use ndarray as stand-in
try:
    import arrayfire as af
except ImportError:
    af = None

if af:
    _Array = af.Array or array
else:
    _Array = array

from bathysphere.utils import (
    join,
    parsePostgresValueIn,
    _parse_str_to_float,
    resolveTaskTree,
    synchronous,
    normal,
    Path
)


ExtentType = (float, float, float, float)
ResponseJSON = (dict, int)
ResponseOctet = (dict, int)


@attr.s
class Array:
    &#34;&#34;&#34;
    Encapsulates ND Array IO and operations, using either
    numpy or arrayfire as a backend. 
    &#34;&#34;&#34;
    data: array = attr.ib(default=None)
    gpu: bool = attr.ib(default=False)

    @property
    def interval(self) -&gt; Interval:
        &#34;&#34;&#34;
        Get range of an array, which may be in GPU memory
        &#34;&#34;&#34;
        if self.gpu:
            tex = af.np_to_af_array(self.data)
            mn = af.min(tex)
            mx = af.max(tex)
        else:
            mn = min(self.data)
            mx = max(self.data)
        return mn, mx

    @property
    def range(self) -&gt; float:
        &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
        return self.data.max() - self.data.min()


    @property
    def normalized(self) -&gt; Array:
        &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
        return (self.data - self.data.min()) / self.range


@attr.s
class Bound:
    &#34;&#34;&#34;
    A bound is on an interval, may ne upper or lower, closed or open
    &#34;&#34;&#34;
    value: Any = attr.ib()
    closed: bool = attr.ib(default=False)


@attr.s
class CloudSQL:
    &#34;&#34;&#34;
    This class encapsulates a connection pool to a cloud based PostgreSQL provider.
    By default it expects a Google CloudSQL database. 
    &#34;&#34;&#34;

    auth: (str, str) = attr.ib()
    instance: str = attr.ib()
    port: int = attr.ib(default=5432)
    pool_size: int = attr.ib(default=4)
    max_overflow: int = attr.ib(default=2)
    pool_timeout: int = attr.ib(default=5)
    pool_recycle: int = attr.ib(default=1800)

    @property
    def engine(self) -&gt; Engine:
        &#34;&#34;&#34;
        The engine property will be used only once per request, so
        can safely be generated as a property. 
        &#34;&#34;&#34;
        user, password = self.auth
        return create_engine(
            URL(
                drivername=&#34;postgres+pg8000&#34;,
                username=user,
                password=password,
                database=&#34;postgres&#34;,
                query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
            ),
            pool_size=self.pool_size,
            max_overflow=self.max_overflow,
            pool_timeout=self.pool_timeout,
            pool_recycle=self.pool_recycle,
        )

    def query(self, table, **kwargs) -&gt; [dict]:
        &#34;&#34;&#34;
        Execute an arbitrary query.
        &#34;&#34;&#34;
        with self.engine.connect() as cursor:
            query: Query = table.select(**kwargs)
            return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]

    def handle(self, request: Request) -&gt; ResponseJSON:
        &#34;&#34;&#34;
        Do some postgres stuff
        &#34;&#34;&#34;
        # pylint: disable=broad-except
        conf = request.body[&#34;table&#34;]
        fields = [
            Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
        ]
        table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

        try:
            records = self.query(table=table)
        except Exception as ex:  
            return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

        try:
            return (
                dumps(
                    {
                        &#34;count&#34;: len(records),
                        &#34;data&#34;: records,
                        &#34;method&#34;: str(request.method),
                        &#34;query_string&#34;: str(request.query_string),
                    }
                ),
                200,
            )
        except Exception as ex:
            return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500


def ConvexHull(points):
    &#34;&#34;&#34;
    Convex hulls are used to speed up spatial relation queries
    &#34;&#34;&#34;
    def segment(u, v, indices, points):
        &#34;&#34;&#34;Bisect the points&#34;&#34;&#34;
        if indices.shape[0] == 0:
            return array([], dtype=int)

        def crossProduct(i, j):
            &#34;&#34;&#34;Calculate angles&#34;&#34;&#34;
            return cross(points[indices, :] - points[i, :], points[j, :] - points[i, :])

        w = indices[argmin(crossProduct(u, v))]
        a = indices[argwhere(crossProduct(w, v) &lt; 0).flatten()]
        b = indices[argwhere(crossProduct(u, w) &lt; 0).flatten()]

        return hstack((segment(w, v, a, points), w, segment(u, w, b, points)))

    u = argmin(points[:, 0])
    v = argmax(points[:, 0])
    indices = arange(0, points.shape[0])
    parted = cross(points[indices, :] - points[u, :], points[v, :] - points[u, :]) &lt; 0

    a = indices[argwhere(~parted)]
    b = indices[argwhere(parted)]

    return hstack((u, segment(v, u, a, points), v, segment(u, v, b, points), u))


class Dataset(_Dataset):
    &#34;&#34;&#34;
    Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
    or drop-outs.

    * Query: Get an array of a single variable
    * Cache: Save chunk in object storage or local filesystem
    &#34;&#34;&#34;

    def query(
        self,
        observed_property: str,
        samples: int = None,
        reduce_dim: bool = False,
        kind: str = &#34;float64&#34;,
    ) -&gt; array:
        &#34;&#34;&#34;
        Extract an observedProperty, and optionally extract pixel samples from it.
        :param observed_property: field to extract
        :param samples: buffer of pixel indices to sample
        :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
        :param kind: format for numerical data
        &#34;&#34;&#34;
        simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
        if samples:
            return array(
                self.variables[observed_property][0, i, j].astype(kind)
                for i, j in samples
            )
        return (
            self.variables[observed_property][:, 0].astype(kind)
            if reduce_dim
            else self.variables[observed_property][:].astype(kind)
        )

    def copy(self, path: str, observed_properties: (str) = None):
        &#34;&#34;&#34;
        Copy parts into a new file
        &#34;&#34;&#34;
        fid = _Dataset(path=path)
        if isfile(path=path) and not self.policy():
            return False
        for name, obj in self.dimensions.items():
            fid.createDimension(name, obj)
        for name, obj in self.variables.items():
            if observed_properties and str(name) not in observed_properties:
                continue  # not matching variables in source data
            fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
            fid.variables[name][:] = self.variables[name][:]
        fid.close()
        return fid

@attr.s
class Extent:
    &#34;&#34;&#34;Extents speed up relational queries&#34;&#34;&#34;
    value: ExtentType = attr.ib()

    def __call__(self):
        &#34;&#34;&#34;Unwrap the extent value when calling instance&#34;&#34;&#34;
        return self.value

    @property
    def vertex_array(self):
        &#34;&#34;&#34;
        Convert an Extent to a VertexArray
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])

    @property
    def path(self) -&gt; Path:
        &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
        ext = self.value
        xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
        return Path(xy)

    @property
    def intervals(self):
        &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
        return (
            Interval(Bound(self.value[0]), Bound(self.value[1])),
            Interval(Bound(self.value[2]), Bound(self.value[3]))
        )


    def __add__(self, other: Extent) -&gt; Extent:
        &#34;&#34;&#34;
        Reduce extents through addition
        &#34;&#34;&#34;
        dat = zip(self.value, other.value)
        return min(next(dat)), max(next(dat)), min(next(dat)), max(next(dat))


    def overlaps(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        def _mapped(item: (Extent, Extent)):
            a, b = item
            return a.overlaps(b)

        return all(map(_mapped, zip(self.intervals, other.intervals)))


    def __contains__(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly contains B
        &#34;&#34;&#34;
        a, b = self.intervals
        c, d = other.intervals

        return c in a and d in b


@attr.s
class Feature:
    &#34;&#34;&#34;
    Format as GeoJSON feature
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    geometry: [[float]] = attr.Factory(list)
    properties: dict = attr.Factory(dict)
   

@attr.s
class FeatureCollection:
    &#34;&#34;&#34;
    GeoJSON feature collection
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    features: [Feature] = attr.Factory(list)
    properties: dict = attr.Factory(dict)
        

@attr.s
class Field:
    &#34;&#34;&#34;Column for Postgres table&#34;&#34;&#34;

    name: Any = attr.ib()
    type: str = attr.ib()

    @staticmethod
    def autoCorrect(
        key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
    ) -&gt; str:
        &#34;&#34;&#34;
        Match fieldnames probabilistically
        &#34;&#34;&#34;
        fields = lookup.keys()
        seq = SequenceMatcher(isjunk=None, autojunk=False)

        def _score(x):
            seq.set_seqs(key.lower(), x.lower())
            return seq.ratio()

        def _reduce(a, b):
            return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

        return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))

    @staticmethod
    def restore(final, units):
        # type: ((str, ), (str, )) -&gt; (str,)
        &#34;&#34;&#34;
        Get the original header name back by reversing clean_fields() operation.
        &#34;&#34;&#34;
        names = map(
            lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
        )
        return tuple(
            map(
                lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
                zip(names, units),
            )
        )

    @staticmethod
    def clean(fields: (str,)) -&gt; ((str,), (str,)):
        &#34;&#34;&#34;
        Make friendly formats for object and table naming. The inverse is restore_fields().
        &#34;&#34;&#34;

        def _clean(x):
            return (
                x.strip()
                .replace(&#34; &#34;, &#34;_&#34;)
                .replace(&#34;%&#34;, &#34;_percent&#34;)
                .replace(&#34;+&#34;, &#34;_plus&#34;)
                .replace(&#34;-&#34;, &#34;_minus&#34;)
            )

        return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))


@attr.s
class File:
    &#34;&#34;&#34;
    Originally used for Satlantic files, repurposed as general file system object.

    Very similar to Assets.
    &#34;&#34;&#34;
    name: str = attr.ib(default=&#34;&#34;)
    sn: int = attr.ib(default=None)
    url: str = attr.ib(default=None)
    time: datetime = attr.ib(default=None)
    ts: datetime = attr.ib(default=attr.Factory(datetime.now))
    kb: float = attr.ib(default=0.0)
    encoding: str = attr.ib(default=None)
    content: Any = attr.ib(default=None)

    def __repr__(self):
        &#34;&#34;&#34;Print formatting&#34;&#34;&#34;
        return &#34;{} ({}): {}&#34;.format(self.__class__.__name__, self.encoding, self.name)

    def __cmp__(self, other):
        &#34;&#34;&#34;Compare wrapper&#34;&#34;&#34;
        if hasattr(other, &#34;sort_key&#34;):
            return self.sort_key().__cmp__(other.sort_key())

    def serialize(self):
        &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
        return {
            &#34;url&#34;: self.url,
            &#34;ts&#34;: self.ts,
            &#34;kb&#34;: self.kb,
            &#34;encoding&#34;: self.encoding,
            &#34;content&#34;: self.content,
        }

    def sort_key(self):
        &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
        return self.time


    @classmethod
    def metadata(cls, url: str, filename: str, ts: str, size: str):
        &#34;&#34;&#34;
        Create a file metadata object
        &#34;&#34;&#34;
        fields = filename.split(&#34;.&#34;)
        encoding = None
        if len(fields) &gt; 1:
            fmt = fields.pop()
            if &#34;sensors&#34; == fmt:
                encoding = FileType.Config
            elif &#34;xml&#34; == fmt:
                encoding = FileType.Schema
            elif &#34;txt&#34; == fmt:
                if fields[-1] == &#34;raw&#34;:
                    fields.pop()  # convention is to have &#34;.raw.txt&#34;
                encoding = FileType.Log

        time = None
        if len(fields) &gt; 1:  # dated files
            ft = fields.pop()
            try:
                dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
                time = datetime.strptime(ft, dt_fmt)
            except ValueError:
                pass

        try:
            sn = int(fields.pop())
        except ValueError:
            sn = None

        path = url + filename

        return cls(
            name=filename,
            sn=sn,  # maybe None
            url=path,  # retrieval path
            time=time,  # file time from name, maybe None
            ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
            kb=_parse_str_to_float(size),  # float kilobytes
            encoding=encoding,
        )

    def _match(self, fmt=None, identity=None):
        # type: (File, set, set) -&gt; bool
        &#34;&#34;&#34;Filter for file objects&#34;&#34;&#34;
        return (not identity or self.sn in identity) and (
            not fmt or self.encoding in fmt
        )

    @staticmethod
    async def metadata_promise(url, auth):
        # type: (str, str) -&gt; tuple
        &#34;&#34;&#34;
        Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
        &#34;&#34;&#34;
        response = get(url, auth=auth)
        if not response.ok:
            return response.content

        df = read_html(response.content, skiprows=3)[0]
        return tuple(
            File.metadata(url, *r)
            for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
        )


@attr.s
class FileSystem:
    &#34;&#34;&#34;
    File systems are made up of files!
    &#34;&#34;&#34;
    @attr.s
    class OverwritePolicy:
        &#34;&#34;&#34;
        Basic logical unit for allowing/preventing mutability
        &#34;&#34;&#34;
        policy: str = attr.ib(default=&#34;never&#34;)

        def __call__(self, *args, **kwargs):
            if self == &#34;always&#34;:
                return True
            if self == &#34;prompt&#34;:
                print(&#34;Cache already exists. Overwrite? [y/N]&#34;)
                return input() in (&#34;Y&#34;, &#34;y&#34;)
            return False

    policy = OverwritePolicy(policy=&#34;never&#34;)

    @staticmethod
    def load_year_cache(local, years):
        # type: (str, (int, )) -&gt; dict
        &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
        combined = dict()
        for year in years:
            fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
            new = unpickle(fid)
            for key in new.keys():
                try:
                    combined[key] = append(combined[key], new[key])
                except KeyError:
                    combined[key] = array([])
                    combined[key] = append(combined[key], new[key])
        return combined

    @staticmethod
    def indexFileMetadata(url, year, auth=None):
        # type: (str, int, (str,)) -&gt; deque
        &#34;&#34;&#34;
        Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
        that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
        for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
        cached at a leisurely interactive pace.
        &#34;&#34;&#34;
        collector = deque()
        for record in resolveTaskTree(
            FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
        ):
            path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
            collector.append(
                {
                    &#34;date&#34;: date(*record),
                    &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                    &#34;url&#34;: path,
                    &#34;files&#34;: File.metadata_promise(path, auth=auth),
                }
            )
        return collector

    @staticmethod
    def indexFromHtmlTable(
        uriPattern: str, 
        start: datetime = None, 
        end: datetime = None, fmt: 
        str = &#34;%Y%m%d%H%M%S&#34;
    ) -&gt; [[dict]]:
        &#34;&#34;&#34;
        Get the entries for all remote files on server in years of interest.

        :param host: hostname
        :param start: datetime object
        :param end: datetime object
        :param fmt: datetime str formatter
        :return:
        &#34;&#34;&#34;
        
        def fetch(year: int):
            nameFilter = lambda x: isinstance(x[1], str) and f&#34;{year}&#34; in x[1]
            table = array(read_html(uriPattern.format(year)).pop())
            filtered = array(list(filter(nameFilter, table))).T
            names = filtered[1, :]
            dates = array([datetime.strptime(name[:14], fmt) for name in names])
            timestamps = filtered[2, :]
            size = filtered[3,:]

            if year in (start.year, end.year):
                (indices,) = where((start &lt; dates) &amp; (end + timedelta(days=1) &gt; dates))
                iterator = zip(names[indices], dates[indices], timestamps[indices], size[indices])
            else:
                iterator = zip(names, dates, timestamps, size)
    
            return [File(name=name, time=date, ts=ts, kb=sz) for name, date, ts, sz in iterator]

        return list(map(fetch, range(start.year, end.year+1)))
        

    @staticmethod
    async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
        # type: (str, int, int, int, (str, )) -&gt; datetime or None
        &#34;&#34;&#34;
        Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

        Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
        into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
        using the `render()` method, until the specified depth is reached.
        &#34;&#34;&#34;

        def __parse(value):
            &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
            return value if type(value) == int else int(value[:-1])

        if count == depth:
            return enum, None

        try:
            formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
            insert = __parse(enum)
        except TypeError:
            return enum, None

        sublevel = formatter.format(url, insert)
        response = get(sublevel, auth=auth)
        if not response.ok:
            return enum, None

        collector = deque()
        for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
            collector.append(
                FileSystem.indexTaskTree(
                    url=sublevel,
                    enum=__parse(record),  # name
                    count=count + 1,
                    depth=depth,
                    auth=auth,
                )
            )

        return enum, collector

    @staticmethod
    def search(pattern, filesystem):
        # type: (str, dict) -&gt; None or str
        &#34;&#34;&#34;
        Recursively search a directory structure for a key.
        Call this on the result of `index`

        :param filesystem: paths
        :param pattern: search key
        :return:
        &#34;&#34;&#34;
        for key, level in filesystem.items():
            if key == pattern:
                return key
            try:
                result = FileSystem._search(pattern, level)
            except AttributeError:
                result = None
            if result:
                return f&#34;{key}/{result}&#34;
        return None

    @staticmethod
    def _search(
        queue: deque,
        pool: Pool,
        fmt: set = None,
        identity: set = None,
        ts: datetime = None
    ) -&gt; list or None:
        &#34;&#34;&#34;
        Get all XML and configuration files within a directory

        Find configurations from metadata by serial number and date.

        The files can be:
        - On a remote server
        - In the bathysphere_functions_cache
        - Supplied as a list of dictionaries
        &#34;&#34;&#34;
        iterators = []
        queue_size = len(queue)

        if identity:
            iterators.append(repeat(identity, queue_size))
        if fmt:
            iterators.append(repeat(fmt, queue_size))
        if ts:
            iterators.append(repeat(ts, queue_size))

        def _chrono(x: File, ts: datetime = None):
            &#34;&#34;&#34;Chronoloigcal sorting method&#34;&#34;&#34;
            return (
                (x.time is None if ts else x.time is not None),
                (ts - x.time if ts else x.time),
            )

        queue = sorted(queue, key=_chrono, reverse=(False if ts else True))
        if fmt or identity:
            matching = pool.starmap(FileSystem._match, zip(queue, *iterators))
            queue = deque(queue)
        else:
            return {}, queue

        collector = dict()
        for condition in matching:
            if not condition:
                queue.rotate(1)
                continue
            file = queue.popleft()
            if not collector.get(file.sn, None):
                collector[file.sn] = deque()
            if (
                not ts or len(collector[file.sn]) == 0
            ):  # limit to length 1 for getting most recent
                collector[file.sn].append(file)
                continue

            queue.append(file)  # put the file back if unused

        return collector, queue

   
    def get(
        self,
        observed_properties,
        path=None,
        transpose=True,
        dataset=None,
        kind=&#34;float64&#34;,
        date=None,
    ):
        # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
        &#34;&#34;&#34;
        Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
        by name, resulting in an array. For previously processed internal data, arrays are stored as
        binary data in either `.pkl` or `.bathysphere_functions_cache` files.

        :param observed_properties: lookup field names
        :param path: path to local files if loading
        :param transpose: transpose the array before saving, makes join later easier
        :param dataset: NetCDF reference as in-memory object
        :param kind: numerical format for arrays
        :param date: specific timestamp to sample
        &#34;&#34;&#34;
        result = dict()

        if isinstance(observed_properties, str):
            fields = keys = [observed_properties]
        elif isinstance(observed_properties, dict):
            keys = observed_properties.keys()
            fields = observed_properties.values()
        else:
            fields = keys = observed_properties
        iterator = zip(*(keys, fields))

        for key, rename in iterator:
            if path:
                try:
                    fid = open(key, &#34;rb&#34;)
                except FileNotFoundError:
                    continue
                data = FileSystem.load_year_cache(fid).transpose() if transpose else FileSystem.load_year_cache(fid)
                fid.close()

            elif dataset:
                data = dataset.variables[key][:].astype(kind)
                FileSystem.set(date, data, key)
            else:
                data = None

            result[rename] = data

        return result

    @staticmethod
    def syncFtp(ftp, remote, local, filesystem=None):
        # type: (FTP, str, str, dict) -&gt; int
        &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
        path = FileSystem.search(pattern=remote, filesystem=filesystem)
        with open(local, &#34;wb+&#34;) as fid:
            return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))

    @staticmethod
    def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
        # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
        &#34;&#34;&#34;
        Build directory structure recursively.

        :param ftp: persistent ftp connection
        :param node: node in current working directory
        :param depth: current depth, do not set
        :param limit: maximum depth,
        :param metadata: pass the object metadata down one level
        :param parent:
        :return:
        &#34;&#34;&#34;

        body = loads(req)
        host = body.get(&#34;host&#34;, None)
        root = body.get(&#34;root&#34;, None)
        ftp = FTP(host, timeout=4)
        assert &#34;230&#34; in ftp.login()  # attach if no open socket
        assert ftp.sock
        if root is not None:
            _ = ftp.cwd(root)

        def _map(rec):
            values = rec.split()
            key = values.pop().strip()
            return {key: values}

        if depth == 0 and parent is None:
            parent = None  # create Location

        if limit is None or depth &lt;= limit:
            try:
                _ = ftp.cwd(node)  # target is a file
            except:
                pass
            else:
                collection = None

                files = []
                ftp.retrlines(&#34;LIST&#34;, files.append)
                for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                    FileSystem.indexFtp(
                        ftp=ftp,
                        graph=graph,
                        node=k,
                        depth=depth + 1,
                        limit=limit,
                        metadata=v,
                        parent=collection,
                    )

                if node != &#34;.&#34;:
                    _ = ftp.cwd(&#34;..&#34;)


class FileType(Enum):
    &#34;&#34;&#34;Well known file types&#34;&#34;&#34;
    Schema = 1
    Config = 2
    Log = 3


@attr.s
class Interval:
    &#34;&#34;&#34;Intervals are convenience data structs for sorting and numerical queries&#34;&#34;&#34;
    lower: Bound = attr.ib(default=None)
    upper: Bound = attr.ib(default=None)


    def overlaps(self, other: Interval) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.upper.value and 
            self.upper.value &gt;= other.lower.value
        )


    def __contains__(self, other: Interval):
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.lower.value and 
            self.upper.value &gt;= other.upper.value
        )


class JSONIOWrapper(TextIOWrapper):
    &#34;&#34;&#34;
    Use JSON messages piped between between processes
    &#34;&#34;&#34;
    @staticmethod
    def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
        &#34;&#34;&#34;
        Log notifications.

        :param message: some event notification
        :param data: data that resulted in message
        :param log: log file or interface
        :param arrow: symbol indicating direction of flow

        :return:
        &#34;&#34;&#34;
        timestamp = datetime.now().isoformat(sep=&#34; &#34;)
        string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
        if log is not None:
            log.write((string + &#34;\n&#34;).encode())
            return None
        print(string)

    def receive(self, log: BytesIO) -&gt; dict:
        &#34;&#34;&#34;
        Receive serialized data from command line interface.
        &#34;&#34;&#34;
        json = self.readline()
        self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
        try:
            data = loads(json.rstrip())
        except decoder.JSONDecodeError as decode_error:
            self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
            message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
            return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

        return data

    def send(self, data: dict, log: BytesIO) -&gt; None:
        &#34;&#34;&#34;
        Write serialized data to interface.
        &#34;&#34;&#34;

        def _transform():
            safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
            return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

        json = _transform()
        self.log(message=&#34;Send&#34;, data=json, log=log)
        self.write(f&#34;{json}\n&#34;)

    def dump(self) -&gt; None:
        &#34;&#34;&#34;
        Propagates messages up through C#, subprocess, and control layers.
        &#34;&#34;&#34;
        response = self.readline()
        while response != &#34;&#34;:
            response = self.readline()
            print(response.rstrip())


@attr.s
class KernelDensityEstimator(KernelDensity):
    &#34;&#34;&#34;Predict events in space&#34;&#34;&#34;
    @staticmethod
    def glm():
        &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
        return LinearRegression()  

    @staticmethod
    def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
        &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
        epsilon = mesh.fields[key]
        field = mesh.nodes.xye(epsilon)
        target = mesh.interp2d(xx, yy, epsilon)  # location suitability

        return field, target

    def intensity(self, field: object):
        &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
        intensity = self.score_samples(field)  # create intensity field
        maximum = intensity.max()
        minimum = intensity.min()
        cost = (intensity - minimum) / (maximum - minimum)

        return intensity, cost

    @staticmethod
    def train(self, target: iter, field: object, xx: iter, yy: iter):
        &#34;&#34;&#34;
        Train kernel density estimator model using a quantized mesh

        :param mesh: Mesh object of the Interpolator super type
        :param key: Spatial field to train on
        :return:
        &#34;&#34;&#34;
        subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
        self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
        return self.intensity(field)

    @staticmethod
    def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
        &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

        xnew = []
        ynew = []

        def prohibit():
            &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
            xtemp = array(xin + xnew)
            ytemp = array(yin + ynew)
            dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
            nearest = dxy.min()
            return nearest &lt; 0.5 * bandwidth

        xmin, ymin = transform(view, native, extent[0], extent[1])
        xmax, ymax = transform(view, native, extent[2], extent[3])

        total = 0
        passes = 0
        while total &lt; count and passes &lt; count * 10:

            sample = kde.sample()
            xx = sample[0][0]
            yy = sample[0][1]

            if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

                if bandwidth is not None and prohibit():
                    xnew.append(xx)
                    ynew.append(yy)
                    total += 1

                else:
                    passes += 1


class ObjectStorage(Minio):
    &#34;&#34;&#34;
    S3 compatible object storage interface using Minio as the client. 
    &#34;&#34;&#34;
    def __init__(self, bucket_name: str, endpoint: str, prefix: str = None, **kwargs):
        self.bucket_name = bucket_name
        self.prefix = prefix
        self.endpoint = endpoint
        
        super().__init__(endpoint, **kwargs)
        if not self.bucket_exists(bucket_name):
            self.make_bucket(bucket_name)

    @property
    def locked(self) -&gt; bool:
        &#34;&#34;&#34;
        Object sub-tree is locked. Denoted by placing a `lock.json` object with the same
        prefix.
        &#34;&#34;&#34;
        return self.stat_object(&#34;lock.json&#34;) is not None

    def publish_events(self, pubsub_channel: str):
        &#34;&#34;&#34;
        Listener for bucket events which then sends confirmation to a redis message queue
        &#34;&#34;&#34;
        fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
        with StrictRedis() as queue:
            for event in self.listen_bucket_notification(
                self.bucket_name, &#34;&#34;, None, fcns
            ):
                queue.publish(pubsub_channel, str(event))

    def stat_object(self, object_name: str):
        &#34;&#34;&#34;
        Determine whether an object key exists
        &#34;&#34;&#34;
        try:
            return super().stat_object(self.bucket_name, object_name)
        except NoSuchKey:
            return None

    def list_objects(self, prefix: str = None):
        &#34;&#34;&#34;
        Return a list of objects in the bucket with the same optional prefix/
        &#34;&#34;&#34;
        return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))

    def put_object(
        self,
        object_name: str,
        data: dict or bytes,
        metadata: dict = None,
        codec: str = &#34;utf-8&#34;,
    ) -&gt; str:
        &#34;&#34;&#34;
        Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

        :param label: label for file
        :param data: data to serialize
        :param metadata: headers
        :param codec: how to encode strings
        &#34;&#34;&#34;
        if isinstance(data, dict):
            content_type = &#34;application/json&#34;
            buffer = bytes(dumps(data).encode(codec))
        elif isinstance(data, bytes):
            content_type = &#34;text/plain&#34;
            buffer = data
        else:
            raise TypeError

        accumulate = []
        given_parts = object_name.split(&#34;/&#34;)
        prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
        if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
            for pp in prefix_parts:
                if pp not in given_parts:
                    accumulate.append(pp)
            accumulate.extend(given_parts)
            object_name = &#34;/&#34;.join(accumulate)

        super().put_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            data=BytesIO(buffer),
            length=len(buffer),
            metadata=metadata,
            content_type=content_type,
        )

        return object_name

    def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
        &#34;&#34;&#34;
        Download the data, may be streaming if desired
        &#34;&#34;&#34;
        data = super().get_object(self.bucket_name, object_name)
        if stream:

            def generate():
                for d in data.stream(32 * 1024):
                    yield d

            result = generate
        else:
            result = data

        return Response(result, mimetype=&#34;application/octet-stream&#34;)

    def updateIndex(
        self,
        object_name: str,
        metadata: dict = None,
        entries: [dict] = None,
        props: dict = None,
    ):
        &#34;&#34;&#34;
        Update contents of index metadata
        &#34;&#34;&#34;

        if entries:
            self.put_object(
                object_name=object_name,
                data={
                    **loads(self.get_object(object_name=object_name).data),
                    **(entries or {}),
                    **(props or {}),
                },
                metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
            )
        else:
            self.copy_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                object_source=object_name,
                metadata=metadata,
            )

        return self

    def delete(
        self, 
        prefix: str, 
        batch: int = 10, 
        conditions: dict = None
    ) -&gt; (Any):
        &#34;&#34;&#34;
        Delete all objects within a subdirectory or abstract collection.

        The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
        default, and therefore needs to be iterated through before returning any errors. 

        :param prefic: file prefix/dataset
        :param batch:  number to delete at a time
        &#34;&#34;&#34;
        remove = ()
        errors = ()
        
        objects_iter = self.list_objects(prefix=prefix)
        stop = False
        while not stop:
            try:
                object_name = next(objects_iter).object_name
            except StopIteration:
                stop = True
            else:
                stat = self.stat_object(object_name)
                if isinstance(conditions, dict):
                    if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                        remove += (object_name,)
                else:
                    remove += (object_name,)

            if len(remove) &gt;= batch or stop:
                for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                    errors += (error,)
                return errors


    @staticmethod
    def metadata_template(
        file_type: str = None, parent: str = None, headers: dict = None
    ) -&gt; dict:

        accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

        return {
            &#34;x-amz-acl&#34;: accessControl,
            &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
            &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
            &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
            &#34;x-amz-meta-service-file-type&#34;: file_type,
            **(headers or {}),
        }

    def unlock(self, object_name: str, session: str = None,) -&gt; bool:
        &#34;&#34;&#34;
        Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
        &#34;&#34;&#34;
        try:
            self.remove_object(self.bucket_name, object_name)
        except NoSuchKey:
            return False
        return True

    def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
        &#34;&#34;&#34;
        Object storage locking decorator for functions.

        When used this implements a mutex lock on the object path,
        which will block competing operations until it is cleared.

        Locks will not block read operations except in special cases. 
        &#34;&#34;&#34;

        # index = load_json(self.get_object(object_name=index_file))

        headers = {}
        session_id = uuid4().hex
        name = &#34;bathysphere&#34;
        lock_file = f&#34;{name}/lock.json&#34;
        # index_file = f&#34;{name}/index.json&#34;

        def decorator(fcn):
            &#34;&#34;&#34;
            Methods applied to the wrapped function
            &#34;&#34;&#34;

            def wrapper(*args, **kwargs):
                &#34;&#34;&#34;
                Actual wrapper that calls the decorated function
                &#34;&#34;&#34;
                if self.stat_object(lock_file):
                    return &#34;Lock in place&#34;, 500
                try:
                    self.put_object(
                        object_name=lock_file,
                        data={&#34;session&#34;: session_id},
                        metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                    )
                except NoSuchKey:
                    return &#34;Could not lock repository&#34;, 500
                try:
                    result = fcn(*args, **kwargs)
                except Exception as ex:
                    result = f&#34;{ex}&#34;, 500
                finally:
                    if lock and not self.unlock(object_name=lock):
                        result = &#34;Failed to unlock&#34;, 500
                return result

            return wrapper

        return decorator


class PostgresType(Enum):
    Numerical = &#34;DOUBLE PRECISION NULL&#34;
    TimeStamp = &#34;TIMESTAMP NOT NULL&#34;
    Geography = &#34;GEOGRAPHY NOT NULL&#34;
    IntIdentity = &#34;INT PRIMARY KEY&#34;
    NullString = &#34;VARCHAR(100) NULL&#34;


@attr.s
class Query:

    sql: str = attr.ib()
    parser: Callable = attr.ib()


@attr.s
class Schema:
    fields: [Field] = attr.ib(default=attr.Factory(list))


@attr.s
class Table:

    name: str = attr.ib()
    schema: Schema = attr.ib(default=Schema())

    @staticmethod
    def _unwrap(x):
        &#34;&#34;&#34;
        Some queries return iterables that need to be unpacked
        &#34;&#34;&#34;
        return {&#34;record&#34;: x[0]}

    def declare(self) -&gt; Query:
        &#34;&#34;&#34;
        Generate a query to create a new table but do not execute
        &#34;&#34;&#34;
        fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
        queryString = f&#34;&#34;&#34;
        CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
        &#34;&#34;&#34;
        return Query(queryString, None)

    def insert(self, data: ()) -&gt; Query:
        &#34;&#34;&#34;
        Generate the query to insert new rows into database.
        &#34;&#34;&#34;
        _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
        columns, values = map(
            join, ((field.name for field in self.schema.fields), _parsedValues)
        )

        queryString = f&#34;&#34;&#34;
        INSERT INTO {self.name} ({columns}) VALUES {values};
        &#34;&#34;&#34;
        return Query(queryString, None)

    def select(
        self,
        order_by: str = None,
        limit: int = 100,
        fields: (str) = (&#34;*&#34;,),
        order: str = &#34;DESC&#34;,
        conditions: ((str)) = (),
    ) -&gt; Query:
        &#34;&#34;&#34;
        Read back values/rows.
        &#34;&#34;&#34;
        _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
        _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

        queryString = f&#34;&#34;&#34;
        SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
        &#34;&#34;&#34;

        return Query(queryString, Table._unwrap)

    def drop(self) -&gt; Query:
        &#34;&#34;&#34;
        Drop the entire table
        &#34;&#34;&#34;
        return Query(f&#34;DROP TABLE {self.name};&#34;, None)


@attr.s
class TimeStamp:
    @staticmethod
    def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
        &#34;&#34;&#34;
        Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
        &#34;&#34;&#34;
        assert len(buffer) == 7
        yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
            int.from_bytes(buffer[:3], byteorder=byteorder),
            int.from_bytes(buffer[3:], byteorder=byteorder),
        )
        return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)


@attr.s
class Topology:

    cells: array = attr.ib(default=None)

    def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
        &#34;&#34;&#34;
        Get element neighbors
        &#34;&#34;&#34;
        queue = dict()
        while indices:
            cell = indices.pop()
            nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
            buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
            key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
            queue[key][cell] = buffer

        return queue


    @staticmethod
    def read(path: str, indexed: bool = True) -&gt; dict:
        &#34;&#34;&#34;
        Read in grid topology of unstructured triangular grid
        &#34;&#34;&#34;
        if path[-3:] == &#34;.nc&#34;:
            fid = Dataset(path)
            topo = fid.variables[&#34;nv&#34;][:].T
        else:
            fid = open(path, &#34;r&#34;)
            df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
            topo = df.__array__()

        n = len(topo)

        basis = 0
        enforce = 1
        minimum = topo.min()
        if (minimum != enforce) if enforce else True:
            topo -= minimum + basis  # zero-index
        
        return {
            &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
            &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
        }

    @property
    def adjacency(self):
        &#34;&#34;&#34;
        Get node parents and node neighbors from topology

        :param topology:
        :return:
        &#34;&#34;&#34;
        _parents = dict()
        _neighbors = dict()

        for element in range(len(self.cells)):
            vertices = self.cells[element]
            for node in vertices:
                try:
                    p = _parents[node]
                except KeyError:
                    p = _parents[node] = []
                p.append(element)  # add element to parents, no possible duplicates

                try:
                    n = _neighbors[node]
                except KeyError:
                    n = _neighbors[node] = []
                (mask,) = where(node != vertices)
                others = vertices[mask]

                for neighbor in others:
                    if neighbor not in n:
                        n.append(neighbor)  # add current element to parents

        solid = zeros(n, dtype=bool)
        for node in range(n):
            difference = _neighbors[node].__len__() - _parents[node].__len__()
            if difference == 1:
                solid[node] = True
            elif difference != 0:
                print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)


@attr.s
class Trie:
    &#34;&#34;&#34;
    A Tree-like data structure is used for string translation, auto-correct, and auto-complete
    functionality when interacting with the backend.

    This is an enhanced Trie, which has a network of nodes representing sequences of symbols.
    The implementation does not re-link paths, and is only branching. 
    &#34;&#34;&#34;

    word = attr.ib(default=None)
    weight = attr.ib(default=0)
    aliases = attr.ib(default=None)
    children = attr.ib(default=attr.Factory(dict))

    def insert(self, key: str, aliases: [str] = None) -&gt; None:
        &#34;&#34;&#34;
        Using the current node as the root, for each symbol in the word create or join 
        a child tree, and iteratively descend. Set the word of the final node to the 
        provided key.

        Optionally provide a list of aliases that can be returned.
        &#34;&#34;&#34;
        node = self
        for symbol in key:
            if symbol not in node.children:
                node.children[symbol] = Trie()
            node = node.children[symbol]
            node.weight += 1

        node.word = key
        node.aliases = aliases

    @staticmethod
    def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
        &#34;&#34;&#34;
        Descend through the tree, calculating the cost tables iteratively for subsets of the
        pattern
        &#34;&#34;&#34;
        _filter = lambda x: len(x)
        row = (previous[0] + 1,)
        for column in range(1, len(pattern) + 1):
            row += (
                min(
                    row[column - 1] + 1,
                    previous[column] + 1,
                    previous[column - 1] + int(pattern[column - 1] != symbol),
                ),
            )

        if min(row) &lt;= cost:
            filtered = tuple(chain(*filter(_filter,
                tuple(Trie.searchRecursive(v, k, pattern, row, cost) for k, v in node.children.items()),
            ))) 
        else:
            filtered = ()

        return (((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()) + filtered

    @staticmethod
    def levenshteinDistance(word1: str, word2: str) -&gt; int:
        &#34;&#34;&#34;
        Calculate the number of mutations needed to transform one sequence into
        a second sequention. This distance function is used to compare words for
        auto-correct functionality.
        &#34;&#34;&#34;
        columns = len(word1) + 1
        rows = len(word2) + 1

        # build first row
        currentRow = [0]
        for column in range(1, columns):
            currentRow.append(currentRow[column - 1] + 1)

        for row in range(1, rows):
            previousRow = currentRow
            currentRow = [previousRow[0] + 1]

            for column in range(1, columns):

                insertCost = currentRow[column - 1] + 1
                deleteCost = previousRow[column] + 1

                if word1[column - 1] != word2[row - 1]:
                    replaceCost = previousRow[column - 1] + 1
                else:
                    replaceCost = previousRow[column - 1]

                currentRow.append(min(insertCost, deleteCost, replaceCost))

        return currentRow[-1]

    @staticmethod
    def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
        &#34;&#34;&#34;
        Use simple memory-efficient search if the structure is trivial. Try `searchRecursive`
        for faster/larger searches on large structures.
        &#34;&#34;&#34;
        _results = ()
        for word in words:
            cost = Trie.levenshteinDistance(pattern, word)
            if cost &lt;= maxCost:
                _results += ((word, cost),)
        return _results


class View:
    count = 0

    def __init__(self, style, extent=None):
        # type: (dict, (float,)) -&gt; View
        &#34;&#34;&#34;
        Setup and return figure and axis instances
        &#34;&#34;&#34;
        rc(&#34;text&#34;, usetex=False)
        # rc(&#34;font&#34;, **{&#34;family&#34;: &#34;sans-serif&#34;, &#34;sans-serif&#34;: [&#34;Arial&#34;]})
        rc(&#34;mathtext&#34;, default=&#34;sf&#34;)
        rc(&#34;lines&#34;, markeredgewidth=1, linewidth=style[&#34;line&#34;])
        rc(&#34;axes&#34;, labelsize=style[&#34;text&#34;], linewidth=(style[&#34;line&#34;] + 1) // 2)
        rc(&#34;xtick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;ytick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;xtick.major&#34;, pad=5)
        rc(&#34;ytick.major&#34;, pad=5)

        self.style = style
        self.extent = extent
        self.fig, self.ax = subplots(
            facecolor=style[&#34;bg&#34;], figsize=(style[&#34;width&#34;], style[&#34;height&#34;])
        )
        padding = style[&#34;padding&#34;]
        subplots_adjust(
            left=padding[0], bottom=padding[1], right=1 - padding[2], top=1 - padding[3]
        )

    def format(self, bg: str, contrast: str, **kwargs):
        &#34;&#34;&#34;
        Setup color styles for figure
        &#34;&#34;&#34;
        self.ax.patch.set_facecolor(bg)  # background colors
        self.ax.edgecolor = contrast  # plotting area border
        self.format_axis(&#34;x&#34;, contrast, **kwargs)
        self.format_axis(&#34;y&#34;, contrast, **kwargs)

    def format_axis(
        self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
    ):
        if axis in (&#34;x&#34;, &#34;X&#34;):
            apply = self.ax.xaxis
            spines = (&#34;left&#34;, &#34;right&#34;)
        elif axis in (&#34;y&#34;, &#34;Y&#34;):
            apply = self.ax.yaxis
            spines = (&#34;top&#34;, &#34;bottom&#34;)
        else:
            raise ValueError

        apply.label.set_color(label)
        self.ax.tick_params(axis=axis.lower(), colors=label)
        for each in spines:
            self.ax.spines[each].set_color(contrast)
        apply.grid(grid)

    def pre_push(self):
        self.fig.canvas.draw()
        self.format(**self.style)
        self.ax.set_frame_on(True)

    def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
        # type: (str, bool, dict) -&gt; BytesIO
        buffer = BytesIO()
        self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
        buffer.seek(0)
        return buffer

    def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
        &#34;&#34;&#34;
        Format figure legend

        Kwargs:
            loc, str -- location on plotting area
            fc, str/arr -- string or RGBA color for face
            ec, str/arr -- string or RGBA color for edges

        Returns: matplotlib legend object
        &#34;&#34;&#34;
        legend = self.ax.legend(loc=loc)
        frame = legend.get_frame()
        frame.set_facecolor(fc)
        frame.set_edgecolor(ec)

        for text in legend.get_texts():
            text.set_color(self.style[&#34;contrast&#34;])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="bathysphere.datatypes.ConvexHull"><code class="name flex">
<span>def <span class="ident">ConvexHull</span></span>(<span>points)</span>
</code></dt>
<dd>
<div class="desc"><p>Convex hulls are used to speed up spatial relation queries</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ConvexHull(points):
    &#34;&#34;&#34;
    Convex hulls are used to speed up spatial relation queries
    &#34;&#34;&#34;
    def segment(u, v, indices, points):
        &#34;&#34;&#34;Bisect the points&#34;&#34;&#34;
        if indices.shape[0] == 0:
            return array([], dtype=int)

        def crossProduct(i, j):
            &#34;&#34;&#34;Calculate angles&#34;&#34;&#34;
            return cross(points[indices, :] - points[i, :], points[j, :] - points[i, :])

        w = indices[argmin(crossProduct(u, v))]
        a = indices[argwhere(crossProduct(w, v) &lt; 0).flatten()]
        b = indices[argwhere(crossProduct(u, w) &lt; 0).flatten()]

        return hstack((segment(w, v, a, points), w, segment(u, w, b, points)))

    u = argmin(points[:, 0])
    v = argmax(points[:, 0])
    indices = arange(0, points.shape[0])
    parted = cross(points[indices, :] - points[u, :], points[v, :] - points[u, :]) &lt; 0

    a = indices[argwhere(~parted)]
    b = indices[argwhere(parted)]

    return hstack((u, segment(v, u, a, points), v, segment(u, v, b, points), u))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="bathysphere.datatypes.Array"><code class="flex name class">
<span>class <span class="ident">Array</span></span>
<span>(</span><span>data: array = None, gpu: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Encapsulates ND Array IO and operations, using either
numpy or arrayfire as a backend. </p>
<p>Method generated by attrs for class Array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Array:
    &#34;&#34;&#34;
    Encapsulates ND Array IO and operations, using either
    numpy or arrayfire as a backend. 
    &#34;&#34;&#34;
    data: array = attr.ib(default=None)
    gpu: bool = attr.ib(default=False)

    @property
    def interval(self) -&gt; Interval:
        &#34;&#34;&#34;
        Get range of an array, which may be in GPU memory
        &#34;&#34;&#34;
        if self.gpu:
            tex = af.np_to_af_array(self.data)
            mn = af.min(tex)
            mx = af.max(tex)
        else:
            mn = min(self.data)
            mx = max(self.data)
        return mn, mx

    @property
    def range(self) -&gt; float:
        &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
        return self.data.max() - self.data.min()


    @property
    def normalized(self) -&gt; Array:
        &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
        return (self.data - self.data.min()) / self.range</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Array.data"><code class="name">var <span class="ident">data</span> : <built-in function array></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Array.gpu"><code class="name">var <span class="ident">gpu</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Array.interval"><code class="name">var <span class="ident">interval</span> : <a title="bathysphere.datatypes.Interval" href="#bathysphere.datatypes.Interval">Interval</a></code></dt>
<dd>
<div class="desc"><p>Get range of an array, which may be in GPU memory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def interval(self) -&gt; Interval:
    &#34;&#34;&#34;
    Get range of an array, which may be in GPU memory
    &#34;&#34;&#34;
    if self.gpu:
        tex = af.np_to_af_array(self.data)
        mn = af.min(tex)
        mx = af.max(tex)
    else:
        mn = min(self.data)
        mx = max(self.data)
    return mn, mx</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Array.normalized"><code class="name">var <span class="ident">normalized</span> : <a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></dt>
<dd>
<div class="desc"><p>Transform to (0,1) range</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def normalized(self) -&gt; Array:
    &#34;&#34;&#34;Transform to (0,1) range&#34;&#34;&#34;
    return (self.data - self.data.min()) / self.range</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Array.range"><code class="name">var <span class="ident">range</span> : float</code></dt>
<dd>
<div class="desc"><p>Calculate range of data, used in other properties and functions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def range(self) -&gt; float:
    &#34;&#34;&#34;Calculate range of data, used in other properties and functions&#34;&#34;&#34;
    return self.data.max() - self.data.min()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Bound"><code class="flex name class">
<span>class <span class="ident">Bound</span></span>
<span>(</span><span>value: Any, closed: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A bound is on an interval, may ne upper or lower, closed or open</p>
<p>Method generated by attrs for class Bound.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Bound:
    &#34;&#34;&#34;
    A bound is on an interval, may ne upper or lower, closed or open
    &#34;&#34;&#34;
    value: Any = attr.ib()
    closed: bool = attr.ib(default=False)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Bound.closed"><code class="name">var <span class="ident">closed</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Bound.value"><code class="name">var <span class="ident">value</span> : Any</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.CloudSQL"><code class="flex name class">
<span>class <span class="ident">CloudSQL</span></span>
<span>(</span><span>auth: (str, str), instance: str, port: int = 5432, pool_size: int = 4, max_overflow: int = 2, pool_timeout: int = 5, pool_recycle: int = 1800)</span>
</code></dt>
<dd>
<div class="desc"><p>This class encapsulates a connection pool to a cloud based PostgreSQL provider.
By default it expects a Google CloudSQL database. </p>
<p>Method generated by attrs for class CloudSQL.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CloudSQL:
    &#34;&#34;&#34;
    This class encapsulates a connection pool to a cloud based PostgreSQL provider.
    By default it expects a Google CloudSQL database. 
    &#34;&#34;&#34;

    auth: (str, str) = attr.ib()
    instance: str = attr.ib()
    port: int = attr.ib(default=5432)
    pool_size: int = attr.ib(default=4)
    max_overflow: int = attr.ib(default=2)
    pool_timeout: int = attr.ib(default=5)
    pool_recycle: int = attr.ib(default=1800)

    @property
    def engine(self) -&gt; Engine:
        &#34;&#34;&#34;
        The engine property will be used only once per request, so
        can safely be generated as a property. 
        &#34;&#34;&#34;
        user, password = self.auth
        return create_engine(
            URL(
                drivername=&#34;postgres+pg8000&#34;,
                username=user,
                password=password,
                database=&#34;postgres&#34;,
                query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
            ),
            pool_size=self.pool_size,
            max_overflow=self.max_overflow,
            pool_timeout=self.pool_timeout,
            pool_recycle=self.pool_recycle,
        )

    def query(self, table, **kwargs) -&gt; [dict]:
        &#34;&#34;&#34;
        Execute an arbitrary query.
        &#34;&#34;&#34;
        with self.engine.connect() as cursor:
            query: Query = table.select(**kwargs)
            return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]

    def handle(self, request: Request) -&gt; ResponseJSON:
        &#34;&#34;&#34;
        Do some postgres stuff
        &#34;&#34;&#34;
        # pylint: disable=broad-except
        conf = request.body[&#34;table&#34;]
        fields = [
            Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
        ]
        table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

        try:
            records = self.query(table=table)
        except Exception as ex:  
            return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

        try:
            return (
                dumps(
                    {
                        &#34;count&#34;: len(records),
                        &#34;data&#34;: records,
                        &#34;method&#34;: str(request.method),
                        &#34;query_string&#34;: str(request.query_string),
                    }
                ),
                200,
            )
        except Exception as ex:
            return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.CloudSQL.auth"><code class="name">var <span class="ident">auth</span> : (str, str)</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.instance"><code class="name">var <span class="ident">instance</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.max_overflow"><code class="name">var <span class="ident">max_overflow</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.pool_recycle"><code class="name">var <span class="ident">pool_recycle</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.pool_size"><code class="name">var <span class="ident">pool_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.pool_timeout"><code class="name">var <span class="ident">pool_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.port"><code class="name">var <span class="ident">port</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.CloudSQL.engine"><code class="name">var <span class="ident">engine</span> : sqlalchemy.engine.base.Engine</code></dt>
<dd>
<div class="desc"><p>The engine property will be used only once per request, so
can safely be generated as a property.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def engine(self) -&gt; Engine:
    &#34;&#34;&#34;
    The engine property will be used only once per request, so
    can safely be generated as a property. 
    &#34;&#34;&#34;
    user, password = self.auth
    return create_engine(
        URL(
            drivername=&#34;postgres+pg8000&#34;,
            username=user,
            password=password,
            database=&#34;postgres&#34;,
            query={&#34;unix_sock&#34;: f&#34;/cloudsql/{self.instance}/.s.PGSQL.{self.port}&#34;},
        ),
        pool_size=self.pool_size,
        max_overflow=self.max_overflow,
        pool_timeout=self.pool_timeout,
        pool_recycle=self.pool_recycle,
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.CloudSQL.handle"><code class="name flex">
<span>def <span class="ident">handle</span></span>(<span>self, request: Request) ‑> ResponseJSON</span>
</code></dt>
<dd>
<div class="desc"><p>Do some postgres stuff</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle(self, request: Request) -&gt; ResponseJSON:
    &#34;&#34;&#34;
    Do some postgres stuff
    &#34;&#34;&#34;
    # pylint: disable=broad-except
    conf = request.body[&#34;table&#34;]
    fields = [
        Field(f[&#34;name&#34;], f.get(&#34;type&#34;, None)) for f in conf[&#34;schema&#34;][&#34;fields&#34;]
    ]
    table = Table(name=conf[&#34;name&#34;], schema=Schema(fields=fields))

    try:
        records = self.query(table=table)
    except Exception as ex:  
        return dumps({&#34;Error&#34;: &#34;Problem executing query&#34;, &#34;detail&#34;: str(ex)}), 500

    try:
        return (
            dumps(
                {
                    &#34;count&#34;: len(records),
                    &#34;data&#34;: records,
                    &#34;method&#34;: str(request.method),
                    &#34;query_string&#34;: str(request.query_string),
                }
            ),
            200,
        )
    except Exception as ex:
        return dumps({&#34;Error&#34;: &#34;Could not serialize result of query&#34;}), 500</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.CloudSQL.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, table, **kwargs) ‑> [dict]</span>
</code></dt>
<dd>
<div class="desc"><p>Execute an arbitrary query.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(self, table, **kwargs) -&gt; [dict]:
    &#34;&#34;&#34;
    Execute an arbitrary query.
    &#34;&#34;&#34;
    with self.engine.connect() as cursor:
        query: Query = table.select(**kwargs)
        return [query.parser(row) for row in cursor.execute(query.sql).fetchall()]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
or drop-outs.</p>
<ul>
<li>Query: Get an array of a single variable</li>
<li>Cache: Save chunk in object storage or local filesystem</li>
</ul>
<p><strong><code>__init__(self, filename, mode="r", clobber=True, diskless=False,
persist=False, keepweakref=False, memory=None, encoding=None,
parallel=False, comm=None, info=None, format='NETCDF4')</code></strong></p>
<p><code>netCDF4.Dataset</code> constructor.</p>
<p><strong><code>filename</code></strong>: Name of netCDF file to hold dataset. Can also
be a python 3 pathlib instance or the URL of an OpenDAP dataset.
When memory is
set this is just used to set the <code>filepath()</code>.</p>
<p><strong><code>mode</code></strong>: access mode. <code>r</code> means read-only; no data can be
modified. <code>w</code> means write; a new file is created, an existing file with
the same name is deleted. <code>a</code> and <code>r+</code> mean append (in analogy with
serial files); an existing file is opened for reading and writing.
Appending <code>s</code> to modes <code>r</code>, <code>w</code>, <code>r+</code> or <code>a</code> will enable unbuffered shared
access to <code>NETCDF3_CLASSIC</code>, <code>NETCDF3_64BIT_OFFSET</code> or
<code>NETCDF3_64BIT_DATA</code> formatted files.
Unbuffered access may be useful even if you don't need shared
access, since it may be faster for programs that don't access data
sequentially. This option is ignored for <code>NETCDF4</code> and <code>NETCDF4_CLASSIC</code>
formatted files.</p>
<p><strong><code>clobber</code></strong>: if <code>True</code> (default), opening a file with <code>mode='w'</code>
will clobber an existing file with the same name.
if <code>False</code>, an
exception will be raised if a file with the same name already exists.</p>
<p><strong><code>format</code></strong>: underlying file format (one of <code>'NETCDF4',
'NETCDF4_CLASSIC', 'NETCDF3_CLASSIC'&lt;code&gt;, &lt;/code&gt;'NETCDF3_64BIT_OFFSET'</code> or
<code>'NETCDF3_64BIT_DATA'</code>.
Only relevant if <code>mode = 'w'</code> (if <code>mode = 'r','a'</code> or <code>'r+'</code> the file format
is automatically detected). Default <code>'NETCDF4'</code>, which means the data is
stored in an HDF5 file, using netCDF 4 API features.
Setting
<code>format='NETCDF4_CLASSIC'</code> will create an HDF5 file, using only netCDF 3
compatible API features. netCDF 3 clients must be recompiled and linked
against the netCDF 4 library to read files in <code>NETCDF4_CLASSIC</code> format.
<code>'NETCDF3_CLASSIC'</code> is the classic netCDF 3 file format that does not
handle 2+ Gb files. <code>'NETCDF3_64BIT_OFFSET'</code> is the 64-bit offset
version of the netCDF 3 file format, which fully supports 2+ GB files, but
is only compatible with clients linked against netCDF version 3.6.0 or
later. <code>'NETCDF3_64BIT_DATA'</code> is the 64-bit data version of the netCDF 3
file format, which supports 64-bit dimension sizes plus unsigned and
64 bit integer data types, but is only compatible with clients linked against
netCDF version 4.4.0 or later.</p>
<p><strong><code>diskless</code></strong>: If <code>True</code>, create diskless (in-core) file.
This is a feature added to the C library after the
netcdf-4.2 release. If you need to access the memory buffer directly,
use the in-memory feature instead (see <code>memory</code> kwarg).</p>
<p><strong><code>persist</code></strong>: if <code>diskless=True</code>, persist file to disk when closed
(default <code>False</code>).</p>
<p><strong><code>keepweakref</code></strong>: if <code>True</code>, child Dimension and Variable instances will keep weak
references to the parent Dataset or Group object.
Default is <code>False</code>, which
means strong references will be kept.
Having Dimension and Variable instances
keep a strong reference to the parent Dataset instance, which in turn keeps a
reference to child Dimension and Variable instances, creates circular references.
Circular references complicate garbage collection, which may mean increased
memory usage for programs that create may Dataset instances with lots of
Variables. It also will result in the Dataset object never being deleted, which
means it may keep open files alive as well. Setting <code>keepweakref=True</code> allows
Dataset instances to be garbage collected as soon as they go out of scope, potentially
reducing memory usage and open file handles.
However, in many cases this is not
desirable, since the associated Variable instances may still be needed, but are
rendered unusable when the parent Dataset instance is garbage collected.</p>
<p><strong><code>memory</code></strong>: if not <code>None</code>, create or open an in-memory Dataset.
If mode = 'r', the memory kwarg must contain a memory buffer object
(an object that supports the python buffer interface).
The Dataset will then be created with contents taken from this block of memory.
If mode = 'w', the memory kwarg should contain the anticipated size
of the Dataset in bytes (used only for NETCDF3 files).
A memory
buffer containing a copy of the Dataset is returned by the
<code>Dataset.close</code> method. Requires netcdf-c version 4.4.1 for mode='r,
netcdf-c 4.6.2 for mode='w'. To persist the file to disk, the raw
bytes from the returned buffer can be written into a binary file.
The Dataset can also be re-opened using this memory buffer.</p>
<p><strong><code>encoding</code></strong>: encoding used to encode filename string into bytes.
Default is None (<code>sys.getdefaultfileencoding()</code> is used).</p>
<p><strong><code>parallel</code></strong>: open for parallel access using MPI (requires mpi4py and
parallel-enabled netcdf-c and hdf5 libraries).
Default is <code>False</code>. If
<code>True</code>, <code>comm</code> and <code>info</code> kwargs may also be specified.</p>
<p><strong><code>comm</code></strong>: MPI_Comm object for parallel access. Default <code>None</code>, which
means MPI_COMM_WORLD will be used.
Ignored if <code>parallel=False</code>.</p>
<p><strong><code>info</code></strong>: MPI_Info object for parallel access. Default <code>None</code>, which
means MPI_INFO_NULL will be used.
Ignored if <code>parallel=False</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataset(_Dataset):
    &#34;&#34;&#34;
    Wrapper for NetCDF Dataset that does back-off in case of remote connection errors
    or drop-outs.

    * Query: Get an array of a single variable
    * Cache: Save chunk in object storage or local filesystem
    &#34;&#34;&#34;

    def query(
        self,
        observed_property: str,
        samples: int = None,
        reduce_dim: bool = False,
        kind: str = &#34;float64&#34;,
    ) -&gt; array:
        &#34;&#34;&#34;
        Extract an observedProperty, and optionally extract pixel samples from it.
        :param observed_property: field to extract
        :param samples: buffer of pixel indices to sample
        :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
        :param kind: format for numerical data
        &#34;&#34;&#34;
        simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
        if samples:
            return array(
                self.variables[observed_property][0, i, j].astype(kind)
                for i, j in samples
            )
        return (
            self.variables[observed_property][:, 0].astype(kind)
            if reduce_dim
            else self.variables[observed_property][:].astype(kind)
        )

    def copy(self, path: str, observed_properties: (str) = None):
        &#34;&#34;&#34;
        Copy parts into a new file
        &#34;&#34;&#34;
        fid = _Dataset(path=path)
        if isfile(path=path) and not self.policy():
            return False
        for name, obj in self.dimensions.items():
            fid.createDimension(name, obj)
        for name, obj in self.variables.items():
            if observed_properties and str(name) not in observed_properties:
                continue  # not matching variables in source data
            fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
            fid.variables[name][:] = self.variables[name][:]
        fid.close()
        return fid</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>netCDF4._netCDF4.Dataset</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Dataset.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, path: str, observed_properties: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Copy parts into a new file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, path: str, observed_properties: (str) = None):
    &#34;&#34;&#34;
    Copy parts into a new file
    &#34;&#34;&#34;
    fid = _Dataset(path=path)
    if isfile(path=path) and not self.policy():
        return False
    for name, obj in self.dimensions.items():
        fid.createDimension(name, obj)
    for name, obj in self.variables.items():
        if observed_properties and str(name) not in observed_properties:
            continue  # not matching variables in source data
        fid.createVariable(name, obj.datatype, obj.dimensions)  # add headers
        fid.variables[name][:] = self.variables[name][:]
    fid.close()
    return fid</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Dataset.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, observed_property: str, samples: int = None, reduce_dim: bool = False, kind: str = 'float64') ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Extract an observedProperty, and optionally extract pixel samples from it.
:param observed_property: field to extract
:param samples: buffer of pixel indices to sample
:param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
:param kind: format for numerical data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(
    self,
    observed_property: str,
    samples: int = None,
    reduce_dim: bool = False,
    kind: str = &#34;float64&#34;,
) -&gt; array:
    &#34;&#34;&#34;
    Extract an observedProperty, and optionally extract pixel samples from it.
    :param observed_property: field to extract
    :param samples: buffer of pixel indices to sample
    :param reduce_dim: if a single dim is stored as double dim, use this to avoid weirdness
    :param kind: format for numerical data
    &#34;&#34;&#34;
    simplefilter(&#34;ignore&#34;)  # ignore known NaN warning
    if samples:
        return array(
            self.variables[observed_property][0, i, j].astype(kind)
            for i, j in samples
        )
    return (
        self.variables[observed_property][:, 0].astype(kind)
        if reduce_dim
        else self.variables[observed_property][:].astype(kind)
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Extent"><code class="flex name class">
<span>class <span class="ident">Extent</span></span>
<span>(</span><span>value: ExtentType)</span>
</code></dt>
<dd>
<div class="desc"><p>Extents speed up relational queries</p>
<p>Method generated by attrs for class Extent.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Extent:
    &#34;&#34;&#34;Extents speed up relational queries&#34;&#34;&#34;
    value: ExtentType = attr.ib()

    def __call__(self):
        &#34;&#34;&#34;Unwrap the extent value when calling instance&#34;&#34;&#34;
        return self.value

    @property
    def vertex_array(self):
        &#34;&#34;&#34;
        Convert an Extent to a VertexArray
        &#34;&#34;&#34;
        e = self.value
        return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])

    @property
    def path(self) -&gt; Path:
        &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
        ext = self.value
        xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
        return Path(xy)

    @property
    def intervals(self):
        &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
        return (
            Interval(Bound(self.value[0]), Bound(self.value[1])),
            Interval(Bound(self.value[2]), Bound(self.value[3]))
        )


    def __add__(self, other: Extent) -&gt; Extent:
        &#34;&#34;&#34;
        Reduce extents through addition
        &#34;&#34;&#34;
        dat = zip(self.value, other.value)
        return min(next(dat)), max(next(dat)), min(next(dat)), max(next(dat))


    def overlaps(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        def _mapped(item: (Extent, Extent)):
            a, b = item
            return a.overlaps(b)

        return all(map(_mapped, zip(self.intervals, other.intervals)))


    def __contains__(self, other: Extent) -&gt; bool:
        &#34;&#34;&#34;
        A wholly contains B
        &#34;&#34;&#34;
        a, b = self.intervals
        c, d = other.intervals

        return c in a and d in b</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Extent.value"><code class="name">var <span class="ident">value</span> : ExtentType</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Extent.intervals"><code class="name">var <span class="ident">intervals</span></code></dt>
<dd>
<div class="desc"><p>Split extent into two intervals for easier parametric comparison</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def intervals(self):
    &#34;&#34;&#34;Split extent into two intervals for easier parametric comparison&#34;&#34;&#34;
    return (
        Interval(Bound(self.value[0]), Bound(self.value[1])),
        Interval(Bound(self.value[2]), Bound(self.value[3]))
    )</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Extent.path"><code class="name">var <span class="ident">path</span> : matplotlib.path.Path</code></dt>
<dd>
<div class="desc"><p>Get extent as a closed Path</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def path(self) -&gt; Path:
    &#34;&#34;&#34;Get extent as a closed Path&#34;&#34;&#34;
    ext = self.value
    xy = array([[ext[0], ext[2]], [ext[0], ext[3]], [ext[1], ext[3]], [ext[1], ext[2]]])
    return Path(xy)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Extent.vertex_array"><code class="name">var <span class="ident">vertex_array</span></code></dt>
<dd>
<div class="desc"><p>Convert an Extent to a VertexArray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def vertex_array(self):
    &#34;&#34;&#34;
    Convert an Extent to a VertexArray
    &#34;&#34;&#34;
    e = self.value
    return array([[e[0], e[2]], [e[1], e[2]], [e[1], e[3]], [e[0], e[3]]])</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Extent.overlaps"><code class="name flex">
<span>def <span class="ident">overlaps</span></span>(<span>self, other: <a title="bathysphere.datatypes.Extent" href="#bathysphere.datatypes.Extent">Extent</a>) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>A wholly or partially contains B</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overlaps(self, other: Extent) -&gt; bool:
    &#34;&#34;&#34;
    A wholly or partially contains B
    &#34;&#34;&#34;
    def _mapped(item: (Extent, Extent)):
        a, b = item
        return a.overlaps(b)

    return all(map(_mapped, zip(self.intervals, other.intervals)))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Feature"><code class="flex name class">
<span>class <span class="ident">Feature</span></span>
</code></dt>
<dd>
<div class="desc"><p>Format as GeoJSON feature</p>
<p>Method generated by attrs for class Feature.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Feature:
    &#34;&#34;&#34;
    Format as GeoJSON feature
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    geometry: [[float]] = attr.Factory(list)
    properties: dict = attr.Factory(dict)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Feature.geometry"><code class="name">var <span class="ident">geometry</span> : [[float]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Feature.properties"><code class="name">var <span class="ident">properties</span> : dict</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Feature.type"><code class="name">var <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection"><code class="flex name class">
<span>class <span class="ident">FeatureCollection</span></span>
</code></dt>
<dd>
<div class="desc"><p>GeoJSON feature collection</p>
<p>Method generated by attrs for class FeatureCollection.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureCollection:
    &#34;&#34;&#34;
    GeoJSON feature collection
    &#34;&#34;&#34;
    type: str = &#34;FeatureCollection&#34;
    features: [Feature] = attr.Factory(list)
    properties: dict = attr.Factory(dict)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FeatureCollection.features"><code class="name">var <span class="ident">features</span> : [<a title="bathysphere.datatypes.Feature" href="#bathysphere.datatypes.Feature">Feature</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection.properties"><code class="name">var <span class="ident">properties</span> : dict</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FeatureCollection.type"><code class="name">var <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Field"><code class="flex name class">
<span>class <span class="ident">Field</span></span>
<span>(</span><span>name: Any, type: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Column for Postgres table</p>
<p>Method generated by attrs for class Field.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Field:
    &#34;&#34;&#34;Column for Postgres table&#34;&#34;&#34;

    name: Any = attr.ib()
    type: str = attr.ib()

    @staticmethod
    def autoCorrect(
        key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
    ) -&gt; str:
        &#34;&#34;&#34;
        Match fieldnames probabilistically
        &#34;&#34;&#34;
        fields = lookup.keys()
        seq = SequenceMatcher(isjunk=None, autojunk=False)

        def _score(x):
            seq.set_seqs(key.lower(), x.lower())
            return seq.ratio()

        def _reduce(a, b):
            return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

        return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))

    @staticmethod
    def restore(final, units):
        # type: ((str, ), (str, )) -&gt; (str,)
        &#34;&#34;&#34;
        Get the original header name back by reversing clean_fields() operation.
        &#34;&#34;&#34;
        names = map(
            lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
        )
        return tuple(
            map(
                lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
                zip(names, units),
            )
        )

    @staticmethod
    def clean(fields: (str,)) -&gt; ((str,), (str,)):
        &#34;&#34;&#34;
        Make friendly formats for object and table naming. The inverse is restore_fields().
        &#34;&#34;&#34;

        def _clean(x):
            return (
                x.strip()
                .replace(&#34; &#34;, &#34;_&#34;)
                .replace(&#34;%&#34;, &#34;_percent&#34;)
                .replace(&#34;+&#34;, &#34;_plus&#34;)
                .replace(&#34;-&#34;, &#34;_minus&#34;)
            )

        return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Field.name"><code class="name">var <span class="ident">name</span> : Any</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Field.type"><code class="name">var <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Field.autoCorrect"><code class="name flex">
<span>def <span class="ident">autoCorrect</span></span>(<span>key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Match fieldnames probabilistically</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def autoCorrect(
    key: str, lookup: bidict, maximum: float = 0.0, threshold: float = 0.25
) -&gt; str:
    &#34;&#34;&#34;
    Match fieldnames probabilistically
    &#34;&#34;&#34;
    fields = lookup.keys()
    seq = SequenceMatcher(isjunk=None, autojunk=False)

    def _score(x):
        seq.set_seqs(key.lower(), x.lower())
        return seq.ratio()

    def _reduce(a, b):
        return b if (b[1] &gt; a[1]) and (b[1] &gt; threshold) else a

    return reduce(_reduce, zip(fields, map(_score, fields)), (key, maximum))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Field.clean"><code class="name flex">
<span>def <span class="ident">clean</span></span>(<span>fields: (str,)) ‑> ((str,), (str,))</span>
</code></dt>
<dd>
<div class="desc"><p>Make friendly formats for object and table naming. The inverse is restore_fields().</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clean(fields: (str,)) -&gt; ((str,), (str,)):
    &#34;&#34;&#34;
    Make friendly formats for object and table naming. The inverse is restore_fields().
    &#34;&#34;&#34;

    def _clean(x):
        return (
            x.strip()
            .replace(&#34; &#34;, &#34;_&#34;)
            .replace(&#34;%&#34;, &#34;_percent&#34;)
            .replace(&#34;+&#34;, &#34;_plus&#34;)
            .replace(&#34;-&#34;, &#34;_minus&#34;)
        )

    return tuple(*zip(map(lambda u, v: (_clean(u), _clean(v)), fields.split(&#34;[&#34;))))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Field.restore"><code class="name flex">
<span>def <span class="ident">restore</span></span>(<span>final, units)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the original header name back by reversing clean_fields() operation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def restore(final, units):
    # type: ((str, ), (str, )) -&gt; (str,)
    &#34;&#34;&#34;
    Get the original header name back by reversing clean_fields() operation.
    &#34;&#34;&#34;
    names = map(
        lambda n: n.replace(&#34;_plus&#34;, &#34;(0+)&#34;).replace(&#34;_minus&#34;, &#34;(0-)&#34;), final
    )
    return tuple(
        map(
            lambda f, u: f&#34;{f} [{u}]&#34;.replace(&#34;_&#34;, &#34; &#34;).replace(&#34;percent&#34;, &#34;%&#34;),
            zip(names, units),
        )
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.File"><code class="flex name class">
<span>class <span class="ident">File</span></span>
<span>(</span><span>name: str = '', sn: int = None, url: str = None, time: datetime = None, ts: datetime = NOTHING, kb: float = 0.0, encoding: str = None, content: Any = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Originally used for Satlantic files, repurposed as general file system object.</p>
<p>Very similar to Assets.</p>
<p>Method generated by attrs for class File.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class File:
    &#34;&#34;&#34;
    Originally used for Satlantic files, repurposed as general file system object.

    Very similar to Assets.
    &#34;&#34;&#34;
    name: str = attr.ib(default=&#34;&#34;)
    sn: int = attr.ib(default=None)
    url: str = attr.ib(default=None)
    time: datetime = attr.ib(default=None)
    ts: datetime = attr.ib(default=attr.Factory(datetime.now))
    kb: float = attr.ib(default=0.0)
    encoding: str = attr.ib(default=None)
    content: Any = attr.ib(default=None)

    def __repr__(self):
        &#34;&#34;&#34;Print formatting&#34;&#34;&#34;
        return &#34;{} ({}): {}&#34;.format(self.__class__.__name__, self.encoding, self.name)

    def __cmp__(self, other):
        &#34;&#34;&#34;Compare wrapper&#34;&#34;&#34;
        if hasattr(other, &#34;sort_key&#34;):
            return self.sort_key().__cmp__(other.sort_key())

    def serialize(self):
        &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
        return {
            &#34;url&#34;: self.url,
            &#34;ts&#34;: self.ts,
            &#34;kb&#34;: self.kb,
            &#34;encoding&#34;: self.encoding,
            &#34;content&#34;: self.content,
        }

    def sort_key(self):
        &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
        return self.time


    @classmethod
    def metadata(cls, url: str, filename: str, ts: str, size: str):
        &#34;&#34;&#34;
        Create a file metadata object
        &#34;&#34;&#34;
        fields = filename.split(&#34;.&#34;)
        encoding = None
        if len(fields) &gt; 1:
            fmt = fields.pop()
            if &#34;sensors&#34; == fmt:
                encoding = FileType.Config
            elif &#34;xml&#34; == fmt:
                encoding = FileType.Schema
            elif &#34;txt&#34; == fmt:
                if fields[-1] == &#34;raw&#34;:
                    fields.pop()  # convention is to have &#34;.raw.txt&#34;
                encoding = FileType.Log

        time = None
        if len(fields) &gt; 1:  # dated files
            ft = fields.pop()
            try:
                dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
                time = datetime.strptime(ft, dt_fmt)
            except ValueError:
                pass

        try:
            sn = int(fields.pop())
        except ValueError:
            sn = None

        path = url + filename

        return cls(
            name=filename,
            sn=sn,  # maybe None
            url=path,  # retrieval path
            time=time,  # file time from name, maybe None
            ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
            kb=_parse_str_to_float(size),  # float kilobytes
            encoding=encoding,
        )

    def _match(self, fmt=None, identity=None):
        # type: (File, set, set) -&gt; bool
        &#34;&#34;&#34;Filter for file objects&#34;&#34;&#34;
        return (not identity or self.sn in identity) and (
            not fmt or self.encoding in fmt
        )

    @staticmethod
    async def metadata_promise(url, auth):
        # type: (str, str) -&gt; tuple
        &#34;&#34;&#34;
        Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
        &#34;&#34;&#34;
        response = get(url, auth=auth)
        if not response.ok:
            return response.content

        df = read_html(response.content, skiprows=3)[0]
        return tuple(
            File.metadata(url, *r)
            for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.File.content"><code class="name">var <span class="ident">content</span> : Any</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.encoding"><code class="name">var <span class="ident">encoding</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.kb"><code class="name">var <span class="ident">kb</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.sn"><code class="name">var <span class="ident">sn</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.time"><code class="name">var <span class="ident">time</span> : datetime.datetime</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.ts"><code class="name">var <span class="ident">ts</span> : datetime.datetime</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.File.url"><code class="name">var <span class="ident">url</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.File.metadata"><code class="name flex">
<span>def <span class="ident">metadata</span></span>(<span>url: str, filename: str, ts: str, size: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a file metadata object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def metadata(cls, url: str, filename: str, ts: str, size: str):
    &#34;&#34;&#34;
    Create a file metadata object
    &#34;&#34;&#34;
    fields = filename.split(&#34;.&#34;)
    encoding = None
    if len(fields) &gt; 1:
        fmt = fields.pop()
        if &#34;sensors&#34; == fmt:
            encoding = FileType.Config
        elif &#34;xml&#34; == fmt:
            encoding = FileType.Schema
        elif &#34;txt&#34; == fmt:
            if fields[-1] == &#34;raw&#34;:
                fields.pop()  # convention is to have &#34;.raw.txt&#34;
            encoding = FileType.Log

    time = None
    if len(fields) &gt; 1:  # dated files
        ft = fields.pop()
        try:
            dt_fmt = &#34;%Y%m%d-%H%M%S&#34; if (ft and len(ft) &gt; 13) else &#34;%Y%m%d-%H%M&#34;
            time = datetime.strptime(ft, dt_fmt)
        except ValueError:
            pass

    try:
        sn = int(fields.pop())
    except ValueError:
        sn = None

    path = url + filename

    return cls(
        name=filename,
        sn=sn,  # maybe None
        url=path,  # retrieval path
        time=time,  # file time from name, maybe None
        ts=datetime.strptime(ts, &#34;%d-%b-%Y %H:%M&#34;),  # timestamp from server
        kb=_parse_str_to_float(size),  # float kilobytes
        encoding=encoding,
    )</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.File.metadata_promise"><code class="name flex">
<span>async def <span class="ident">metadata_promise</span></span>(<span>url, auth)</span>
</code></dt>
<dd>
<div class="desc"><p>Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
async def metadata_promise(url, auth):
    # type: (str, str) -&gt; tuple
    &#34;&#34;&#34;
    Produce a coroutine that will yield file metadata for all files in a remote directory/catalog.
    &#34;&#34;&#34;
    response = get(url, auth=auth)
    if not response.ok:
        return response.content

    df = read_html(response.content, skiprows=3)[0]
    return tuple(
        File.metadata(url, *r)
        for r in zip(*(df[ii][:-1].tolist() for ii in (1, 2, 3)))
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.File.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Format as JSON style dictionary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self):
    &#34;&#34;&#34;Format as JSON style dictionary&#34;&#34;&#34;
    return {
        &#34;url&#34;: self.url,
        &#34;ts&#34;: self.ts,
        &#34;kb&#34;: self.kb,
        &#34;encoding&#34;: self.encoding,
        &#34;content&#34;: self.content,
    }</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.File.sort_key"><code class="name flex">
<span>def <span class="ident">sort_key</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compare by time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_key(self):
    &#34;&#34;&#34;Compare by time&#34;&#34;&#34;
    return self.time</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FileSystem"><code class="flex name class">
<span>class <span class="ident">FileSystem</span></span>
</code></dt>
<dd>
<div class="desc"><p>File systems are made up of files!</p>
<p>Method generated by attrs for class FileSystem.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FileSystem:
    &#34;&#34;&#34;
    File systems are made up of files!
    &#34;&#34;&#34;
    @attr.s
    class OverwritePolicy:
        &#34;&#34;&#34;
        Basic logical unit for allowing/preventing mutability
        &#34;&#34;&#34;
        policy: str = attr.ib(default=&#34;never&#34;)

        def __call__(self, *args, **kwargs):
            if self == &#34;always&#34;:
                return True
            if self == &#34;prompt&#34;:
                print(&#34;Cache already exists. Overwrite? [y/N]&#34;)
                return input() in (&#34;Y&#34;, &#34;y&#34;)
            return False

    policy = OverwritePolicy(policy=&#34;never&#34;)

    @staticmethod
    def load_year_cache(local, years):
        # type: (str, (int, )) -&gt; dict
        &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
        combined = dict()
        for year in years:
            fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
            new = unpickle(fid)
            for key in new.keys():
                try:
                    combined[key] = append(combined[key], new[key])
                except KeyError:
                    combined[key] = array([])
                    combined[key] = append(combined[key], new[key])
        return combined

    @staticmethod
    def indexFileMetadata(url, year, auth=None):
        # type: (str, int, (str,)) -&gt; deque
        &#34;&#34;&#34;
        Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
        that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
        for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
        cached at a leisurely interactive pace.
        &#34;&#34;&#34;
        collector = deque()
        for record in resolveTaskTree(
            FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
        ):
            path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
            collector.append(
                {
                    &#34;date&#34;: date(*record),
                    &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                    &#34;url&#34;: path,
                    &#34;files&#34;: File.metadata_promise(path, auth=auth),
                }
            )
        return collector

    @staticmethod
    def indexFromHtmlTable(
        uriPattern: str, 
        start: datetime = None, 
        end: datetime = None, fmt: 
        str = &#34;%Y%m%d%H%M%S&#34;
    ) -&gt; [[dict]]:
        &#34;&#34;&#34;
        Get the entries for all remote files on server in years of interest.

        :param host: hostname
        :param start: datetime object
        :param end: datetime object
        :param fmt: datetime str formatter
        :return:
        &#34;&#34;&#34;
        
        def fetch(year: int):
            nameFilter = lambda x: isinstance(x[1], str) and f&#34;{year}&#34; in x[1]
            table = array(read_html(uriPattern.format(year)).pop())
            filtered = array(list(filter(nameFilter, table))).T
            names = filtered[1, :]
            dates = array([datetime.strptime(name[:14], fmt) for name in names])
            timestamps = filtered[2, :]
            size = filtered[3,:]

            if year in (start.year, end.year):
                (indices,) = where((start &lt; dates) &amp; (end + timedelta(days=1) &gt; dates))
                iterator = zip(names[indices], dates[indices], timestamps[indices], size[indices])
            else:
                iterator = zip(names, dates, timestamps, size)
    
            return [File(name=name, time=date, ts=ts, kb=sz) for name, date, ts, sz in iterator]

        return list(map(fetch, range(start.year, end.year+1)))
        

    @staticmethod
    async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
        # type: (str, int, int, int, (str, )) -&gt; datetime or None
        &#34;&#34;&#34;
        Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

        Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
        into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
        using the `render()` method, until the specified depth is reached.
        &#34;&#34;&#34;

        def __parse(value):
            &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
            return value if type(value) == int else int(value[:-1])

        if count == depth:
            return enum, None

        try:
            formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
            insert = __parse(enum)
        except TypeError:
            return enum, None

        sublevel = formatter.format(url, insert)
        response = get(sublevel, auth=auth)
        if not response.ok:
            return enum, None

        collector = deque()
        for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
            collector.append(
                FileSystem.indexTaskTree(
                    url=sublevel,
                    enum=__parse(record),  # name
                    count=count + 1,
                    depth=depth,
                    auth=auth,
                )
            )

        return enum, collector

    @staticmethod
    def search(pattern, filesystem):
        # type: (str, dict) -&gt; None or str
        &#34;&#34;&#34;
        Recursively search a directory structure for a key.
        Call this on the result of `index`

        :param filesystem: paths
        :param pattern: search key
        :return:
        &#34;&#34;&#34;
        for key, level in filesystem.items():
            if key == pattern:
                return key
            try:
                result = FileSystem._search(pattern, level)
            except AttributeError:
                result = None
            if result:
                return f&#34;{key}/{result}&#34;
        return None

    @staticmethod
    def _search(
        queue: deque,
        pool: Pool,
        fmt: set = None,
        identity: set = None,
        ts: datetime = None
    ) -&gt; list or None:
        &#34;&#34;&#34;
        Get all XML and configuration files within a directory

        Find configurations from metadata by serial number and date.

        The files can be:
        - On a remote server
        - In the bathysphere_functions_cache
        - Supplied as a list of dictionaries
        &#34;&#34;&#34;
        iterators = []
        queue_size = len(queue)

        if identity:
            iterators.append(repeat(identity, queue_size))
        if fmt:
            iterators.append(repeat(fmt, queue_size))
        if ts:
            iterators.append(repeat(ts, queue_size))

        def _chrono(x: File, ts: datetime = None):
            &#34;&#34;&#34;Chronoloigcal sorting method&#34;&#34;&#34;
            return (
                (x.time is None if ts else x.time is not None),
                (ts - x.time if ts else x.time),
            )

        queue = sorted(queue, key=_chrono, reverse=(False if ts else True))
        if fmt or identity:
            matching = pool.starmap(FileSystem._match, zip(queue, *iterators))
            queue = deque(queue)
        else:
            return {}, queue

        collector = dict()
        for condition in matching:
            if not condition:
                queue.rotate(1)
                continue
            file = queue.popleft()
            if not collector.get(file.sn, None):
                collector[file.sn] = deque()
            if (
                not ts or len(collector[file.sn]) == 0
            ):  # limit to length 1 for getting most recent
                collector[file.sn].append(file)
                continue

            queue.append(file)  # put the file back if unused

        return collector, queue

   
    def get(
        self,
        observed_properties,
        path=None,
        transpose=True,
        dataset=None,
        kind=&#34;float64&#34;,
        date=None,
    ):
        # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
        &#34;&#34;&#34;
        Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
        by name, resulting in an array. For previously processed internal data, arrays are stored as
        binary data in either `.pkl` or `.bathysphere_functions_cache` files.

        :param observed_properties: lookup field names
        :param path: path to local files if loading
        :param transpose: transpose the array before saving, makes join later easier
        :param dataset: NetCDF reference as in-memory object
        :param kind: numerical format for arrays
        :param date: specific timestamp to sample
        &#34;&#34;&#34;
        result = dict()

        if isinstance(observed_properties, str):
            fields = keys = [observed_properties]
        elif isinstance(observed_properties, dict):
            keys = observed_properties.keys()
            fields = observed_properties.values()
        else:
            fields = keys = observed_properties
        iterator = zip(*(keys, fields))

        for key, rename in iterator:
            if path:
                try:
                    fid = open(key, &#34;rb&#34;)
                except FileNotFoundError:
                    continue
                data = FileSystem.load_year_cache(fid).transpose() if transpose else FileSystem.load_year_cache(fid)
                fid.close()

            elif dataset:
                data = dataset.variables[key][:].astype(kind)
                FileSystem.set(date, data, key)
            else:
                data = None

            result[rename] = data

        return result

    @staticmethod
    def syncFtp(ftp, remote, local, filesystem=None):
        # type: (FTP, str, str, dict) -&gt; int
        &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
        path = FileSystem.search(pattern=remote, filesystem=filesystem)
        with open(local, &#34;wb+&#34;) as fid:
            return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))

    @staticmethod
    def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
        # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
        &#34;&#34;&#34;
        Build directory structure recursively.

        :param ftp: persistent ftp connection
        :param node: node in current working directory
        :param depth: current depth, do not set
        :param limit: maximum depth,
        :param metadata: pass the object metadata down one level
        :param parent:
        :return:
        &#34;&#34;&#34;

        body = loads(req)
        host = body.get(&#34;host&#34;, None)
        root = body.get(&#34;root&#34;, None)
        ftp = FTP(host, timeout=4)
        assert &#34;230&#34; in ftp.login()  # attach if no open socket
        assert ftp.sock
        if root is not None:
            _ = ftp.cwd(root)

        def _map(rec):
            values = rec.split()
            key = values.pop().strip()
            return {key: values}

        if depth == 0 and parent is None:
            parent = None  # create Location

        if limit is None or depth &lt;= limit:
            try:
                _ = ftp.cwd(node)  # target is a file
            except:
                pass
            else:
                collection = None

                files = []
                ftp.retrlines(&#34;LIST&#34;, files.append)
                for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                    FileSystem.indexFtp(
                        ftp=ftp,
                        graph=graph,
                        node=k,
                        depth=depth + 1,
                        limit=limit,
                        metadata=v,
                        parent=collection,
                    )

                if node != &#34;.&#34;:
                    _ = ftp.cwd(&#34;..&#34;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.OverwritePolicy"><code class="name">var <span class="ident">OverwritePolicy</span></code></dt>
<dd>
<div class="desc"><p>Basic logical unit for allowing/preventing mutability</p></div>
</dd>
<dt id="bathysphere.datatypes.FileSystem.policy"><code class="name">var <span class="ident">policy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.indexFileMetadata"><code class="name flex">
<span>def <span class="ident">indexFileMetadata</span></span>(<span>url, year, auth=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
that contains a <coroutine> in the place of file meta_data. This only takes a few seconds, compared to minutes
for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
cached at a leisurely interactive pace.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def indexFileMetadata(url, year, auth=None):
    # type: (str, int, (str,)) -&gt; deque
    &#34;&#34;&#34;
    Callable method to map a remote HTTP-accessible file catalog by date, and then build an time-indexed structure
    that contains a &lt;coroutine&gt; in the place of file meta_data. This only takes a few seconds, compared to minutes
    for resolving all files. Usually, only some data is needed immediately, so tasks can be resolved on demand and
    cached at a leisurely interactive pace.
    &#34;&#34;&#34;
    collector = deque()
    for record in resolveTaskTree(
        FileSystem.indexTaskTree(url=url, enum=year, auth=auth, depth=2)
    ):
        path = &#34;{}/{:04}/{:02}/{:02}/&#34;.format(url, *record)
        collector.append(
            {
                &#34;date&#34;: date(*record),
                &#34;name&#34;: &#34;{}-{:02}-{}&#34;.format(*record),
                &#34;url&#34;: path,
                &#34;files&#34;: File.metadata_promise(path, auth=auth),
            }
        )
    return collector</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexFromHtmlTable"><code class="name flex">
<span>def <span class="ident">indexFromHtmlTable</span></span>(<span>uriPattern: str, start: datetime = None, end: datetime = None, fmt: str = '%Y%m%d%H%M%S') ‑> [[dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the entries for all remote files on server in years of interest.</p>
<p>:param host: hostname
:param start: datetime object
:param end: datetime object
:param fmt: datetime str formatter
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def indexFromHtmlTable(
    uriPattern: str, 
    start: datetime = None, 
    end: datetime = None, fmt: 
    str = &#34;%Y%m%d%H%M%S&#34;
) -&gt; [[dict]]:
    &#34;&#34;&#34;
    Get the entries for all remote files on server in years of interest.

    :param host: hostname
    :param start: datetime object
    :param end: datetime object
    :param fmt: datetime str formatter
    :return:
    &#34;&#34;&#34;
    
    def fetch(year: int):
        nameFilter = lambda x: isinstance(x[1], str) and f&#34;{year}&#34; in x[1]
        table = array(read_html(uriPattern.format(year)).pop())
        filtered = array(list(filter(nameFilter, table))).T
        names = filtered[1, :]
        dates = array([datetime.strptime(name[:14], fmt) for name in names])
        timestamps = filtered[2, :]
        size = filtered[3,:]

        if year in (start.year, end.year):
            (indices,) = where((start &lt; dates) &amp; (end + timedelta(days=1) &gt; dates))
            iterator = zip(names[indices], dates[indices], timestamps[indices], size[indices])
        else:
            iterator = zip(names, dates, timestamps, size)

        return [File(name=name, time=date, ts=ts, kb=sz) for name, date, ts, sz in iterator]

    return list(map(fetch, range(start.year, end.year+1)))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexFtp"><code class="name flex">
<span>def <span class="ident">indexFtp</span></span>(<span>req, node='.', depth=0, limit=None, metadata=None, parent=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Build directory structure recursively.</p>
<p>:param ftp: persistent ftp connection
:param node: node in current working directory
:param depth: current depth, do not set
:param limit: maximum depth,
:param metadata: pass the object metadata down one level
:param parent:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def indexFtp(req, node=&#34;.&#34;, depth=0, limit=None, metadata=None, parent=None):
    # type: (FTP, str, int, int or None, dict or None, dict) -&gt; None
    &#34;&#34;&#34;
    Build directory structure recursively.

    :param ftp: persistent ftp connection
    :param node: node in current working directory
    :param depth: current depth, do not set
    :param limit: maximum depth,
    :param metadata: pass the object metadata down one level
    :param parent:
    :return:
    &#34;&#34;&#34;

    body = loads(req)
    host = body.get(&#34;host&#34;, None)
    root = body.get(&#34;root&#34;, None)
    ftp = FTP(host, timeout=4)
    assert &#34;230&#34; in ftp.login()  # attach if no open socket
    assert ftp.sock
    if root is not None:
        _ = ftp.cwd(root)

    def _map(rec):
        values = rec.split()
        key = values.pop().strip()
        return {key: values}

    if depth == 0 and parent is None:
        parent = None  # create Location

    if limit is None or depth &lt;= limit:
        try:
            _ = ftp.cwd(node)  # target is a file
        except:
            pass
        else:
            collection = None

            files = []
            ftp.retrlines(&#34;LIST&#34;, files.append)
            for k, v in reduce(lambda x, y: {**x, **y}, map(_map, files), {}).items():
                FileSystem.indexFtp(
                    ftp=ftp,
                    graph=graph,
                    node=k,
                    depth=depth + 1,
                    limit=limit,
                    metadata=v,
                    parent=collection,
                )

            if node != &#34;.&#34;:
                _ = ftp.cwd(&#34;..&#34;)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.indexTaskTree"><code class="name flex">
<span>async def <span class="ident">indexTaskTree</span></span>(<span>url, enum, count=0, depth=2, auth=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Private method is used by <code>metadata()</code> to build a temporal index with multiple levels of resolution on demand.</p>
<p>Recursively <code>GET</code> file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
into nested tuples of (index, <coroutine>). The coroutine is then resolved to another (index, <coroutine>) tuple,
using the <code>render()</code> method, until the specified depth is reached.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
async def indexTaskTree(url, enum, count=0, depth=2, auth=None):
    # type: (str, int, int, int, (str, )) -&gt; datetime or None
    &#34;&#34;&#34;
    Private method is used by `metadata()` to build a temporal index with multiple levels of resolution on demand.

    Recursively `GET` file metadata in a destination file catalog, based on date, then bathysphere_functions_parse the tabular HTML
    into nested tuples of (index, &lt;coroutine&gt;). The coroutine is then resolved to another (index, &lt;coroutine&gt;) tuple,
    using the `render()` method, until the specified depth is reached.
    &#34;&#34;&#34;

    def __parse(value):
        &#34;&#34;&#34;Convenience method for integer type conversion&#34;&#34;&#34;
        return value if type(value) == int else int(value[:-1])

    if count == depth:
        return enum, None

    try:
        formatter = &#34;{{}}/{{:0{}d}}&#34;.format(4 if count == 0 else 2)
        insert = __parse(enum)
    except TypeError:
        return enum, None

    sublevel = formatter.format(url, insert)
    response = get(sublevel, auth=auth)
    if not response.ok:
        return enum, None

    collector = deque()
    for record in deque(response.content.decode().split(&#34;\n&#34;)[3:-1]):
        collector.append(
            FileSystem.indexTaskTree(
                url=sublevel,
                enum=__parse(record),  # name
                count=count + 1,
                depth=depth,
                auth=auth,
            )
        )

    return enum, collector</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.load_year_cache"><code class="name flex">
<span>def <span class="ident">load_year_cache</span></span>(<span>local, years)</span>
</code></dt>
<dd>
<div class="desc"><p>Load a local binary file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def load_year_cache(local, years):
    # type: (str, (int, )) -&gt; dict
    &#34;&#34;&#34;Load a local binary file&#34;&#34;&#34;
    combined = dict()
    for year in years:
        fid = open(f&#34;{local}/{year}_checkpoint.pickle&#34;, &#34;rb&#34;)
        new = unpickle(fid)
        for key in new.keys():
            try:
                combined[key] = append(combined[key], new[key])
            except KeyError:
                combined[key] = array([])
                combined[key] = append(combined[key], new[key])
    return combined</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>pattern, filesystem)</span>
</code></dt>
<dd>
<div class="desc"><p>Recursively search a directory structure for a key.
Call this on the result of <code>index</code></p>
<p>:param filesystem: paths
:param pattern: search key
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def search(pattern, filesystem):
    # type: (str, dict) -&gt; None or str
    &#34;&#34;&#34;
    Recursively search a directory structure for a key.
    Call this on the result of `index`

    :param filesystem: paths
    :param pattern: search key
    :return:
    &#34;&#34;&#34;
    for key, level in filesystem.items():
        if key == pattern:
            return key
        try:
            result = FileSystem._search(pattern, level)
        except AttributeError:
            result = None
        if result:
            return f&#34;{key}/{result}&#34;
    return None</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.FileSystem.syncFtp"><code class="name flex">
<span>def <span class="ident">syncFtp</span></span>(<span>ftp, remote, local, filesystem=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Find and copy a file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def syncFtp(ftp, remote, local, filesystem=None):
    # type: (FTP, str, str, dict) -&gt; int
    &#34;&#34;&#34;Find and copy a file&#34;&#34;&#34;
    path = FileSystem.search(pattern=remote, filesystem=filesystem)
    with open(local, &#34;wb+&#34;) as fid:
        return int(ftp.retrbinary(f&#34;RETR {path}&#34;, fid.write))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.FileSystem.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, observed_properties, path=None, transpose=True, dataset=None, kind='float64', date=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
by name, resulting in an array. For previously processed internal data, arrays are stored as
binary data in either <code>.pkl</code> or <code>.bathysphere_functions_cache</code> files.</p>
<p>:param observed_properties: lookup field names
:param path: path to local files if loading
:param transpose: transpose the array before saving, makes join later easier
:param dataset: NetCDF reference as in-memory object
:param kind: numerical format for arrays
:param date: specific timestamp to sample</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(
    self,
    observed_properties,
    path=None,
    transpose=True,
    dataset=None,
    kind=&#34;float64&#34;,
    date=None,
):
    # type: (str or [str] or dict, str, bool, Dataset, str, datetime) -&gt; dict
    &#34;&#34;&#34;
    Load variables from NetCDF or pickled files into memory. For NetCDF, each variable is accessed
    by name, resulting in an array. For previously processed internal data, arrays are stored as
    binary data in either `.pkl` or `.bathysphere_functions_cache` files.

    :param observed_properties: lookup field names
    :param path: path to local files if loading
    :param transpose: transpose the array before saving, makes join later easier
    :param dataset: NetCDF reference as in-memory object
    :param kind: numerical format for arrays
    :param date: specific timestamp to sample
    &#34;&#34;&#34;
    result = dict()

    if isinstance(observed_properties, str):
        fields = keys = [observed_properties]
    elif isinstance(observed_properties, dict):
        keys = observed_properties.keys()
        fields = observed_properties.values()
    else:
        fields = keys = observed_properties
    iterator = zip(*(keys, fields))

    for key, rename in iterator:
        if path:
            try:
                fid = open(key, &#34;rb&#34;)
            except FileNotFoundError:
                continue
            data = FileSystem.load_year_cache(fid).transpose() if transpose else FileSystem.load_year_cache(fid)
            fid.close()

        elif dataset:
            data = dataset.variables[key][:].astype(kind)
            FileSystem.set(date, data, key)
        else:
            data = None

        result[rename] = data

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.FileType"><code class="flex name class">
<span>class <span class="ident">FileType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Well known file types</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FileType(Enum):
    &#34;&#34;&#34;Well known file types&#34;&#34;&#34;
    Schema = 1
    Config = 2
    Log = 3</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.FileType.Config"><code class="name">var <span class="ident">Config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Log"><code class="name">var <span class="ident">Log</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.FileType.Schema"><code class="name">var <span class="ident">Schema</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Interval"><code class="flex name class">
<span>class <span class="ident">Interval</span></span>
<span>(</span><span>lower: <a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a> = None, upper: <a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Intervals are convenience data structs for sorting and numerical queries</p>
<p>Method generated by attrs for class Interval.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Interval:
    &#34;&#34;&#34;Intervals are convenience data structs for sorting and numerical queries&#34;&#34;&#34;
    lower: Bound = attr.ib(default=None)
    upper: Bound = attr.ib(default=None)


    def overlaps(self, other: Interval) -&gt; bool:
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.upper.value and 
            self.upper.value &gt;= other.lower.value
        )


    def __contains__(self, other: Interval):
        &#34;&#34;&#34;
        A wholly or partially contains B
        &#34;&#34;&#34;
        return (
            self.lower.value &lt;= other.lower.value and 
            self.upper.value &gt;= other.upper.value
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Interval.lower"><code class="name">var <span class="ident">lower</span> : <a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Interval.upper"><code class="name">var <span class="ident">upper</span> : <a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Interval.overlaps"><code class="name flex">
<span>def <span class="ident">overlaps</span></span>(<span>self, other: <a title="bathysphere.datatypes.Interval" href="#bathysphere.datatypes.Interval">Interval</a>) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>A wholly or partially contains B</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overlaps(self, other: Interval) -&gt; bool:
    &#34;&#34;&#34;
    A wholly or partially contains B
    &#34;&#34;&#34;
    return (
        self.lower.value &lt;= other.upper.value and 
        self.upper.value &gt;= other.lower.value
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper"><code class="flex name class">
<span>class <span class="ident">JSONIOWrapper</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Use JSON messages piped between between processes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JSONIOWrapper(TextIOWrapper):
    &#34;&#34;&#34;
    Use JSON messages piped between between processes
    &#34;&#34;&#34;
    @staticmethod
    def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
        &#34;&#34;&#34;
        Log notifications.

        :param message: some event notification
        :param data: data that resulted in message
        :param log: log file or interface
        :param arrow: symbol indicating direction of flow

        :return:
        &#34;&#34;&#34;
        timestamp = datetime.now().isoformat(sep=&#34; &#34;)
        string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
        if log is not None:
            log.write((string + &#34;\n&#34;).encode())
            return None
        print(string)

    def receive(self, log: BytesIO) -&gt; dict:
        &#34;&#34;&#34;
        Receive serialized data from command line interface.
        &#34;&#34;&#34;
        json = self.readline()
        self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
        try:
            data = loads(json.rstrip())
        except decoder.JSONDecodeError as decode_error:
            self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
            message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
            return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

        return data

    def send(self, data: dict, log: BytesIO) -&gt; None:
        &#34;&#34;&#34;
        Write serialized data to interface.
        &#34;&#34;&#34;

        def _transform():
            safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
            return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

        json = _transform()
        self.log(message=&#34;Send&#34;, data=json, log=log)
        self.write(f&#34;{json}\n&#34;)

    def dump(self) -&gt; None:
        &#34;&#34;&#34;
        Propagates messages up through C#, subprocess, and control layers.
        &#34;&#34;&#34;
        response = self.readline()
        while response != &#34;&#34;:
            response = self.readline()
            print(response.rstrip())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>_io.TextIOWrapper</li>
<li>_io._TextIOBase</li>
<li>_io._IOBase</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.JSONIOWrapper.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>message: str, data: str, log: BytesIO = None, arrow: str = &#x27;-&gt;&#x27;) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Log notifications.</p>
<p>:param message: some event notification
:param data: data that resulted in message
:param log: log file or interface
:param arrow: symbol indicating direction of flow</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def log(message: str, data: str, log: BytesIO = None, arrow: str = &#34;-&gt;&#34;) -&gt; None:
    &#34;&#34;&#34;
    Log notifications.

    :param message: some event notification
    :param data: data that resulted in message
    :param log: log file or interface
    :param arrow: symbol indicating direction of flow

    :return:
    &#34;&#34;&#34;
    timestamp = datetime.now().isoformat(sep=&#34; &#34;)
    string = f&#34;[{timestamp}] (PID {getpid()}) {message} {arrow} {data}&#34;
    if log is not None:
        log.write((string + &#34;\n&#34;).encode())
        return None
    print(string)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.JSONIOWrapper.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Propagates messages up through C#, subprocess, and control layers.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self) -&gt; None:
    &#34;&#34;&#34;
    Propagates messages up through C#, subprocess, and control layers.
    &#34;&#34;&#34;
    response = self.readline()
    while response != &#34;&#34;:
        response = self.readline()
        print(response.rstrip())</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper.receive"><code class="name flex">
<span>def <span class="ident">receive</span></span>(<span>self, log: BytesIO) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Receive serialized data from command line interface.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def receive(self, log: BytesIO) -&gt; dict:
    &#34;&#34;&#34;
    Receive serialized data from command line interface.
    &#34;&#34;&#34;
    json = self.readline()
    self.log(&#34;Receive&#34;, json.rstrip(), log=log, arrow=&#34;&lt;-&#34;)
    try:
        data = loads(json.rstrip())
    except decoder.JSONDecodeError as decode_error:
        self.log(message=&#34;Job cancelled&#34;, data=decode_error.msg, log=log)
        message = &#34;no data received&#34; if json is &#34;\n&#34; else decode_error.msg
        return {&#34;status&#34;: &#34;error&#34;, &#34;message&#34;: message, &#34;data&#34;: json}

    return data</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.JSONIOWrapper.send"><code class="name flex">
<span>def <span class="ident">send</span></span>(<span>self, data: dict, log: BytesIO) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Write serialized data to interface.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send(self, data: dict, log: BytesIO) -&gt; None:
    &#34;&#34;&#34;
    Write serialized data to interface.
    &#34;&#34;&#34;

    def _transform():
        safe_keys = {key.replace(&#34; &#34;, &#34;_&#34;): value for key, value in data.items()}
        return f&#34;&#39;{dumps(safe_keys)}&#39;&#34;.replace(&#34; &#34;, &#34;&#34;)

    json = _transform()
    self.log(message=&#34;Send&#34;, data=json, log=log)
    self.write(f&#34;{json}\n&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator"><code class="flex name class">
<span>class <span class="ident">KernelDensityEstimator</span></span>
</code></dt>
<dd>
<div class="desc"><p>Predict events in space</p>
<p>Method generated by attrs for class KernelDensityEstimator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KernelDensityEstimator(KernelDensity):
    &#34;&#34;&#34;Predict events in space&#34;&#34;&#34;
    @staticmethod
    def glm():
        &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
        return LinearRegression()  

    @staticmethod
    def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
        &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
        epsilon = mesh.fields[key]
        field = mesh.nodes.xye(epsilon)
        target = mesh.interp2d(xx, yy, epsilon)  # location suitability

        return field, target

    def intensity(self, field: object):
        &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
        intensity = self.score_samples(field)  # create intensity field
        maximum = intensity.max()
        minimum = intensity.min()
        cost = (intensity - minimum) / (maximum - minimum)

        return intensity, cost

    @staticmethod
    def train(self, target: iter, field: object, xx: iter, yy: iter):
        &#34;&#34;&#34;
        Train kernel density estimator model using a quantized mesh

        :param mesh: Mesh object of the Interpolator super type
        :param key: Spatial field to train on
        :return:
        &#34;&#34;&#34;
        subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
        self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
        return self.intensity(field)

    @staticmethod
    def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
        &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

        xnew = []
        ynew = []

        def prohibit():
            &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
            xtemp = array(xin + xnew)
            ytemp = array(yin + ynew)
            dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
            nearest = dxy.min()
            return nearest &lt; 0.5 * bandwidth

        xmin, ymin = transform(view, native, extent[0], extent[1])
        xmax, ymax = transform(view, native, extent[2], extent[3])

        total = 0
        passes = 0
        while total &lt; count and passes &lt; count * 10:

            sample = kde.sample()
            xx = sample[0][0]
            yy = sample[0][1]

            if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

                if bandwidth is not None and prohibit():
                    xnew.append(xx)
                    ynew.append(yy)
                    total += 1

                else:
                    passes += 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.neighbors._kde.KernelDensity</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh"><code class="name flex">
<span>def <span class="ident">get_epsilon_from_mesh</span></span>(<span>mesh: object, key: str, xx, yy)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve probability field</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_epsilon_from_mesh(mesh: object, key: str, xx, yy):
    &#34;&#34;&#34;Retrieve probability field&#34;&#34;&#34;
    epsilon = mesh.fields[key]
    field = mesh.nodes.xye(epsilon)
    target = mesh.interp2d(xx, yy, epsilon)  # location suitability

    return field, target</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.glm"><code class="name flex">
<span>def <span class="ident">glm</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>create linear regression model object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def glm():
    &#34;&#34;&#34;create linear regression model object&#34;&#34;&#34;
    return LinearRegression()  </code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>extent, count, view, native, kde, xin, yin, bandwidth=1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict new locations based on trained model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def predict(extent, count, view, native, kde, xin, yin, bandwidth=1000):
    &#34;&#34;&#34; Predict new locations based on trained model&#34;&#34;&#34;

    xnew = []
    ynew = []

    def prohibit():
        &#34;&#34;&#34; Strict local inhibition &#34;&#34;&#34;
        xtemp = array(xin + xnew)
        ytemp = array(yin + ynew)
        dxy = ((xtemp - xx) ** 2 + (ytemp - yy) ** 2) ** 0.5
        nearest = dxy.min()
        return nearest &lt; 0.5 * bandwidth

    xmin, ymin = transform(view, native, extent[0], extent[1])
    xmax, ymax = transform(view, native, extent[2], extent[3])

    total = 0
    passes = 0
    while total &lt; count and passes &lt; count * 10:

        sample = kde.sample()
        xx = sample[0][0]
        yy = sample[0][1]

        if (xmax &gt; xx &gt; xmin) and (ymax &gt; yy &gt; ymin):  # particle is in window

            if bandwidth is not None and prohibit():
                xnew.append(xx)
                ynew.append(yy)
                total += 1

            else:
                passes += 1</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.KernelDensityEstimator.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, target: iter, field: object, xx: iter, yy: iter)</span>
</code></dt>
<dd>
<div class="desc"><p>Train kernel density estimator model using a quantized mesh</p>
<p>:param mesh: Mesh object of the Interpolator super type
:param key: Spatial field to train on
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def train(self, target: iter, field: object, xx: iter, yy: iter):
    &#34;&#34;&#34;
    Train kernel density estimator model using a quantized mesh

    :param mesh: Mesh object of the Interpolator super type
    :param key: Spatial field to train on
    :return:
    &#34;&#34;&#34;
    subset, _ = where(~isnan(target.data))  # mark non-NaN values to retain
    self.fit(hstack((xx[subset], yy[subset], target[subset])))  # train estimator
    return self.intensity(field)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.KernelDensityEstimator.intensity"><code class="name flex">
<span>def <span class="ident">intensity</span></span>(<span>self, field: object)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate density of observations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intensity(self, field: object):
    &#34;&#34;&#34;Calculate density of observations&#34;&#34;&#34;
    intensity = self.score_samples(field)  # create intensity field
    maximum = intensity.max()
    minimum = intensity.min()
    cost = (intensity - minimum) / (maximum - minimum)

    return intensity, cost</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage"><code class="flex name class">
<span>class <span class="ident">ObjectStorage</span></span>
<span>(</span><span>bucket_name: str, endpoint: str, prefix: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>S3 compatible object storage interface using Minio as the client.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ObjectStorage(Minio):
    &#34;&#34;&#34;
    S3 compatible object storage interface using Minio as the client. 
    &#34;&#34;&#34;
    def __init__(self, bucket_name: str, endpoint: str, prefix: str = None, **kwargs):
        self.bucket_name = bucket_name
        self.prefix = prefix
        self.endpoint = endpoint
        
        super().__init__(endpoint, **kwargs)
        if not self.bucket_exists(bucket_name):
            self.make_bucket(bucket_name)

    @property
    def locked(self) -&gt; bool:
        &#34;&#34;&#34;
        Object sub-tree is locked. Denoted by placing a `lock.json` object with the same
        prefix.
        &#34;&#34;&#34;
        return self.stat_object(&#34;lock.json&#34;) is not None

    def publish_events(self, pubsub_channel: str):
        &#34;&#34;&#34;
        Listener for bucket events which then sends confirmation to a redis message queue
        &#34;&#34;&#34;
        fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
        with StrictRedis() as queue:
            for event in self.listen_bucket_notification(
                self.bucket_name, &#34;&#34;, None, fcns
            ):
                queue.publish(pubsub_channel, str(event))

    def stat_object(self, object_name: str):
        &#34;&#34;&#34;
        Determine whether an object key exists
        &#34;&#34;&#34;
        try:
            return super().stat_object(self.bucket_name, object_name)
        except NoSuchKey:
            return None

    def list_objects(self, prefix: str = None):
        &#34;&#34;&#34;
        Return a list of objects in the bucket with the same optional prefix/
        &#34;&#34;&#34;
        return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))

    def put_object(
        self,
        object_name: str,
        data: dict or bytes,
        metadata: dict = None,
        codec: str = &#34;utf-8&#34;,
    ) -&gt; str:
        &#34;&#34;&#34;
        Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

        :param label: label for file
        :param data: data to serialize
        :param metadata: headers
        :param codec: how to encode strings
        &#34;&#34;&#34;
        if isinstance(data, dict):
            content_type = &#34;application/json&#34;
            buffer = bytes(dumps(data).encode(codec))
        elif isinstance(data, bytes):
            content_type = &#34;text/plain&#34;
            buffer = data
        else:
            raise TypeError

        accumulate = []
        given_parts = object_name.split(&#34;/&#34;)
        prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
        if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
            for pp in prefix_parts:
                if pp not in given_parts:
                    accumulate.append(pp)
            accumulate.extend(given_parts)
            object_name = &#34;/&#34;.join(accumulate)

        super().put_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            data=BytesIO(buffer),
            length=len(buffer),
            metadata=metadata,
            content_type=content_type,
        )

        return object_name

    def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
        &#34;&#34;&#34;
        Download the data, may be streaming if desired
        &#34;&#34;&#34;
        data = super().get_object(self.bucket_name, object_name)
        if stream:

            def generate():
                for d in data.stream(32 * 1024):
                    yield d

            result = generate
        else:
            result = data

        return Response(result, mimetype=&#34;application/octet-stream&#34;)

    def updateIndex(
        self,
        object_name: str,
        metadata: dict = None,
        entries: [dict] = None,
        props: dict = None,
    ):
        &#34;&#34;&#34;
        Update contents of index metadata
        &#34;&#34;&#34;

        if entries:
            self.put_object(
                object_name=object_name,
                data={
                    **loads(self.get_object(object_name=object_name).data),
                    **(entries or {}),
                    **(props or {}),
                },
                metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
            )
        else:
            self.copy_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                object_source=object_name,
                metadata=metadata,
            )

        return self

    def delete(
        self, 
        prefix: str, 
        batch: int = 10, 
        conditions: dict = None
    ) -&gt; (Any):
        &#34;&#34;&#34;
        Delete all objects within a subdirectory or abstract collection.

        The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
        default, and therefore needs to be iterated through before returning any errors. 

        :param prefic: file prefix/dataset
        :param batch:  number to delete at a time
        &#34;&#34;&#34;
        remove = ()
        errors = ()
        
        objects_iter = self.list_objects(prefix=prefix)
        stop = False
        while not stop:
            try:
                object_name = next(objects_iter).object_name
            except StopIteration:
                stop = True
            else:
                stat = self.stat_object(object_name)
                if isinstance(conditions, dict):
                    if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                        remove += (object_name,)
                else:
                    remove += (object_name,)

            if len(remove) &gt;= batch or stop:
                for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                    errors += (error,)
                return errors


    @staticmethod
    def metadata_template(
        file_type: str = None, parent: str = None, headers: dict = None
    ) -&gt; dict:

        accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

        return {
            &#34;x-amz-acl&#34;: accessControl,
            &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
            &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
            &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
            &#34;x-amz-meta-service-file-type&#34;: file_type,
            **(headers or {}),
        }

    def unlock(self, object_name: str, session: str = None,) -&gt; bool:
        &#34;&#34;&#34;
        Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
        &#34;&#34;&#34;
        try:
            self.remove_object(self.bucket_name, object_name)
        except NoSuchKey:
            return False
        return True

    def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
        &#34;&#34;&#34;
        Object storage locking decorator for functions.

        When used this implements a mutex lock on the object path,
        which will block competing operations until it is cleared.

        Locks will not block read operations except in special cases. 
        &#34;&#34;&#34;

        # index = load_json(self.get_object(object_name=index_file))

        headers = {}
        session_id = uuid4().hex
        name = &#34;bathysphere&#34;
        lock_file = f&#34;{name}/lock.json&#34;
        # index_file = f&#34;{name}/index.json&#34;

        def decorator(fcn):
            &#34;&#34;&#34;
            Methods applied to the wrapped function
            &#34;&#34;&#34;

            def wrapper(*args, **kwargs):
                &#34;&#34;&#34;
                Actual wrapper that calls the decorated function
                &#34;&#34;&#34;
                if self.stat_object(lock_file):
                    return &#34;Lock in place&#34;, 500
                try:
                    self.put_object(
                        object_name=lock_file,
                        data={&#34;session&#34;: session_id},
                        metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                    )
                except NoSuchKey:
                    return &#34;Could not lock repository&#34;, 500
                try:
                    result = fcn(*args, **kwargs)
                except Exception as ex:
                    result = f&#34;{ex}&#34;, 500
                finally:
                    if lock and not self.unlock(object_name=lock):
                        result = &#34;Failed to unlock&#34;, 500
                return result

            return wrapper

        return decorator</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>minio.api.Minio</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.metadata_template"><code class="name flex">
<span>def <span class="ident">metadata_template</span></span>(<span>file_type: str = None, parent: str = None, headers: dict = None) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def metadata_template(
    file_type: str = None, parent: str = None, headers: dict = None
) -&gt; dict:

    accessControl = &#34;private&#34; if file_type == &#34;lock&#34; else &#34;public-read&#34;

    return {
        &#34;x-amz-acl&#34;: accessControl,
        &#34;x-amz-meta-parent&#34;: parent or &#34;&#34;,
        &#34;x-amz-meta-created&#34;: datetime.utcnow().isoformat(),
        &#34;x-amz-meta-extent&#34;: &#34;null&#34;,
        &#34;x-amz-meta-service-file-type&#34;: file_type,
        **(headers or {}),
    }</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.locked"><code class="name">var <span class="ident">locked</span> : bool</code></dt>
<dd>
<div class="desc"><p>Object sub-tree is locked. Denoted by placing a <code>lock.json</code> object with the same
prefix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def locked(self) -&gt; bool:
    &#34;&#34;&#34;
    Object sub-tree is locked. Denoted by placing a `lock.json` object with the same
    prefix.
    &#34;&#34;&#34;
    return self.stat_object(&#34;lock.json&#34;) is not None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.ObjectStorage.delete"><code class="name flex">
<span>def <span class="ident">delete</span></span>(<span>self, prefix: str, batch: int = 10, conditions: dict = None) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Delete all objects within a subdirectory or abstract collection.</p>
<p>The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
default, and therefore needs to be iterated through before returning any errors. </p>
<p>:param prefic: file prefix/dataset
:param batch:
number to delete at a time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete(
    self, 
    prefix: str, 
    batch: int = 10, 
    conditions: dict = None
) -&gt; (Any):
    &#34;&#34;&#34;
    Delete all objects within a subdirectory or abstract collection.

    The remove_objects method is a bit tricky. It returns a generator which is not evaulated by
    default, and therefore needs to be iterated through before returning any errors. 

    :param prefic: file prefix/dataset
    :param batch:  number to delete at a time
    &#34;&#34;&#34;
    remove = ()
    errors = ()
    
    objects_iter = self.list_objects(prefix=prefix)
    stop = False
    while not stop:
        try:
            object_name = next(objects_iter).object_name
        except StopIteration:
            stop = True
        else:
            stat = self.stat_object(object_name)
            if isinstance(conditions, dict):
                if all(stat.metadata.get(k) == v for k, v in conditions.items()):
                    remove += (object_name,)
            else:
                remove += (object_name,)

        if len(remove) &gt;= batch or stop:
            for error in self.remove_objects(bucket_name=self.bucket_name, objects_iter=remove):
                errors += (error,)
            return errors</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.get_object"><code class="name flex">
<span>def <span class="ident">get_object</span></span>(<span>self, object_name: str, stream: bool = False) ‑> flask.wrappers.Response</span>
</code></dt>
<dd>
<div class="desc"><p>Download the data, may be streaming if desired</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_object(self, object_name: str, stream: bool = False) -&gt; Response:
    &#34;&#34;&#34;
    Download the data, may be streaming if desired
    &#34;&#34;&#34;
    data = super().get_object(self.bucket_name, object_name)
    if stream:

        def generate():
            for d in data.stream(32 * 1024):
                yield d

        result = generate
    else:
        result = data

    return Response(result, mimetype=&#34;application/octet-stream&#34;)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.list_objects"><code class="name flex">
<span>def <span class="ident">list_objects</span></span>(<span>self, prefix: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a list of objects in the bucket with the same optional prefix/</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_objects(self, prefix: str = None):
    &#34;&#34;&#34;
    Return a list of objects in the bucket with the same optional prefix/
    &#34;&#34;&#34;
    return super().list_objects(self.bucket_name, prefix=(prefix or self.prefix))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.publish_events"><code class="name flex">
<span>def <span class="ident">publish_events</span></span>(<span>self, pubsub_channel: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Listener for bucket events which then sends confirmation to a redis message queue</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def publish_events(self, pubsub_channel: str):
    &#34;&#34;&#34;
    Listener for bucket events which then sends confirmation to a redis message queue
    &#34;&#34;&#34;
    fcns = (&#34;s3:ObjectCreated:*&#34;, &#34;s3:ObjectRemoved:*&#34;, &#34;s3:ObjectAccessed:*&#34;)
    with StrictRedis() as queue:
        for event in self.listen_bucket_notification(
            self.bucket_name, &#34;&#34;, None, fcns
        ):
            queue.publish(pubsub_channel, str(event))</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.put_object"><code class="name flex">
<span>def <span class="ident">put_object</span></span>(<span>self, object_name: str, data: dict or bytes, metadata: dict = None, codec: str = 'utf-8') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Create an s3 connection if necessary, then create bucket if it doesn't exist.</p>
<p>:param label: label for file
:param data: data to serialize
:param metadata: headers
:param codec: how to encode strings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def put_object(
    self,
    object_name: str,
    data: dict or bytes,
    metadata: dict = None,
    codec: str = &#34;utf-8&#34;,
) -&gt; str:
    &#34;&#34;&#34;
    Create an s3 connection if necessary, then create bucket if it doesn&#39;t exist.

    :param label: label for file
    :param data: data to serialize
    :param metadata: headers
    :param codec: how to encode strings
    &#34;&#34;&#34;
    if isinstance(data, dict):
        content_type = &#34;application/json&#34;
        buffer = bytes(dumps(data).encode(codec))
    elif isinstance(data, bytes):
        content_type = &#34;text/plain&#34;
        buffer = data
    else:
        raise TypeError

    accumulate = []
    given_parts = object_name.split(&#34;/&#34;)
    prefix_parts = (self.prefix or &#34;&#34;).split(&#34;/&#34;)
    if len(given_parts) &gt; 1 and len(prefix_parts) &gt; 0:
        for pp in prefix_parts:
            if pp not in given_parts:
                accumulate.append(pp)
        accumulate.extend(given_parts)
        object_name = &#34;/&#34;.join(accumulate)

    super().put_object(
        bucket_name=self.bucket_name,
        object_name=object_name,
        data=BytesIO(buffer),
        length=len(buffer),
        metadata=metadata,
        content_type=content_type,
    )

    return object_name</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.session"><code class="name flex">
<span>def <span class="ident">session</span></span>(<span>self, lock: bool = False) ‑> ResponseJSON or ResponseOctet</span>
</code></dt>
<dd>
<div class="desc"><p>Object storage locking decorator for functions.</p>
<p>When used this implements a mutex lock on the object path,
which will block competing operations until it is cleared.</p>
<p>Locks will not block read operations except in special cases.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def session(self, lock: bool = False) -&gt; ResponseJSON or ResponseOctet:
    &#34;&#34;&#34;
    Object storage locking decorator for functions.

    When used this implements a mutex lock on the object path,
    which will block competing operations until it is cleared.

    Locks will not block read operations except in special cases. 
    &#34;&#34;&#34;

    # index = load_json(self.get_object(object_name=index_file))

    headers = {}
    session_id = uuid4().hex
    name = &#34;bathysphere&#34;
    lock_file = f&#34;{name}/lock.json&#34;
    # index_file = f&#34;{name}/index.json&#34;

    def decorator(fcn):
        &#34;&#34;&#34;
        Methods applied to the wrapped function
        &#34;&#34;&#34;

        def wrapper(*args, **kwargs):
            &#34;&#34;&#34;
            Actual wrapper that calls the decorated function
            &#34;&#34;&#34;
            if self.stat_object(lock_file):
                return &#34;Lock in place&#34;, 500
            try:
                self.put_object(
                    object_name=lock_file,
                    data={&#34;session&#34;: session_id},
                    metadata=self.metadata_template(&#34;lock&#34;, headers=headers),
                )
            except NoSuchKey:
                return &#34;Could not lock repository&#34;, 500
            try:
                result = fcn(*args, **kwargs)
            except Exception as ex:
                result = f&#34;{ex}&#34;, 500
            finally:
                if lock and not self.unlock(object_name=lock):
                    result = &#34;Failed to unlock&#34;, 500
            return result

        return wrapper

    return decorator</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.stat_object"><code class="name flex">
<span>def <span class="ident">stat_object</span></span>(<span>self, object_name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine whether an object key exists</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stat_object(self, object_name: str):
    &#34;&#34;&#34;
    Determine whether an object key exists
    &#34;&#34;&#34;
    try:
        return super().stat_object(self.bucket_name, object_name)
    except NoSuchKey:
        return None</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.unlock"><code class="name flex">
<span>def <span class="ident">unlock</span></span>(<span>self, object_name: str, session: str = None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unlock(self, object_name: str, session: str = None,) -&gt; bool:
    &#34;&#34;&#34;
    Unlock the dataset or bathysphere_functions_repository IFF it contains the session ID
    &#34;&#34;&#34;
    try:
        self.remove_object(self.bucket_name, object_name)
    except NoSuchKey:
        return False
    return True</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.ObjectStorage.updateIndex"><code class="name flex">
<span>def <span class="ident">updateIndex</span></span>(<span>self, object_name: str, metadata: dict = None, entries: [dict] = None, props: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Update contents of index metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def updateIndex(
    self,
    object_name: str,
    metadata: dict = None,
    entries: [dict] = None,
    props: dict = None,
):
    &#34;&#34;&#34;
    Update contents of index metadata
    &#34;&#34;&#34;

    if entries:
        self.put_object(
            object_name=object_name,
            data={
                **loads(self.get_object(object_name=object_name).data),
                **(entries or {}),
                **(props or {}),
            },
            metadata={**self.stat_object(object_name).metadata, **(metadata or {})},
        )
    else:
        self.copy_object(
            bucket_name=self.bucket_name,
            object_name=object_name,
            object_source=object_name,
            metadata=metadata,
        )

    return self</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.PostgresType"><code class="flex name class">
<span>class <span class="ident">PostgresType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PostgresType(Enum):
    Numerical = &#34;DOUBLE PRECISION NULL&#34;
    TimeStamp = &#34;TIMESTAMP NOT NULL&#34;
    Geography = &#34;GEOGRAPHY NOT NULL&#34;
    IntIdentity = &#34;INT PRIMARY KEY&#34;
    NullString = &#34;VARCHAR(100) NULL&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.PostgresType.Geography"><code class="name">var <span class="ident">Geography</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.IntIdentity"><code class="name">var <span class="ident">IntIdentity</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.NullString"><code class="name">var <span class="ident">NullString</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.Numerical"><code class="name">var <span class="ident">Numerical</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.PostgresType.TimeStamp"><code class="name">var <span class="ident">TimeStamp</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Query"><code class="flex name class">
<span>class <span class="ident">Query</span></span>
<span>(</span><span>sql: str, parser: Callable)</span>
</code></dt>
<dd>
<div class="desc"><p>Method generated by attrs for class Query.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Query:

    sql: str = attr.ib()
    parser: Callable = attr.ib()</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Query.parser"><code class="name">var <span class="ident">parser</span> : Callable</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Query.sql"><code class="name">var <span class="ident">sql</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Schema"><code class="flex name class">
<span>class <span class="ident">Schema</span></span>
<span>(</span><span>fields: [<a title="bathysphere.datatypes.Field" href="#bathysphere.datatypes.Field">Field</a>] = NOTHING)</span>
</code></dt>
<dd>
<div class="desc"><p>Method generated by attrs for class Schema.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Schema:
    fields: [Field] = attr.ib(default=attr.Factory(list))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Schema.fields"><code class="name">var <span class="ident">fields</span> : [<a title="bathysphere.datatypes.Field" href="#bathysphere.datatypes.Field">Field</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Table"><code class="flex name class">
<span>class <span class="ident">Table</span></span>
<span>(</span><span>name: str, schema: <a title="bathysphere.datatypes.Schema" href="#bathysphere.datatypes.Schema">Schema</a> = Schema(fields=[]))</span>
</code></dt>
<dd>
<div class="desc"><p>Method generated by attrs for class Table.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Table:

    name: str = attr.ib()
    schema: Schema = attr.ib(default=Schema())

    @staticmethod
    def _unwrap(x):
        &#34;&#34;&#34;
        Some queries return iterables that need to be unpacked
        &#34;&#34;&#34;
        return {&#34;record&#34;: x[0]}

    def declare(self) -&gt; Query:
        &#34;&#34;&#34;
        Generate a query to create a new table but do not execute
        &#34;&#34;&#34;
        fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
        queryString = f&#34;&#34;&#34;
        CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
        &#34;&#34;&#34;
        return Query(queryString, None)

    def insert(self, data: ()) -&gt; Query:
        &#34;&#34;&#34;
        Generate the query to insert new rows into database.
        &#34;&#34;&#34;
        _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
        columns, values = map(
            join, ((field.name for field in self.schema.fields), _parsedValues)
        )

        queryString = f&#34;&#34;&#34;
        INSERT INTO {self.name} ({columns}) VALUES {values};
        &#34;&#34;&#34;
        return Query(queryString, None)

    def select(
        self,
        order_by: str = None,
        limit: int = 100,
        fields: (str) = (&#34;*&#34;,),
        order: str = &#34;DESC&#34;,
        conditions: ((str)) = (),
    ) -&gt; Query:
        &#34;&#34;&#34;
        Read back values/rows.
        &#34;&#34;&#34;
        _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
        _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

        queryString = f&#34;&#34;&#34;
        SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
        &#34;&#34;&#34;

        return Query(queryString, Table._unwrap)

    def drop(self) -&gt; Query:
        &#34;&#34;&#34;
        Drop the entire table
        &#34;&#34;&#34;
        return Query(f&#34;DROP TABLE {self.name};&#34;, None)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Table.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bathysphere.datatypes.Table.schema"><code class="name">var <span class="ident">schema</span> : <a title="bathysphere.datatypes.Schema" href="#bathysphere.datatypes.Schema">Schema</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Table.declare"><code class="name flex">
<span>def <span class="ident">declare</span></span>(<span>self) ‑> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate a query to create a new table but do not execute</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def declare(self) -&gt; Query:
    &#34;&#34;&#34;
    Generate a query to create a new table but do not execute
    &#34;&#34;&#34;
    fieldString = join(f&#34;{f.name} {f.type}&#34; for f in self.schema.fields)
    queryString = f&#34;&#34;&#34;
    CREATE TABLE IF NOT EXISTS {self.name}({fieldString});
    &#34;&#34;&#34;
    return Query(queryString, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.drop"><code class="name flex">
<span>def <span class="ident">drop</span></span>(<span>self) ‑> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Drop the entire table</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop(self) -&gt; Query:
    &#34;&#34;&#34;
    Drop the entire table
    &#34;&#34;&#34;
    return Query(f&#34;DROP TABLE {self.name};&#34;, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, data: ()) ‑> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate the query to insert new rows into database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, data: ()) -&gt; Query:
    &#34;&#34;&#34;
    Generate the query to insert new rows into database.
    &#34;&#34;&#34;
    _parsedValues = (f&#34;({join(map(parsePostgresValueIn, row))})&#34; for row in data)
    columns, values = map(
        join, ((field.name for field in self.schema.fields), _parsedValues)
    )

    queryString = f&#34;&#34;&#34;
    INSERT INTO {self.name} ({columns}) VALUES {values};
    &#34;&#34;&#34;
    return Query(queryString, None)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Table.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, order_by: str = None, limit: int = 100, fields: str = ('*',), order: str = 'DESC', conditions: str = ()) ‑> <a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read back values/rows.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(
    self,
    order_by: str = None,
    limit: int = 100,
    fields: (str) = (&#34;*&#34;,),
    order: str = &#34;DESC&#34;,
    conditions: ((str)) = (),
) -&gt; Query:
    &#34;&#34;&#34;
    Read back values/rows.
    &#34;&#34;&#34;
    _order = f&#34;ORDER BY {order_by} {order}&#34; if order_by else &#34;&#34;
    _conditions = f&#34;WHERE {&#39; AND &#39;.join(conditions)}&#34; if conditions else &#34;&#34;

    queryString = f&#34;&#34;&#34;
    SELECT {&#39;, &#39;.join(fields)} FROM {self.name} {_conditions} {_order} LIMIT {limit};
    &#34;&#34;&#34;

    return Query(queryString, Table._unwrap)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.TimeStamp"><code class="flex name class">
<span>class <span class="ident">TimeStamp</span></span>
</code></dt>
<dd>
<div class="desc"><p>Method generated by attrs for class TimeStamp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TimeStamp:
    @staticmethod
    def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
        &#34;&#34;&#34;
        Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
        &#34;&#34;&#34;
        assert len(buffer) == 7
        yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
            int.from_bytes(buffer[:3], byteorder=byteorder),
            int.from_bytes(buffer[3:], byteorder=byteorder),
        )
        return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.TimeStamp.parseBinary"><code class="name flex">
<span>def <span class="ident">parseBinary</span></span>(<span>buffer: bytes, byteorder: str = 'big') ‑> datetime.datetime</span>
</code></dt>
<dd>
<div class="desc"><p>Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parseBinary(buffer: bytes, byteorder: str = &#34;big&#34;) -&gt; datetime:
    &#34;&#34;&#34;
    Convert two byte words into integer strings, and then date time. Only works for Satlantic date formats.
    &#34;&#34;&#34;
    assert len(buffer) == 7
    yyyydddhhmmssmmm = &#34;{:07}{:09}&#34;.format(
        int.from_bytes(buffer[:3], byteorder=byteorder),
        int.from_bytes(buffer[3:], byteorder=byteorder),
    )
    return datetime.strptime(yyyydddhhmmssmmm, &#34;%Y%j%H%M%S%f&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Topology"><code class="flex name class">
<span>class <span class="ident">Topology</span></span>
<span>(</span><span>cells: array = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Method generated by attrs for class Topology.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Topology:

    cells: array = attr.ib(default=None)

    def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
        &#34;&#34;&#34;
        Get element neighbors
        &#34;&#34;&#34;
        queue = dict()
        while indices:
            cell = indices.pop()
            nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
            buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
            key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
            queue[key][cell] = buffer

        return queue


    @staticmethod
    def read(path: str, indexed: bool = True) -&gt; dict:
        &#34;&#34;&#34;
        Read in grid topology of unstructured triangular grid
        &#34;&#34;&#34;
        if path[-3:] == &#34;.nc&#34;:
            fid = Dataset(path)
            topo = fid.variables[&#34;nv&#34;][:].T
        else:
            fid = open(path, &#34;r&#34;)
            df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
            topo = df.__array__()

        n = len(topo)

        basis = 0
        enforce = 1
        minimum = topo.min()
        if (minimum != enforce) if enforce else True:
            topo -= minimum + basis  # zero-index
        
        return {
            &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
            &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
        }

    @property
    def adjacency(self):
        &#34;&#34;&#34;
        Get node parents and node neighbors from topology

        :param topology:
        :return:
        &#34;&#34;&#34;
        _parents = dict()
        _neighbors = dict()

        for element in range(len(self.cells)):
            vertices = self.cells[element]
            for node in vertices:
                try:
                    p = _parents[node]
                except KeyError:
                    p = _parents[node] = []
                p.append(element)  # add element to parents, no possible duplicates

                try:
                    n = _neighbors[node]
                except KeyError:
                    n = _neighbors[node] = []
                (mask,) = where(node != vertices)
                others = vertices[mask]

                for neighbor in others:
                    if neighbor not in n:
                        n.append(neighbor)  # add current element to parents

        solid = zeros(n, dtype=bool)
        for node in range(n):
            difference = _neighbors[node].__len__() - _parents[node].__len__()
            if difference == 1:
                solid[node] = True
            elif difference != 0:
                print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.cells"><code class="name">var <span class="ident">cells</span> : <built-in function array></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>path: str, indexed: bool = True) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read in grid topology of unstructured triangular grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def read(path: str, indexed: bool = True) -&gt; dict:
    &#34;&#34;&#34;
    Read in grid topology of unstructured triangular grid
    &#34;&#34;&#34;
    if path[-3:] == &#34;.nc&#34;:
        fid = Dataset(path)
        topo = fid.variables[&#34;nv&#34;][:].T
    else:
        fid = open(path, &#34;r&#34;)
        df = read_csv(fid, sep=&#34;,&#34;, usecols=arange(4 if indexed else 3), header=None)
        topo = df.__array__()

    n = len(topo)

    basis = 0
    enforce = 1
    minimum = topo.min()
    if (minimum != enforce) if enforce else True:
        topo -= minimum + basis  # zero-index
    
    return {
        &#34;indices&#34;: topo[:, 0] if indexed else arange(n),
        &#34;topology&#34;: topo[:, 0] if indexed else arange(n),
    }</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.adjacency"><code class="name">var <span class="ident">adjacency</span></code></dt>
<dd>
<div class="desc"><p>Get node parents and node neighbors from topology</p>
<p>:param topology:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def adjacency(self):
    &#34;&#34;&#34;
    Get node parents and node neighbors from topology

    :param topology:
    :return:
    &#34;&#34;&#34;
    _parents = dict()
    _neighbors = dict()

    for element in range(len(self.cells)):
        vertices = self.cells[element]
        for node in vertices:
            try:
                p = _parents[node]
            except KeyError:
                p = _parents[node] = []
            p.append(element)  # add element to parents, no possible duplicates

            try:
                n = _neighbors[node]
            except KeyError:
                n = _neighbors[node] = []
            (mask,) = where(node != vertices)
            others = vertices[mask]

            for neighbor in others:
                if neighbor not in n:
                    n.append(neighbor)  # add current element to parents

    solid = zeros(n, dtype=bool)
    for node in range(n):
        difference = _neighbors[node].__len__() - _parents[node].__len__()
        if difference == 1:
            solid[node] = True
        elif difference != 0:
            print(&#34;Error. Nonsense dimensions in detecting solid boundary nodes.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Topology.cell_adjacency"><code class="name flex">
<span>def <span class="ident">cell_adjacency</span></span>(<span>self, parents: dict, indices: [int]) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Get element neighbors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cell_adjacency(self, parents: dict, indices: [int]) -&gt; dict:
    &#34;&#34;&#34;
    Get element neighbors
    &#34;&#34;&#34;
    queue = dict()
    while indices:
        cell = indices.pop()
        nodes = [set(parents[key]) - {cell} for key in self.cells[cell, :]]
        buffer = [nodes[ii] &amp; nodes[ii - 1] for ii in range(3)]
        key = &#34;neighbor&#34; if 0 &lt; len(buffer) &lt;= 3 else &#34;error&#34;
        queue[key][cell] = buffer

    return queue</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.Trie"><code class="flex name class">
<span>class <span class="ident">Trie</span></span>
<span>(</span><span>word=None, weight=0, aliases=None, children=NOTHING)</span>
</code></dt>
<dd>
<div class="desc"><p>A Tree-like data structure is used for string translation, auto-correct, and auto-complete
functionality when interacting with the backend.</p>
<p>This is an enhanced Trie, which has a network of nodes representing sequences of symbols.
The implementation does not re-link paths, and is only branching. </p>
<p>Method generated by attrs for class Trie.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trie:
    &#34;&#34;&#34;
    A Tree-like data structure is used for string translation, auto-correct, and auto-complete
    functionality when interacting with the backend.

    This is an enhanced Trie, which has a network of nodes representing sequences of symbols.
    The implementation does not re-link paths, and is only branching. 
    &#34;&#34;&#34;

    word = attr.ib(default=None)
    weight = attr.ib(default=0)
    aliases = attr.ib(default=None)
    children = attr.ib(default=attr.Factory(dict))

    def insert(self, key: str, aliases: [str] = None) -&gt; None:
        &#34;&#34;&#34;
        Using the current node as the root, for each symbol in the word create or join 
        a child tree, and iteratively descend. Set the word of the final node to the 
        provided key.

        Optionally provide a list of aliases that can be returned.
        &#34;&#34;&#34;
        node = self
        for symbol in key:
            if symbol not in node.children:
                node.children[symbol] = Trie()
            node = node.children[symbol]
            node.weight += 1

        node.word = key
        node.aliases = aliases

    @staticmethod
    def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
        &#34;&#34;&#34;
        Descend through the tree, calculating the cost tables iteratively for subsets of the
        pattern
        &#34;&#34;&#34;
        _filter = lambda x: len(x)
        row = (previous[0] + 1,)
        for column in range(1, len(pattern) + 1):
            row += (
                min(
                    row[column - 1] + 1,
                    previous[column] + 1,
                    previous[column - 1] + int(pattern[column - 1] != symbol),
                ),
            )

        if min(row) &lt;= cost:
            filtered = tuple(chain(*filter(_filter,
                tuple(Trie.searchRecursive(v, k, pattern, row, cost) for k, v in node.children.items()),
            ))) 
        else:
            filtered = ()

        return (((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()) + filtered

    @staticmethod
    def levenshteinDistance(word1: str, word2: str) -&gt; int:
        &#34;&#34;&#34;
        Calculate the number of mutations needed to transform one sequence into
        a second sequention. This distance function is used to compare words for
        auto-correct functionality.
        &#34;&#34;&#34;
        columns = len(word1) + 1
        rows = len(word2) + 1

        # build first row
        currentRow = [0]
        for column in range(1, columns):
            currentRow.append(currentRow[column - 1] + 1)

        for row in range(1, rows):
            previousRow = currentRow
            currentRow = [previousRow[0] + 1]

            for column in range(1, columns):

                insertCost = currentRow[column - 1] + 1
                deleteCost = previousRow[column] + 1

                if word1[column - 1] != word2[row - 1]:
                    replaceCost = previousRow[column - 1] + 1
                else:
                    replaceCost = previousRow[column - 1]

                currentRow.append(min(insertCost, deleteCost, replaceCost))

        return currentRow[-1]

    @staticmethod
    def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
        &#34;&#34;&#34;
        Use simple memory-efficient search if the structure is trivial. Try `searchRecursive`
        for faster/larger searches on large structures.
        &#34;&#34;&#34;
        _results = ()
        for word in words:
            cost = Trie.levenshteinDistance(pattern, word)
            if cost &lt;= maxCost:
                _results += ((word, cost),)
        return _results</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bathysphere.datatypes.Trie.levenshteinDistance"><code class="name flex">
<span>def <span class="ident">levenshteinDistance</span></span>(<span>word1: str, word2: str) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the number of mutations needed to transform one sequence into
a second sequention. This distance function is used to compare words for
auto-correct functionality.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def levenshteinDistance(word1: str, word2: str) -&gt; int:
    &#34;&#34;&#34;
    Calculate the number of mutations needed to transform one sequence into
    a second sequention. This distance function is used to compare words for
    auto-correct functionality.
    &#34;&#34;&#34;
    columns = len(word1) + 1
    rows = len(word2) + 1

    # build first row
    currentRow = [0]
    for column in range(1, columns):
        currentRow.append(currentRow[column - 1] + 1)

    for row in range(1, rows):
        previousRow = currentRow
        currentRow = [previousRow[0] + 1]

        for column in range(1, columns):

            insertCost = currentRow[column - 1] + 1
            deleteCost = previousRow[column] + 1

            if word1[column - 1] != word2[row - 1]:
                replaceCost = previousRow[column - 1] + 1
            else:
                replaceCost = previousRow[column - 1]

            currentRow.append(min(insertCost, deleteCost, replaceCost))

    return currentRow[-1]</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Trie.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>words: {str}, pattern: str, maxCost: int) ‑> (str, int)</span>
</code></dt>
<dd>
<div class="desc"><p>Use simple memory-efficient search if the structure is trivial. Try <code>searchRecursive</code>
for faster/larger searches on large structures.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def search(words: {str}, pattern: str, maxCost: int) -&gt; ((str, int)):
    &#34;&#34;&#34;
    Use simple memory-efficient search if the structure is trivial. Try `searchRecursive`
    for faster/larger searches on large structures.
    &#34;&#34;&#34;
    _results = ()
    for word in words:
        cost = Trie.levenshteinDistance(pattern, word)
        if cost &lt;= maxCost:
            _results += ((word, cost),)
    return _results</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.Trie.searchRecursive"><code class="name flex">
<span>def <span class="ident">searchRecursive</span></span>(<span>node, symbol: str, pattern: str, previous: (int,), cost: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Descend through the tree, calculating the cost tables iteratively for subsets of the
pattern</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def searchRecursive(node, symbol: str, pattern: str, previous: (int,), cost: int):
    &#34;&#34;&#34;
    Descend through the tree, calculating the cost tables iteratively for subsets of the
    pattern
    &#34;&#34;&#34;
    _filter = lambda x: len(x)
    row = (previous[0] + 1,)
    for column in range(1, len(pattern) + 1):
        row += (
            min(
                row[column - 1] + 1,
                previous[column] + 1,
                previous[column - 1] + int(pattern[column - 1] != symbol),
            ),
        )

    if min(row) &lt;= cost:
        filtered = tuple(chain(*filter(_filter,
            tuple(Trie.searchRecursive(v, k, pattern, row, cost) for k, v in node.children.items()),
        ))) 
    else:
        filtered = ()

    return (((node.word, row[-1]),) if row[-1] &lt;= cost and node.word is not None else ()) + filtered</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.Trie.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, key: str, aliases: [str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Using the current node as the root, for each symbol in the word create or join
a child tree, and iteratively descend. Set the word of the final node to the
provided key.</p>
<p>Optionally provide a list of aliases that can be returned.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, key: str, aliases: [str] = None) -&gt; None:
    &#34;&#34;&#34;
    Using the current node as the root, for each symbol in the word create or join 
    a child tree, and iteratively descend. Set the word of the final node to the 
    provided key.

    Optionally provide a list of aliases that can be returned.
    &#34;&#34;&#34;
    node = self
    for symbol in key:
        if symbol not in node.children:
            node.children[symbol] = Trie()
        node = node.children[symbol]
        node.weight += 1

    node.word = key
    node.aliases = aliases</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bathysphere.datatypes.View"><code class="flex name class">
<span>class <span class="ident">View</span></span>
<span>(</span><span>style, extent=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup and return figure and axis instances</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class View:
    count = 0

    def __init__(self, style, extent=None):
        # type: (dict, (float,)) -&gt; View
        &#34;&#34;&#34;
        Setup and return figure and axis instances
        &#34;&#34;&#34;
        rc(&#34;text&#34;, usetex=False)
        # rc(&#34;font&#34;, **{&#34;family&#34;: &#34;sans-serif&#34;, &#34;sans-serif&#34;: [&#34;Arial&#34;]})
        rc(&#34;mathtext&#34;, default=&#34;sf&#34;)
        rc(&#34;lines&#34;, markeredgewidth=1, linewidth=style[&#34;line&#34;])
        rc(&#34;axes&#34;, labelsize=style[&#34;text&#34;], linewidth=(style[&#34;line&#34;] + 1) // 2)
        rc(&#34;xtick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;ytick&#34;, labelsize=style[&#34;text&#34;])
        rc(&#34;xtick.major&#34;, pad=5)
        rc(&#34;ytick.major&#34;, pad=5)

        self.style = style
        self.extent = extent
        self.fig, self.ax = subplots(
            facecolor=style[&#34;bg&#34;], figsize=(style[&#34;width&#34;], style[&#34;height&#34;])
        )
        padding = style[&#34;padding&#34;]
        subplots_adjust(
            left=padding[0], bottom=padding[1], right=1 - padding[2], top=1 - padding[3]
        )

    def format(self, bg: str, contrast: str, **kwargs):
        &#34;&#34;&#34;
        Setup color styles for figure
        &#34;&#34;&#34;
        self.ax.patch.set_facecolor(bg)  # background colors
        self.ax.edgecolor = contrast  # plotting area border
        self.format_axis(&#34;x&#34;, contrast, **kwargs)
        self.format_axis(&#34;y&#34;, contrast, **kwargs)

    def format_axis(
        self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
    ):
        if axis in (&#34;x&#34;, &#34;X&#34;):
            apply = self.ax.xaxis
            spines = (&#34;left&#34;, &#34;right&#34;)
        elif axis in (&#34;y&#34;, &#34;Y&#34;):
            apply = self.ax.yaxis
            spines = (&#34;top&#34;, &#34;bottom&#34;)
        else:
            raise ValueError

        apply.label.set_color(label)
        self.ax.tick_params(axis=axis.lower(), colors=label)
        for each in spines:
            self.ax.spines[each].set_color(contrast)
        apply.grid(grid)

    def pre_push(self):
        self.fig.canvas.draw()
        self.format(**self.style)
        self.ax.set_frame_on(True)

    def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
        # type: (str, bool, dict) -&gt; BytesIO
        buffer = BytesIO()
        self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
        buffer.seek(0)
        return buffer

    def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
        &#34;&#34;&#34;
        Format figure legend

        Kwargs:
            loc, str -- location on plotting area
            fc, str/arr -- string or RGBA color for face
            ec, str/arr -- string or RGBA color for edges

        Returns: matplotlib legend object
        &#34;&#34;&#34;
        legend = self.ax.legend(loc=loc)
        frame = legend.get_frame()
        frame.set_facecolor(fc)
        frame.set_edgecolor(ec)

        for text in legend.get_texts():
            text.set_color(self.style[&#34;contrast&#34;])</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="bathysphere.datatypes.View.count"><code class="name">var <span class="ident">count</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bathysphere.datatypes.View.format"><code class="name flex">
<span>def <span class="ident">format</span></span>(<span>self, bg: str, contrast: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup color styles for figure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format(self, bg: str, contrast: str, **kwargs):
    &#34;&#34;&#34;
    Setup color styles for figure
    &#34;&#34;&#34;
    self.ax.patch.set_facecolor(bg)  # background colors
    self.ax.edgecolor = contrast  # plotting area border
    self.format_axis(&#34;x&#34;, contrast, **kwargs)
    self.format_axis(&#34;y&#34;, contrast, **kwargs)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.format_axis"><code class="name flex">
<span>def <span class="ident">format_axis</span></span>(<span>self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_axis(
    self, axis: str, contrast: str, label: str, grid: bool, **kwargs: dict
):
    if axis in (&#34;x&#34;, &#34;X&#34;):
        apply = self.ax.xaxis
        spines = (&#34;left&#34;, &#34;right&#34;)
    elif axis in (&#34;y&#34;, &#34;Y&#34;):
        apply = self.ax.yaxis
        spines = (&#34;top&#34;, &#34;bottom&#34;)
    else:
        raise ValueError

    apply.label.set_color(label)
    self.ax.tick_params(axis=axis.lower(), colors=label)
    for each in spines:
        self.ax.spines[each].set_color(contrast)
    apply.grid(grid)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.legend"><code class="name flex">
<span>def <span class="ident">legend</span></span>(<span>self, loc: str = 'best', fc: str = 'none', ec: str = 'none')</span>
</code></dt>
<dd>
<div class="desc"><p>Format figure legend</p>
<h2 id="kwargs">Kwargs</h2>
<p>loc, str &ndash; location on plotting area
fc, str/arr &ndash; string or RGBA color for face
ec, str/arr &ndash; string or RGBA color for edges</p>
<p>Returns: matplotlib legend object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def legend(self, loc: str = &#34;best&#34;, fc: str = &#34;none&#34;, ec: str = &#34;none&#34;):
    &#34;&#34;&#34;
    Format figure legend

    Kwargs:
        loc, str -- location on plotting area
        fc, str/arr -- string or RGBA color for face
        ec, str/arr -- string or RGBA color for edges

    Returns: matplotlib legend object
    &#34;&#34;&#34;
    legend = self.ax.legend(loc=loc)
    frame = legend.get_frame()
    frame.set_facecolor(fc)
    frame.set_edgecolor(ec)

    for text in legend.get_texts():
        text.set_color(self.style[&#34;contrast&#34;])</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.pre_push"><code class="name flex">
<span>def <span class="ident">pre_push</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_push(self):
    self.fig.canvas.draw()
    self.format(**self.style)
    self.ax.set_frame_on(True)</code></pre>
</details>
</dd>
<dt id="bathysphere.datatypes.View.push"><code class="name flex">
<span>def <span class="ident">push</span></span>(<span>self, encoding='png', transparent=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push(self, encoding=&#34;png&#34;, transparent=False, **kwargs):
    # type: (str, bool, dict) -&gt; BytesIO
    buffer = BytesIO()
    self.fig.savefig(buffer, format=encoding, transparent=transparent, **kwargs)
    buffer.seek(0)
    return buffer</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="bathysphere" href="index.html">bathysphere</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="bathysphere.datatypes.ConvexHull" href="#bathysphere.datatypes.ConvexHull">ConvexHull</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="bathysphere.datatypes.Array" href="#bathysphere.datatypes.Array">Array</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Array.data" href="#bathysphere.datatypes.Array.data">data</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.gpu" href="#bathysphere.datatypes.Array.gpu">gpu</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.interval" href="#bathysphere.datatypes.Array.interval">interval</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.normalized" href="#bathysphere.datatypes.Array.normalized">normalized</a></code></li>
<li><code><a title="bathysphere.datatypes.Array.range" href="#bathysphere.datatypes.Array.range">range</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Bound" href="#bathysphere.datatypes.Bound">Bound</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Bound.closed" href="#bathysphere.datatypes.Bound.closed">closed</a></code></li>
<li><code><a title="bathysphere.datatypes.Bound.value" href="#bathysphere.datatypes.Bound.value">value</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.CloudSQL" href="#bathysphere.datatypes.CloudSQL">CloudSQL</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.CloudSQL.auth" href="#bathysphere.datatypes.CloudSQL.auth">auth</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.engine" href="#bathysphere.datatypes.CloudSQL.engine">engine</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.handle" href="#bathysphere.datatypes.CloudSQL.handle">handle</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.instance" href="#bathysphere.datatypes.CloudSQL.instance">instance</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.max_overflow" href="#bathysphere.datatypes.CloudSQL.max_overflow">max_overflow</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.pool_recycle" href="#bathysphere.datatypes.CloudSQL.pool_recycle">pool_recycle</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.pool_size" href="#bathysphere.datatypes.CloudSQL.pool_size">pool_size</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.pool_timeout" href="#bathysphere.datatypes.CloudSQL.pool_timeout">pool_timeout</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.port" href="#bathysphere.datatypes.CloudSQL.port">port</a></code></li>
<li><code><a title="bathysphere.datatypes.CloudSQL.query" href="#bathysphere.datatypes.CloudSQL.query">query</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Dataset" href="#bathysphere.datatypes.Dataset">Dataset</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Dataset.copy" href="#bathysphere.datatypes.Dataset.copy">copy</a></code></li>
<li><code><a title="bathysphere.datatypes.Dataset.query" href="#bathysphere.datatypes.Dataset.query">query</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Extent" href="#bathysphere.datatypes.Extent">Extent</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Extent.intervals" href="#bathysphere.datatypes.Extent.intervals">intervals</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.overlaps" href="#bathysphere.datatypes.Extent.overlaps">overlaps</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.path" href="#bathysphere.datatypes.Extent.path">path</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.value" href="#bathysphere.datatypes.Extent.value">value</a></code></li>
<li><code><a title="bathysphere.datatypes.Extent.vertex_array" href="#bathysphere.datatypes.Extent.vertex_array">vertex_array</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Feature" href="#bathysphere.datatypes.Feature">Feature</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Feature.geometry" href="#bathysphere.datatypes.Feature.geometry">geometry</a></code></li>
<li><code><a title="bathysphere.datatypes.Feature.properties" href="#bathysphere.datatypes.Feature.properties">properties</a></code></li>
<li><code><a title="bathysphere.datatypes.Feature.type" href="#bathysphere.datatypes.Feature.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FeatureCollection" href="#bathysphere.datatypes.FeatureCollection">FeatureCollection</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.FeatureCollection.features" href="#bathysphere.datatypes.FeatureCollection.features">features</a></code></li>
<li><code><a title="bathysphere.datatypes.FeatureCollection.properties" href="#bathysphere.datatypes.FeatureCollection.properties">properties</a></code></li>
<li><code><a title="bathysphere.datatypes.FeatureCollection.type" href="#bathysphere.datatypes.FeatureCollection.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Field" href="#bathysphere.datatypes.Field">Field</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Field.autoCorrect" href="#bathysphere.datatypes.Field.autoCorrect">autoCorrect</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.clean" href="#bathysphere.datatypes.Field.clean">clean</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.name" href="#bathysphere.datatypes.Field.name">name</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.restore" href="#bathysphere.datatypes.Field.restore">restore</a></code></li>
<li><code><a title="bathysphere.datatypes.Field.type" href="#bathysphere.datatypes.Field.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.File" href="#bathysphere.datatypes.File">File</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.File.content" href="#bathysphere.datatypes.File.content">content</a></code></li>
<li><code><a title="bathysphere.datatypes.File.encoding" href="#bathysphere.datatypes.File.encoding">encoding</a></code></li>
<li><code><a title="bathysphere.datatypes.File.kb" href="#bathysphere.datatypes.File.kb">kb</a></code></li>
<li><code><a title="bathysphere.datatypes.File.metadata" href="#bathysphere.datatypes.File.metadata">metadata</a></code></li>
<li><code><a title="bathysphere.datatypes.File.metadata_promise" href="#bathysphere.datatypes.File.metadata_promise">metadata_promise</a></code></li>
<li><code><a title="bathysphere.datatypes.File.name" href="#bathysphere.datatypes.File.name">name</a></code></li>
<li><code><a title="bathysphere.datatypes.File.serialize" href="#bathysphere.datatypes.File.serialize">serialize</a></code></li>
<li><code><a title="bathysphere.datatypes.File.sn" href="#bathysphere.datatypes.File.sn">sn</a></code></li>
<li><code><a title="bathysphere.datatypes.File.sort_key" href="#bathysphere.datatypes.File.sort_key">sort_key</a></code></li>
<li><code><a title="bathysphere.datatypes.File.time" href="#bathysphere.datatypes.File.time">time</a></code></li>
<li><code><a title="bathysphere.datatypes.File.ts" href="#bathysphere.datatypes.File.ts">ts</a></code></li>
<li><code><a title="bathysphere.datatypes.File.url" href="#bathysphere.datatypes.File.url">url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FileSystem" href="#bathysphere.datatypes.FileSystem">FileSystem</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.FileSystem.OverwritePolicy" href="#bathysphere.datatypes.FileSystem.OverwritePolicy">OverwritePolicy</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.get" href="#bathysphere.datatypes.FileSystem.get">get</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexFileMetadata" href="#bathysphere.datatypes.FileSystem.indexFileMetadata">indexFileMetadata</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexFromHtmlTable" href="#bathysphere.datatypes.FileSystem.indexFromHtmlTable">indexFromHtmlTable</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexFtp" href="#bathysphere.datatypes.FileSystem.indexFtp">indexFtp</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.indexTaskTree" href="#bathysphere.datatypes.FileSystem.indexTaskTree">indexTaskTree</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.load_year_cache" href="#bathysphere.datatypes.FileSystem.load_year_cache">load_year_cache</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.policy" href="#bathysphere.datatypes.FileSystem.policy">policy</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.search" href="#bathysphere.datatypes.FileSystem.search">search</a></code></li>
<li><code><a title="bathysphere.datatypes.FileSystem.syncFtp" href="#bathysphere.datatypes.FileSystem.syncFtp">syncFtp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.FileType" href="#bathysphere.datatypes.FileType">FileType</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.FileType.Config" href="#bathysphere.datatypes.FileType.Config">Config</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Log" href="#bathysphere.datatypes.FileType.Log">Log</a></code></li>
<li><code><a title="bathysphere.datatypes.FileType.Schema" href="#bathysphere.datatypes.FileType.Schema">Schema</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Interval" href="#bathysphere.datatypes.Interval">Interval</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Interval.lower" href="#bathysphere.datatypes.Interval.lower">lower</a></code></li>
<li><code><a title="bathysphere.datatypes.Interval.overlaps" href="#bathysphere.datatypes.Interval.overlaps">overlaps</a></code></li>
<li><code><a title="bathysphere.datatypes.Interval.upper" href="#bathysphere.datatypes.Interval.upper">upper</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.JSONIOWrapper" href="#bathysphere.datatypes.JSONIOWrapper">JSONIOWrapper</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.dump" href="#bathysphere.datatypes.JSONIOWrapper.dump">dump</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.log" href="#bathysphere.datatypes.JSONIOWrapper.log">log</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.receive" href="#bathysphere.datatypes.JSONIOWrapper.receive">receive</a></code></li>
<li><code><a title="bathysphere.datatypes.JSONIOWrapper.send" href="#bathysphere.datatypes.JSONIOWrapper.send">send</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.KernelDensityEstimator" href="#bathysphere.datatypes.KernelDensityEstimator">KernelDensityEstimator</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh" href="#bathysphere.datatypes.KernelDensityEstimator.get_epsilon_from_mesh">get_epsilon_from_mesh</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.glm" href="#bathysphere.datatypes.KernelDensityEstimator.glm">glm</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.intensity" href="#bathysphere.datatypes.KernelDensityEstimator.intensity">intensity</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.predict" href="#bathysphere.datatypes.KernelDensityEstimator.predict">predict</a></code></li>
<li><code><a title="bathysphere.datatypes.KernelDensityEstimator.train" href="#bathysphere.datatypes.KernelDensityEstimator.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.ObjectStorage" href="#bathysphere.datatypes.ObjectStorage">ObjectStorage</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.ObjectStorage.delete" href="#bathysphere.datatypes.ObjectStorage.delete">delete</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.get_object" href="#bathysphere.datatypes.ObjectStorage.get_object">get_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.list_objects" href="#bathysphere.datatypes.ObjectStorage.list_objects">list_objects</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.locked" href="#bathysphere.datatypes.ObjectStorage.locked">locked</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.metadata_template" href="#bathysphere.datatypes.ObjectStorage.metadata_template">metadata_template</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.publish_events" href="#bathysphere.datatypes.ObjectStorage.publish_events">publish_events</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.put_object" href="#bathysphere.datatypes.ObjectStorage.put_object">put_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.session" href="#bathysphere.datatypes.ObjectStorage.session">session</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.stat_object" href="#bathysphere.datatypes.ObjectStorage.stat_object">stat_object</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.unlock" href="#bathysphere.datatypes.ObjectStorage.unlock">unlock</a></code></li>
<li><code><a title="bathysphere.datatypes.ObjectStorage.updateIndex" href="#bathysphere.datatypes.ObjectStorage.updateIndex">updateIndex</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.PostgresType" href="#bathysphere.datatypes.PostgresType">PostgresType</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.PostgresType.Geography" href="#bathysphere.datatypes.PostgresType.Geography">Geography</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.IntIdentity" href="#bathysphere.datatypes.PostgresType.IntIdentity">IntIdentity</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.NullString" href="#bathysphere.datatypes.PostgresType.NullString">NullString</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.Numerical" href="#bathysphere.datatypes.PostgresType.Numerical">Numerical</a></code></li>
<li><code><a title="bathysphere.datatypes.PostgresType.TimeStamp" href="#bathysphere.datatypes.PostgresType.TimeStamp">TimeStamp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Query" href="#bathysphere.datatypes.Query">Query</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Query.parser" href="#bathysphere.datatypes.Query.parser">parser</a></code></li>
<li><code><a title="bathysphere.datatypes.Query.sql" href="#bathysphere.datatypes.Query.sql">sql</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Schema" href="#bathysphere.datatypes.Schema">Schema</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Schema.fields" href="#bathysphere.datatypes.Schema.fields">fields</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Table" href="#bathysphere.datatypes.Table">Table</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.Table.declare" href="#bathysphere.datatypes.Table.declare">declare</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.drop" href="#bathysphere.datatypes.Table.drop">drop</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.insert" href="#bathysphere.datatypes.Table.insert">insert</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.name" href="#bathysphere.datatypes.Table.name">name</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.schema" href="#bathysphere.datatypes.Table.schema">schema</a></code></li>
<li><code><a title="bathysphere.datatypes.Table.select" href="#bathysphere.datatypes.Table.select">select</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.TimeStamp" href="#bathysphere.datatypes.TimeStamp">TimeStamp</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.TimeStamp.parseBinary" href="#bathysphere.datatypes.TimeStamp.parseBinary">parseBinary</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Topology" href="#bathysphere.datatypes.Topology">Topology</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Topology.adjacency" href="#bathysphere.datatypes.Topology.adjacency">adjacency</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.cell_adjacency" href="#bathysphere.datatypes.Topology.cell_adjacency">cell_adjacency</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.cells" href="#bathysphere.datatypes.Topology.cells">cells</a></code></li>
<li><code><a title="bathysphere.datatypes.Topology.read" href="#bathysphere.datatypes.Topology.read">read</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.Trie" href="#bathysphere.datatypes.Trie">Trie</a></code></h4>
<ul class="">
<li><code><a title="bathysphere.datatypes.Trie.insert" href="#bathysphere.datatypes.Trie.insert">insert</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.levenshteinDistance" href="#bathysphere.datatypes.Trie.levenshteinDistance">levenshteinDistance</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.search" href="#bathysphere.datatypes.Trie.search">search</a></code></li>
<li><code><a title="bathysphere.datatypes.Trie.searchRecursive" href="#bathysphere.datatypes.Trie.searchRecursive">searchRecursive</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bathysphere.datatypes.View" href="#bathysphere.datatypes.View">View</a></code></h4>
<ul class="two-column">
<li><code><a title="bathysphere.datatypes.View.count" href="#bathysphere.datatypes.View.count">count</a></code></li>
<li><code><a title="bathysphere.datatypes.View.format" href="#bathysphere.datatypes.View.format">format</a></code></li>
<li><code><a title="bathysphere.datatypes.View.format_axis" href="#bathysphere.datatypes.View.format_axis">format_axis</a></code></li>
<li><code><a title="bathysphere.datatypes.View.legend" href="#bathysphere.datatypes.View.legend">legend</a></code></li>
<li><code><a title="bathysphere.datatypes.View.pre_push" href="#bathysphere.datatypes.View.pre_push">pre_push</a></code></li>
<li><code><a title="bathysphere.datatypes.View.push" href="#bathysphere.datatypes.View.push">push</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>